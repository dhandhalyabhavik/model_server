diff --git a/Dockerfile.redhat b/Dockerfile.redhat
index 53f3efe2..f2a2b8f3 100644
--- a/Dockerfile.redhat
+++ b/Dockerfile.redhat
@@ -192,18 +192,20 @@ RUN if [ "$CHECK_COVERAGE" == "1" ] ; then true ; else  bazel test ${debug_bazel
     bazel coverage --test_output=streamed --combined_report=lcov //src:ovms_test &&\
     genhtml --output genhtml "$(bazel info output_path)/_coverage/_coverage_report.dat" &&\
     bazel test ${debug_bazel_flags} --jobs $JOBS --test_summary=detailed --test_output=streamed //src:ovms_test
-    
-RUN bazel build ${debug_bazel_flags} ${minitrace_flags} --jobs $JOBS //src:ovms
-RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:libsampleloader.so
-
-RUN cd /ovms/src/example/SampleCpuExtension/ && make 
 
 # C api shared library
 RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:ovms_shared
 RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:poc3
 
+# Copy binary for //src:ovms
+RUN cp $(bazel info bazel-bin)/src/libovms_shared.so src
+RUN bazel build ${debug_bazel_flags} ${minitrace_flags} --jobs $JOBS //src:ovms
+RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:libsampleloader.so
+
+RUN cd /ovms/src/example/SampleCpuExtension/ && make
+
 COPY ${ovms_metadata_file} metadata.json
 
-RUN ./bazel-bin/src/./ovms
+RUN cd bazel-bin/src/ && ./ovms --version && ./ovms && cd /ovms
 COPY release_files/thirdparty-licenses/ /ovms/release_files/thirdparty-licenses/
 COPY release_files/LICENSE /ovms/release_files/LICENSE
diff --git a/Dockerfile.ubuntu b/Dockerfile.ubuntu
index 72d0fe96..8f99ce32 100644
--- a/Dockerfile.ubuntu
+++ b/Dockerfile.ubuntu
@@ -231,20 +231,21 @@ RUN if [ "$CHECK_COVERAGE" == "1" ] ; then true ; else bazel test ${debug_bazel_
     genhtml --output genhtml "$(bazel info output_path)/_coverage/_coverage_report.dat" &&\
     bazel test ${debug_bazel_flags} --jobs $JOBS --test_summary=detailed --test_output=streamed //src:ovms_test
 
+# C api shared library
+RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:ovms_shared
+RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:poc3
+
+# Copy binary for //src:ovms
+RUN cp $(bazel info bazel-bin)/src/libovms_shared.so src
 RUN bazel build ${debug_bazel_flags} ${minitrace_flags} --jobs $JOBS //src:ovms
 RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:libsampleloader.so
 
 RUN cd /ovms/src/example/SampleCpuExtension/ && make
 
-# C api shared library
-RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:ovms_shared
-RUN bazel build ${debug_bazel_flags} --jobs $JOBS //src:poc3
-
 ARG ovms_metadata_file
 COPY ${ovms_metadata_file} metadata.json
 
-RUN ./bazel-bin/src/./ovms --version
-RUN ./bazel-bin/src/./ovms
+RUN cd bazel-bin/src/ && ./ovms --version && ./ovms && cd /ovms
 COPY release_files/thirdparty-licenses/ /ovms/release_files/thirdparty-licenses/
 COPY release_files/LICENSE /ovms/release_files/LICENSE
 
diff --git a/Makefile b/Makefile
index 432f666d..abdf9889 100644
--- a/Makefile
+++ b/Makefile
@@ -89,7 +89,7 @@ OVMS_CPP_IMAGE_TAG ?= latest
 PRODUCT_NAME = "OpenVINO Model Server"
 PRODUCT_VERSION ?= "2022.2"
 
-OVMS_CPP_CONTAINTER_NAME ?= server-test$(shell date +%Y-%m-%d-%H.%M.%S)
+OVMS_CPP_CONTAINTER_NAME = server-test$(shell date +%Y-%m-%d-%H.%M.%S)
 OVMS_CPP_CONTAINTER_PORT ?= 9178
 
 TEST_PATH ?= tests/functional/
@@ -243,7 +243,7 @@ test_checksec:
 	@docker cp $(OVMS_CPP_CONTAINTER_NAME):/ovms_release/bin/ovms /tmp
 	@docker rm -f $(OVMS_CPP_CONTAINTER_NAME)
 	@checksec --file=/tmp/ovms --format=csv > checksec.txt
-	@if ! grep -FRq "Full RELRO,Canary found,NX enabled,PIE enabled,No RPATH,RUNPATH,Symbols,Yes" checksec.txt; then\
+	@if ! grep -FRq "Full RELRO,Canary found,NX enabled,PIE enabled,No RPATH,RUNPATH,Symbols,No" checksec.txt; then\
  		error Run checksec on ovms binary and fix issues.;\
 	fi
 	@rm -f checksec.txt
diff --git a/check_coverage.bat b/check_coverage.bat
index 1be541a2..327530e9 100755
--- a/check_coverage.bat
+++ b/check_coverage.bat
@@ -5,9 +5,8 @@
 #MIN_FUNCTION_COV=87.4
 
 #Rhel
-MIN_LINES_COV=73.7
-MIN_FUNCTION_COV=74.1
-
+MIN_LINES_COV=74.1
+MIN_FUNCTION_COV=75.6
 
 LINES_COV=`cat genhtml/index.html | grep "headerCovTableEntry.*%" | grep -oP  ">\K(\d*.\d*) " | head -n 1`
 FUNC_COV=`cat genhtml/index.html | grep "headerCovTableEntry.*%" | grep -oP  ">\K(\d*.\d*) " | tail -n 1`
diff --git a/cppclean.sh b/cppclean.sh
index 582c8525..d7b65696 100755
--- a/cppclean.sh
+++ b/cppclean.sh
@@ -42,11 +42,11 @@ if [ ${NO_WARNINGS_NOTUSED} -gt 3 ]; then
     echo "Failed probably due to unnecessary forward includes: ${NO_WARNINGS_NOTUSED}";
     exit 1;
 fi
-if [ ${NO_WARNINGS} -gt  181 ]; then
+if [ ${NO_WARNINGS} -gt  183 ]; then
     echo "Failed due to higher than allowed number of issues in code: ${NO_WARNINGS}"
     exit 1
 fi
-if [ ${NO_WARNINGS_TEST} -gt  124 ]; then
+if [ ${NO_WARNINGS_TEST} -gt  128 ]; then
     echo "Failed due to higher than allowed number of issues in test code: ${NO_WARNINGS_TEST}"
     cat ${CPPCLEAN_RESULTS_FILE_TEST}
     exit 1
diff --git a/docs/parameters.md b/docs/parameters.md
index 93f24f8d..9d37ab3a 100644
--- a/docs/parameters.md
+++ b/docs/parameters.md
@@ -33,7 +33,7 @@ Configuration options for the server are defined only via command-line options a
 | `rest_workers` | `integer` | Number of HTTP server threads. Effective when `rest_port` > 0. Default value is set based on the number of CPUs. |
 | `file_system_poll_wait_seconds` | `integer` | Time interval between config and model versions changes detection in seconds. Default value is 1. Zero value disables changes monitoring. |
 | `sequence_cleaner_poll_wait_minutes` | `integer` | Time interval (in minutes) between next sequence cleaner scans. Sequences of the models that are subjects to idle sequence cleanup that have been inactive since the last scan are removed. Zero value disables sequence cleaner. See [idle sequence cleanup](stateful_models.md). |
-| `custom_node_resources_cleaner_interval` | `integer` | Time interval (in seconds) between two consecutive resources cleanup scans. Default is 1. Must be greater than 0. See [custom node development](custom_node_development.md). |
+| `custom_node_resources_cleaner_interval_seconds` | `integer` | Time interval (in seconds) between two consecutive resources cleanup scans. Default is 1. Must be greater than 0. See [custom node development](custom_node_development.md). |
 | `cpu_extension` | `string` | Optional path to a library with [custom layers implementation](https://docs.openvino.ai/2022.2/openvino_docs_Extensibility_UG_Intro.html). |
 | `log_level` | `"DEBUG"/"INFO"/"ERROR"` | Serving logging level |
 | `log_path` | `string` | Optional path to the log file. |
diff --git a/src/BUILD b/src/BUILD
index c155c576..ac02cbec 100644
--- a/src/BUILD
+++ b/src/BUILD
@@ -96,6 +96,8 @@ cc_library(
         "azurefilesystem.hpp",
         "buffer.cpp",
         "buffer.hpp",
+        "capi_frontend/capi_utils.cpp",
+        "capi_frontend/capi_utils.hpp",
         "cleaner_utils.cpp",
         "cleaner_utils.hpp",
         "cli_parser.cpp",
@@ -235,8 +237,6 @@ cc_library(
         "pipelineeventqueue.hpp",
         "pipeline_factory.cpp",
         "pipeline_factory.hpp",
-        "poc_api_impl.cpp",
-        "poc_api_impl.hpp",
         "pocapi.hpp",
         "pocapiinternal.cpp",
         "pocapiinternal.hpp",
@@ -265,6 +265,7 @@ cc_library(
         "serialization.hpp",
         "servablemanagermodule.cpp",
         "servablemanagermodule.hpp",
+        "server_options.hpp",
         "server.cpp",
         "server.hpp",
         "session_id.hpp",
@@ -537,25 +538,6 @@ cc_binary(
     ]
 )
 
-cc_binary(
-    name = "poc",
-    srcs = [
-        "main2.cpp",
-    ],
-    linkopts = [
-        "-lxml2",
-        "-luuid",
-        "-lstdc++fs",
-        "-lcrypto",
-    ],
-    copts = [
-    ],
-    deps = [
-        "//src:ovms_lib",
-    ],
-    linkstatic = True,
-)
-
 # POC which can start OVMS with C-API built by bazel
 # Standalone example for building outside of bazel (with bare g++ is inside MakefileCapi)
 cc_binary(
@@ -577,6 +559,19 @@ cc_binary(
     linkstatic = True,
 )
 
+cc_import(
+  name = "shared_lib",
+  hdrs = ["pocapi.hpp"],
+  shared_library = "libovms_shared.so",
+)
+
+cc_library(
+    name = "cpp_headers",
+    srcs = ["server.hpp",
+        "module.hpp",],
+    deps = ["//src:shared_lib"]
+)
+
 cc_binary(
     name = "ovms",
     srcs = [
@@ -587,14 +582,13 @@ cc_binary(
         "-luuid",
         "-lstdc++fs",
         "-lcrypto",
+        "-lovms_shared"
     ],
     copts = [
         "-Wconversion",
         "-Werror",
     ],
-    deps = [
-        "//src:ovms_lib",
-    ],
+    deps = ["//src:cpp_headers"],
     linkstatic = False,
 )
 
@@ -646,6 +640,7 @@ cc_test(
         "test/ovinferrequestqueue_test.cpp",
         "test/ov_utils_test.cpp",
         "test/pipelinedefinitionstatus_test.cpp",
+        "test/capi_predict_validation_test.cpp",
         "test/predict_validation_test.cpp",
         "test/prediction_service_test.cpp",
         "test/tfs_rest_parser_row_test.cpp",
diff --git a/src/buffer.cpp b/src/buffer.cpp
index b08b895c..fce2841f 100644
--- a/src/buffer.cpp
+++ b/src/buffer.cpp
@@ -17,8 +17,6 @@
 
 #include <cstring>
 
-#include "logging.hpp"
-
 namespace ovms {
 Buffer::Buffer(const void* pptr, size_t byteSize, BufferType bufferType, std::optional<uint32_t> bufferDeviceId, bool createCopy) :
     ptr(createCopy ? nullptr : pptr),
@@ -32,11 +30,22 @@ Buffer::Buffer(const void* pptr, size_t byteSize, BufferType bufferType, std::op
     ownedCopy = std::make_unique<char[]>(byteSize);
     std::memcpy(ownedCopy.get(), pptr, byteSize);
 }
+
 const void* Buffer::data() const {
     return (ptr != nullptr) ? ptr : ownedCopy.get();
 }
+
 size_t Buffer::getByteSize() const {
     return byteSize;
 }
+
+BufferType Buffer::getBufferType() const {
+    return this->bufferType;
+}
+
+const std::optional<uint32_t>& Buffer::getDeviceId() const {
+    return bufferDeviceId;
+}
+
 Buffer::~Buffer() = default;
 }  // namespace ovms
diff --git a/src/buffer.hpp b/src/buffer.hpp
index 2921638d..00776523 100644
--- a/src/buffer.hpp
+++ b/src/buffer.hpp
@@ -31,6 +31,8 @@ public:
     Buffer(const void* ptr, size_t byteSize, BufferType bufferType = OVMS_BUFFERTYPE_CPU, std::optional<uint32_t> bufferDeviceId = std::nullopt, bool createCopy = false);
     ~Buffer();
     const void* data() const;
+    BufferType getBufferType() const;
+    const std::optional<uint32_t>& getDeviceId() const;
     size_t getByteSize() const;
 };
 
diff --git a/src/poc_api_impl.cpp b/src/capi_frontend/capi_utils.cpp
similarity index 79%
rename from src/poc_api_impl.cpp
rename to src/capi_frontend/capi_utils.cpp
index 7411448c..db89d15e 100644
--- a/src/poc_api_impl.cpp
+++ b/src/capi_frontend/capi_utils.cpp
@@ -13,15 +13,16 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 //*****************************************************************************
-#include "poc_api_impl.hpp"
+#include "capi_utils.hpp"
 
-#include "server.hpp"
+#include <string>
+
+#include "../shape.hpp"
 
 namespace ovms {
 
-int ServerImpl::start(GeneralOptionsImpl* go, MultiModelOptionsImpl* mmo) {
-    Server& server = Server::instance();
-    return server.start(go, mmo);
+std::string tensorShapeToString(const Shape& shape) {
+    return shape.toString();
 }
 
 }  // namespace ovms
diff --git a/src/main2.cpp b/src/capi_frontend/capi_utils.hpp
similarity index 52%
rename from src/main2.cpp
rename to src/capi_frontend/capi_utils.hpp
index cda8af69..d30158d2 100644
--- a/src/main2.cpp
+++ b/src/capi_frontend/capi_utils.hpp
@@ -13,26 +13,11 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 //*****************************************************************************
-#include <iostream>
-#include <thread>
+#pragma once
+#include <string>
 
-#include "pocapi.hpp"
+namespace ovms {
+class Shape;
+std::string tensorShapeToString(const Shape& tensorShape);
 
-int main(int argc, char** argv) {
-    std::thread t([&argv, &argc]() {
-        OVMS_Start(argc, argv);
-    });
-    std::this_thread::sleep_for(std::chrono::milliseconds(1000));
-    // get model instance and have a lock on reload
-    float a[10] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 11};
-    float b[10] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10};
-    OVMS_Infer((char*)"dummy", a, b);
-    for (int i = 0; i < 10; ++i) {
-        std::cout << b[i] << " ";
-    }
-    std::cout << std::endl;
-    std::cout << __LINE__ << "FINISHED, press ctrl+c to stop " << std::endl;
-    t.join();
-    std::cout << __LINE__ << "FINISHED" << std::endl;
-    return 0;
-}
+}  // namespace ovms
diff --git a/src/cli_parser.cpp b/src/cli_parser.cpp
index 41887850..53d55ca0 100644
--- a/src/cli_parser.cpp
+++ b/src/cli_parser.cpp
@@ -20,7 +20,7 @@
 
 #include <sysexits.h>
 
-#include "poc_api_impl.hpp"
+#include "server_options.hpp"
 #include "version.hpp"
 
 namespace ovms {
@@ -37,7 +37,7 @@ void CLIParser::parse(int argc, char** argv) {
                 "Show binary version")
             ("port",
                 "gRPC server port",
-                cxxopts::value<uint64_t>()->default_value("9178"),
+                cxxopts::value<uint32_t>()->default_value("9178"),
                 "PORT")
             ("grpc_bind_address",
                 "Network interface address to bind to for the gRPC API",
@@ -45,7 +45,7 @@ void CLIParser::parse(int argc, char** argv) {
                 "GRPC_BIND_ADDRESS")
             ("rest_port",
                 "REST server port, the REST server will not be started if rest_port is blank or set to 0",
-                cxxopts::value<uint64_t>()->default_value("0"),
+                cxxopts::value<uint32_t>()->default_value("0"),
                 "REST_PORT")
             ("rest_bind_address",
                 "Network interface address to bind to for the REST API",
@@ -53,11 +53,11 @@ void CLIParser::parse(int argc, char** argv) {
                 "REST_BIND_ADDRESS")
             ("grpc_workers",
                 "Number of gRPC servers. Default 1. Increase for multi client, high throughput scenarios",
-                cxxopts::value<uint>()->default_value("1"),
+                cxxopts::value<uint32_t>()->default_value("1"),
                 "GRPC_WORKERS")
             ("rest_workers",
                 "Number of worker threads in REST server - has no effect if rest_port is not set. Default value depends on number of CPUs. ",
-                cxxopts::value<uint>(),
+                cxxopts::value<uint32_t>(),
                 "REST_WORKERS")
             ("log_level",
                 "serving log level - one of TRACE, DEBUG, INFO, WARNING, ERROR",
@@ -75,16 +75,16 @@ void CLIParser::parse(int argc, char** argv) {
                 cxxopts::value<std::string>(), "GRPC_CHANNEL_ARGUMENTS")
             ("file_system_poll_wait_seconds",
                 "Time interval between config and model versions changes detection. Default is 1. Zero or negative value disables changes monitoring.",
-                cxxopts::value<uint>()->default_value("1"),
+                cxxopts::value<uint32_t>()->default_value("1"),
                 "FILE_SYSTEM_POLL_WAIT_SECONDS")
             ("sequence_cleaner_poll_wait_minutes",
                 "Time interval between two consecutive sequence cleanup scans. Default is 5. Zero value disables sequence cleaner.",
                 cxxopts::value<uint32_t>()->default_value("5"),
                 "SEQUENCE_CLEANER_POLL_WAIT_MINUTES")
-            ("custom_node_resources_cleaner_interval",
+            ("custom_node_resources_cleaner_interval_seconds",
                 "Time interval between two consecutive resources cleanup scans. Default is 1. Must be greater than 0.",
                 cxxopts::value<uint32_t>()->default_value("1"),
-                "CUSTOM_NODE_RESOURCES_CLEANER_INTERVAL")
+                "CUSTOM_NODE_RESOURCES_CLEANER_INTERVAL_SECONDS")
             ("cache_dir",
                 "Overrides model cache directory. By default cache files are saved into /opt/cache if the directory is present. When enabled, first model load will produce cache files.",
                 cxxopts::value<std::string>(),
@@ -182,8 +182,8 @@ void CLIParser::parse(int argc, char** argv) {
 }
 
 void CLIParser::prepare(GeneralOptionsImpl* go, MultiModelOptionsImpl* mmo) {
-    go->grpcPort = result->operator[]("port").as<uint64_t>();
-    go->restPort = result->operator[]("rest_port").as<uint64_t>();
+    go->grpcPort = result->operator[]("port").as<uint32_t>();
+    go->restPort = result->operator[]("rest_port").as<uint32_t>();
 
     if (result->count("model_name"))
         mmo->modelName = result->operator[]("model_name").as<std::string>();
@@ -203,10 +203,10 @@ void CLIParser::prepare(GeneralOptionsImpl* go, MultiModelOptionsImpl* mmo) {
     if (result->count("rest_bind_address"))
         go->restBindAddress = result->operator[]("rest_bind_address").as<std::string>();
 
-    go->grpcWorkers = result->operator[]("grpc_workers").as<uint>();
+    go->grpcWorkers = result->operator[]("grpc_workers").as<uint32_t>();
 
     if (result->count("rest_workers"))
-        go->restWorkers = result->operator[]("rest_workers").as<uint>();
+        go->restWorkers = result->operator[]("rest_workers").as<uint32_t>();
 
     if (result->count("batch_size"))
         mmo->batchSize = result->operator[]("batch_size").as<std::string>();
@@ -254,9 +254,9 @@ void CLIParser::prepare(GeneralOptionsImpl* go, MultiModelOptionsImpl* mmo) {
     if (result->count("grpc_channel_arguments"))
         go->grpcChannelArguments = result->operator[]("grpc_channel_arguments").as<std::string>();
 
-    go->filesystemPollWaitSeconds = result->operator[]("file_system_poll_wait_seconds").as<uint>();
+    go->filesystemPollWaitSeconds = result->operator[]("file_system_poll_wait_seconds").as<uint32_t>();
     go->sequenceCleanerPollWaitMinutes = result->operator[]("sequence_cleaner_poll_wait_minutes").as<uint32_t>();
-    go->resourcesCleanerPollWaitSeconds = result->operator[]("custom_node_resources_cleaner_interval").as<uint32_t>();
+    go->resourcesCleanerPollWaitSeconds = result->operator[]("custom_node_resources_cleaner_interval_seconds").as<uint32_t>();
 
     if (result != nullptr && result->count("cache_dir")) {
         go->cacheDir = result->operator[]("cache_dir").as<std::string>();
diff --git a/src/cli_parser.hpp b/src/cli_parser.hpp
index fb54d76e..5d50fae1 100644
--- a/src/cli_parser.hpp
+++ b/src/cli_parser.hpp
@@ -22,7 +22,6 @@
 namespace ovms {
 
 struct GeneralOptionsImpl;
-struct SingleModelOptionsImpl;
 struct MultiModelOptionsImpl;
 
 class CLIParser {
diff --git a/src/config.cpp b/src/config.cpp
index 695c62b0..f5136547 100644
--- a/src/config.cpp
+++ b/src/config.cpp
@@ -24,7 +24,7 @@
 
 #include "cli_parser.hpp"
 #include "modelconfig.hpp"
-#include "poc_api_impl.hpp"
+#include "server_options.hpp"
 
 namespace ovms {
 
@@ -173,13 +173,13 @@ bool Config::validate() {
 }
 
 const std::string& Config::configPath() const { return this->mmo.configPath; }
-uint64_t Config::port() const { return this->go.grpcPort; }
+uint32_t Config::port() const { return this->go.grpcPort; }
 const std::string Config::cpuExtensionLibraryPath() const { return this->go.cpuExtensionLibraryPath; }
 const std::string Config::grpcBindAddress() const { return this->go.grpcBindAddress; }
-uint64_t Config::restPort() const { return this->go.restPort; }
+uint32_t Config::restPort() const { return this->go.restPort; }
 const std::string Config::restBindAddress() const { return this->go.restBindAddress; }
-uint Config::grpcWorkers() const { return this->go.grpcWorkers; }
-uint Config::restWorkers() const { return this->go.restWorkers.value_or(DEFAULT_REST_WORKERS); }
+uint32_t Config::grpcWorkers() const { return this->go.grpcWorkers; }
+uint32_t Config::restWorkers() const { return this->go.restWorkers.value_or(DEFAULT_REST_WORKERS); }
 const std::string& Config::modelName() const { return this->mmo.modelName; }
 const std::string& Config::modelPath() const { return this->mmo.modelPath; }
 const std::string& Config::batchSize() const {
@@ -207,7 +207,7 @@ const std::string& Config::logPath() const { return this->go.logPath; }
 const std::string& Config::tracePath() const { return this->go.tracePath; }
 #endif
 const std::string& Config::grpcChannelArguments() const { return this->go.grpcChannelArguments; }
-uint Config::filesystemPollWaitSeconds() const { return this->go.filesystemPollWaitSeconds; }
+uint32_t Config::filesystemPollWaitSeconds() const { return this->go.filesystemPollWaitSeconds; }
 uint32_t Config::sequenceCleanerPollWaitMinutes() const { return this->go.sequenceCleanerPollWaitMinutes; }
 uint32_t Config::resourcesCleanerPollWaitSeconds() const { return this->go.resourcesCleanerPollWaitSeconds; }
 const std::string Config::cacheDir() const { return this->go.cacheDir; }
diff --git a/src/config.hpp b/src/config.hpp
index 5dbd5f01..a186d84a 100644
--- a/src/config.hpp
+++ b/src/config.hpp
@@ -18,7 +18,7 @@
 #include <optional>
 #include <string>
 
-#include "poc_api_impl.hpp"
+#include "server_options.hpp"
 
 namespace ovms {
 
@@ -91,9 +91,9 @@ public:
     /**
          * @brief Gets the grpc port
          * 
-         * @return uint64_t
+         * @return uint32_t
          */
-    uint64_t port() const;
+    uint32_t port() const;
 
     /**
          * @brief Get the gRPC network interface address to bind to
@@ -112,9 +112,9 @@ public:
     /**
          * @brief Gets the REST port
          * 
-         * @return uint64_t
+         * @return uint32_t
          */
-    uint64_t restPort() const;
+    uint32_t restPort() const;
 
     /**
          * @brief Get the rest network interface address to bind to
@@ -128,14 +128,14 @@ public:
          * 
          * @return uint
          */
-    uint grpcWorkers() const;
+    uint32_t grpcWorkers() const;
 
     /**
          * @brief Gets the rest workers count
          * 
          * @return uint
          */
-    uint restWorkers() const;
+    uint32_t restWorkers() const;
 
     /**
          * @brief Get the model name
@@ -277,7 +277,7 @@ public:
      * 
      * @return uint 
      */
-    uint filesystemPollWaitSeconds() const;
+    uint32_t filesystemPollWaitSeconds() const;
 
     /**
      * @brief Get the sequence cleanup poll wait time in minutes
diff --git a/src/exit_node.cpp b/src/exit_node.cpp
index e8db9b74..7de469b7 100644
--- a/src/exit_node.cpp
+++ b/src/exit_node.cpp
@@ -62,7 +62,10 @@ Status OutputGetter<const TensorMap&>::get(const std::string& name, ov::Tensor&
 template <typename ResponseType>
 Status ExitNode<ResponseType>::fetchResults(const TensorMap& inputTensors) {
     OutputGetter<const TensorMap&> outputGetter(inputTensors);
-    return serializePredictResponse(outputGetter, this->outputsInfo, this->response, getOutputMapKeyName, useSharedOutputContent);
+    // TODO fix name version (it does not work properly anyway right now)
+    static const std::string name{""};
+    static const model_version_t version{1};
+    return serializePredictResponse(outputGetter, name, version, this->outputsInfo, this->response, getOutputMapKeyName);
 }
 
 template <typename ResponseType>
diff --git a/src/grpc_utils.cpp b/src/grpc_utils.cpp
index b05fa8d1..d6ea5942 100644
--- a/src/grpc_utils.cpp
+++ b/src/grpc_utils.cpp
@@ -68,6 +68,8 @@ const grpc::Status grpc(const Status& status) {
         {StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, grpc::StatusCode::INVALID_ARGUMENT},
         {StatusCode::INVALID_BATCH_SIZE, grpc::StatusCode::INVALID_ARGUMENT},
         {StatusCode::INVALID_SHAPE, grpc::StatusCode::INVALID_ARGUMENT},
+        {StatusCode::INVALID_BUFFER_TYPE, grpc::StatusCode::INVALID_ARGUMENT},
+        {StatusCode::INVALID_DEVICE_ID, grpc::StatusCode::INVALID_ARGUMENT},
         {StatusCode::INVALID_PRECISION, grpc::StatusCode::INVALID_ARGUMENT},
         {StatusCode::INVALID_VALUE_COUNT, grpc::StatusCode::INVALID_ARGUMENT},
         {StatusCode::INVALID_CONTENT_SIZE, grpc::StatusCode::INVALID_ARGUMENT},
diff --git a/src/http_server.cpp b/src/http_server.cpp
index 9652aee8..6a759c84 100644
--- a/src/http_server.cpp
+++ b/src/http_server.cpp
@@ -110,6 +110,8 @@ const net_http::HTTPStatusCode http(const ovms::Status& status) {
         {StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, net_http::HTTPStatusCode::BAD_REQUEST},
         {StatusCode::INVALID_BATCH_SIZE, net_http::HTTPStatusCode::BAD_REQUEST},
         {StatusCode::INVALID_SHAPE, net_http::HTTPStatusCode::BAD_REQUEST},
+        {StatusCode::INVALID_BUFFER_TYPE, net_http::HTTPStatusCode::BAD_REQUEST},
+        {StatusCode::INVALID_DEVICE_ID, net_http::HTTPStatusCode::BAD_REQUEST},
         {StatusCode::INVALID_PRECISION, net_http::HTTPStatusCode::BAD_REQUEST},
         {StatusCode::INVALID_VALUE_COUNT, net_http::HTTPStatusCode::BAD_REQUEST},
         {StatusCode::INVALID_CONTENT_SIZE, net_http::HTTPStatusCode::BAD_REQUEST},
diff --git a/src/inferenceparameter.cpp b/src/inferenceparameter.cpp
index 5044aa9b..bb5a8513 100644
--- a/src/inferenceparameter.cpp
+++ b/src/inferenceparameter.cpp
@@ -13,6 +13,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 //*****************************************************************************
+
 #include "inferenceparameter.hpp"
 
 #include <stdexcept>
@@ -21,32 +22,46 @@
 namespace ovms {
 // TODO should we own our own copy of value?
 //
-static size_t DataTypeToByteSize(DataType datatype) {
-    switch (datatype) {
-    case OVMS_DATATYPE_FP32:
-    case OVMS_DATATYPE_I32:
-    case OVMS_DATATYPE_U32:
-        return 4;
-    default:
-        throw std::invalid_argument("Unsupported");
+size_t DataTypeToByteSize(OVMS_DataType datatype) {
+    static std::unordered_map<OVMS_DataType, size_t> datatypeSizeMap{
+        {OVMS_DATATYPE_BOOL, 1},
+        {OVMS_DATATYPE_U1, 1},
+        {OVMS_DATATYPE_U4, 1},
+        {OVMS_DATATYPE_U8, 1},
+        {OVMS_DATATYPE_U16, 2},
+        {OVMS_DATATYPE_U32, 4},
+        {OVMS_DATATYPE_U64, 8},
+        {OVMS_DATATYPE_I4, 1},
+        {OVMS_DATATYPE_I8, 1},
+        {OVMS_DATATYPE_I16, 2},
+        {OVMS_DATATYPE_I32, 4},
+        {OVMS_DATATYPE_I64, 8},
+        {OVMS_DATATYPE_FP16, 2},
+        {OVMS_DATATYPE_FP32, 4},
+        {OVMS_DATATYPE_FP64, 8},
+        {OVMS_DATATYPE_BF16, 2},
+        // {"BYTES", }, TODO
+    };
+    auto it = datatypeSizeMap.find(datatype);
+    if (it == datatypeSizeMap.end()) {
         return 0;
     }
+    return it->second;
 }
-InferenceParameter::InferenceParameter(const char* name, DataType datatype, const void* data) :
+
+InferenceParameter::InferenceParameter(const char* name, OVMS_DataType datatype, const void* data) :
     name(name),
     datatype(datatype),
     data(reinterpret_cast<const char*>(data), DataTypeToByteSize(datatype)) {
 }
-//   InferenceParameter::InferenceParameter(const char* name, DataType datatype, const void* data, size_t byteSize);
+
 const std::string& InferenceParameter::getName() const {
     return this->name;
 }
-DataType InferenceParameter::getDataType() const {
+OVMS_DataType InferenceParameter::getDataType() const {
     return this->datatype;
 }
-size_t InferenceParameter::getByteSize() const {
-    return this->data.size();
-}
+
 const void* InferenceParameter::getData() const {
     return reinterpret_cast<const void*>(this->data.c_str());
 }
diff --git a/src/inferenceparameter.hpp b/src/inferenceparameter.hpp
index 204dab51..c899009c 100644
--- a/src/inferenceparameter.hpp
+++ b/src/inferenceparameter.hpp
@@ -15,23 +15,24 @@
 // limitations under the License.
 //*****************************************************************************
 #include <string>
+#include <unordered_map>
 
 #include "pocapi.hpp"
 
 namespace ovms {
+size_t DataTypeToByteSize(OVMS_DataType datatype);
 
 // TODO should we own our own copy of value?
 class InferenceParameter {
     const std::string name;
-    DataType datatype;
+    OVMS_DataType datatype;
     const std::string data;
 
 public:
-    InferenceParameter(const char* name, DataType datatype, const void* data);
-    InferenceParameter(const char* name, DataType datatype, const void* data, size_t byteSize);
+    InferenceParameter(const char* name, OVMS_DataType datatype, const void* data);
+    InferenceParameter(const char* name, OVMS_DataType datatype, const void* data, size_t byteSize);
     const std::string& getName() const;
-    DataType getDataType() const;
-    size_t getByteSize() const;
+    OVMS_DataType getDataType() const;
     const void* getData() const;
 };
 }  // namespace ovms
diff --git a/src/inferencerequest.cpp b/src/inferencerequest.cpp
index e58c9dd5..b1129c20 100644
--- a/src/inferencerequest.cpp
+++ b/src/inferencerequest.cpp
@@ -17,7 +17,9 @@
 
 #include "status.hpp"
 namespace ovms {
-
+// this constructor can be removed with prediction tests overhaul
+InferenceRequest::InferenceRequest() :
+    InferenceRequest("CONSTRUCTOR_USED_ONLY_IN_PREDICTION_TESTS", 42) {}
 InferenceRequest::InferenceRequest(const char* servableName, model_version_t servableVersion) :
     servableName(servableName),
     servableVersion(servableVersion) {
@@ -29,7 +31,7 @@ const std::string& InferenceRequest::getServableName() const {
 model_version_t InferenceRequest::getServableVersion() const {
     return this->servableVersion;
 }
-Status InferenceRequest::addInput(const char* name, DataType datatype, const size_t* shape, size_t dimCount) {
+Status InferenceRequest::addInput(const char* name, OVMS_DataType datatype, const size_t* shape, size_t dimCount) {
     auto [it, emplaced] = inputs.emplace(name, InferenceTensor{datatype, shape, dimCount});
     return emplaced ? StatusCode::OK : StatusCode::DOUBLE_TENSOR_INSERT;
 }
@@ -60,6 +62,9 @@ Status InferenceRequest::getInput(const char* name, const InferenceTensor** tens
     *tensor = &it->second;
     return StatusCode::OK;
 }
+uint64_t InferenceRequest::getInputsSize() const {
+    return inputs.size();
+}
 Status InferenceRequest::removeInput(const char* name) {
     auto count = inputs.erase(name);
     if (count) {
@@ -67,7 +72,7 @@ Status InferenceRequest::removeInput(const char* name) {
     }
     return StatusCode::NONEXISTENT_TENSOR_FOR_REMOVAL;
 }
-Status InferenceRequest::addParameter(const char* parameterName, DataType datatype, const void* data) {
+Status InferenceRequest::addParameter(const char* parameterName, OVMS_DataType datatype, const void* data) {
     auto [it, emplaced] = parameters.emplace(parameterName, InferenceParameter{parameterName, datatype, data});
     return emplaced ? StatusCode::OK : StatusCode::DOUBLE_PARAMETER_INSERT;
 }
@@ -84,4 +89,24 @@ const InferenceParameter* InferenceRequest::getParameter(const char* name) const
         return &it->second;
     return nullptr;
 }
+Status InferenceRequest::getBatchSize(size_t& batchSize, size_t batchSizeIndex) const {
+    if (inputs.size() == 0) {
+        return StatusCode::INTERNAL_ERROR;  // TODO test
+    }
+    // we make here the same assumption as with bs=auto in TFS/KFS API
+    const InferenceTensor& tensor = inputs.begin()->second;
+    const auto& shape = tensor.getShape();
+    if (batchSizeIndex >= shape.size()) {
+        return StatusCode::INTERNAL_ERROR;  // TODO test
+    }
+    batchSize = shape[batchSizeIndex];
+    return StatusCode::OK;
+}
+std::map<std::string, shape_t> InferenceRequest::getRequestShapes() const {
+    std::map<std::string, shape_t> result;
+    for (auto& [name, tensor] : inputs) {
+        result.emplace(name, tensor.getShape());
+    }
+    return result;
+}
 }  // namespace ovms
diff --git a/src/inferencerequest.hpp b/src/inferencerequest.hpp
index 9686355c..b5e8afe4 100644
--- a/src/inferencerequest.hpp
+++ b/src/inferencerequest.hpp
@@ -14,6 +14,7 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 //*****************************************************************************
+#include <map>
 #include <string>
 #include <unordered_map>
 
@@ -33,15 +34,18 @@ class InferenceRequest {
     std::unordered_map<std::string, InferenceTensor> inputs;
 
 public:
+    // this constructor can be removed with prediction tests overhaul
+    InferenceRequest();
     InferenceRequest(const char* modelName, model_version_t modelVersion);
-    Status addInput(const char* name, DataType datatype, const size_t* shape, size_t dimCount);
+    Status addInput(const char* name, OVMS_DataType datatype, const size_t* shape, size_t dimCount);
     Status getInput(const char* name, const InferenceTensor** tensor) const;
+    uint64_t getInputsSize() const;
     Status removeInput(const char* name);
     Status removeAllInputs();
 
     Status setInputBuffer(const char* name, const void* addr, size_t byteSize, BufferType, std::optional<uint32_t> deviceId);
     Status removeInputBuffer(const char* name);
-    Status addParameter(const char* parameterName, DataType datatype, const void* data);
+    Status addParameter(const char* parameterName, OVMS_DataType datatype, const void* data);
     Status removeParameter(const char* parameterName);
     const InferenceParameter* getParameter(const char* name) const;
 
@@ -57,5 +61,8 @@ public:
     Status setTimeoutMicorseconds(uint64_t microseconds);
     InferenceParameter* getInferenceParameter(const char* name);
     InferenceTensor* getTensor(const char* name);
+    // TODO add tests & seek if those can be removed by potentialy exposing inputs map?
+    Status getBatchSize(size_t& batchSize, size_t batchSizeIndex) const;
+    std::map<std::string, shape_t> getRequestShapes() const;
 };
 }  // namespace ovms
diff --git a/src/inferenceresponse.cpp b/src/inferenceresponse.cpp
index 08d48c20..268281ea 100644
--- a/src/inferenceresponse.cpp
+++ b/src/inferenceresponse.cpp
@@ -15,8 +15,10 @@
 //*****************************************************************************
 #include "inferenceresponse.hpp"
 
+#include <algorithm>
 #include <string>
 #include <unordered_map>
+#include <utility>
 
 #include "inferenceparameter.hpp"
 #include "inferencetensor.hpp"
@@ -24,38 +26,80 @@
 #include "status.hpp"
 
 namespace ovms {
+// this constructor can be removed with prediction tests overhaul
+InferenceResponse::InferenceResponse() :
+    InferenceResponse("CONSTRUCTOR_USED_ONLY_IN_PREDICTION_TESTS", 42) {}
 InferenceResponse::InferenceResponse(const std::string& servableName, model_version_t servableVersion) :
     servableName(servableName),
     servableVersion(servableVersion) {}
 const std::string& InferenceResponse::getServableName() const {
     return this->servableName;
 }
+
 model_version_t InferenceResponse::getServableVersion() const {
     return this->servableVersion;
 }
-Status InferenceResponse::addOutput(const std::string& name, DataType datatype, const size_t* shape, size_t dimCount) {
+
+Status InferenceResponse::addOutput(const std::string& name, OVMS_DataType datatype, const size_t* shape, size_t dimCount) {
     // TODO insert tensor with wrong shape/datatype/name/dimcount
-    // TODO reuse infer response/request
-    auto [it, emplaced] = outputs.emplace(name, InferenceTensor{datatype, shape, dimCount});
-    return emplaced ? StatusCode::OK : StatusCode::DOUBLE_TENSOR_INSERT;
+    auto it = std::find_if(outputs.begin(),
+        outputs.end(),
+        [&name](const std::pair<std::string, InferenceTensor>& pair) {
+            return name == pair.first;
+        });
+    if (outputs.end() != it) {
+        return StatusCode::DOUBLE_TENSOR_INSERT;
+    }
+
+    auto pair = std::pair<std::string, InferenceTensor>(name, InferenceTensor{datatype, shape, dimCount});
+    outputs.push_back(std::move(pair));
+    return StatusCode::OK;
 }
-Status InferenceResponse::getOutput(const char* name, InferenceTensor** tensor) {
-    auto it = outputs.find(name);
-    if (outputs.end() == it) {
+
+Status InferenceResponse::getOutput(uint32_t id, const std::string** name, const InferenceTensor** tensor) const {
+    if (outputs.size() <= id) {
         *tensor = nullptr;
         return StatusCode::NONEXISTENT_TENSOR;
     }
-    *tensor = &it->second;
+    *name = &(outputs[id].first);
+    *tensor = &(outputs[id].second);
+    return StatusCode::OK;
+}
+
+Status InferenceResponse::getOutput(uint32_t id, const std::string** name, InferenceTensor** tensor) {
+    return const_cast<const InferenceResponse*>(this)->getOutput(id, name, const_cast<const InferenceTensor**>(tensor));
+}
+
+Status InferenceResponse::addParameter(const char* parameterName, OVMS_DataType datatype, const void* data) {
+    auto it = std::find_if(parameters.begin(),
+        parameters.end(),
+        [&parameterName](const InferenceParameter& parameter) {
+            return parameterName == parameter.getName();
+        });
+    if (parameters.end() != it) {
+        return StatusCode::DOUBLE_PARAMETER_INSERT;
+    }
+    parameters.emplace_back(parameterName, datatype, data);
     return StatusCode::OK;
 }
-Status InferenceResponse::addParameter(const char* parameterName, DataType datatype, const void* data) {
-    auto [it, emplaced] = parameters.emplace(parameterName, InferenceParameter{parameterName, datatype, data});
-    return emplaced ? StatusCode::OK : StatusCode::DOUBLE_PARAMETER_INSERT;
+
+const InferenceParameter* InferenceResponse::getParameter(uint32_t id) const {
+    if (id >= parameters.size()) {
+        return nullptr;
+    }
+    return &parameters[id];
 }
-const InferenceParameter* InferenceResponse::getParameter(const char* name) const {
-    auto it = parameters.find(name);
-    if (it != parameters.end())
-        return &it->second;
-    return nullptr;
+
+uint32_t InferenceResponse::getOutputCount() const {
+    return this->outputs.size();
+}
+
+uint32_t InferenceResponse::getParameterCount() const {
+    return this->parameters.size();
 }
+
+void InferenceResponse::Clear() {
+    outputs.clear();
+    parameters.clear();
+}  // TODO remove
 }  // namespace ovms
diff --git a/src/inferenceresponse.hpp b/src/inferenceresponse.hpp
index fa3b2f58..60ce028f 100644
--- a/src/inferenceresponse.hpp
+++ b/src/inferenceresponse.hpp
@@ -15,7 +15,8 @@
 // limitations under the License.
 //*****************************************************************************
 #include <string>
-#include <unordered_map>
+#include <utility>
+#include <vector>
 
 #include "inferenceparameter.hpp"
 #include "inferencetensor.hpp"
@@ -29,21 +30,29 @@ class Status;
 class InferenceResponse {
     const std::string& servableName;
     const model_version_t servableVersion;
-    std::unordered_map<std::string, InferenceParameter> parameters;
-    std::unordered_map<std::string, InferenceTensor> outputs;
+    std::vector<InferenceParameter> parameters;                    // TODO after benchmark app verify if additional map<int, name> wouldn't be better
+    std::vector<std::pair<std::string, InferenceTensor>> outputs;  // TODO after benchmark app verify if additional map<int, name> wouldn't be better
 
 public:
+    // this constructor can be removed with prediction tests overhaul
+    InferenceResponse();
     InferenceResponse(const std::string& servableName, model_version_t servableVersion);
-    Status addOutput(const std::string& name, DataType datatype, const size_t* shape, size_t dimCount);
-    Status getOutput(const char* name, InferenceTensor** tensor);
-    Status addParameter(const char* parameterName, DataType datatype, const void* data);
-    const InferenceParameter* getParameter(const char* name) const;
+    Status addOutput(const std::string& name, OVMS_DataType datatype, const size_t* shape, size_t dimCount);
+    Status getOutput(uint32_t id, const std::string** name, const InferenceTensor** tensor) const;  // TODO consider in the future if we need getOutput by name
+    Status getOutput(uint32_t id, const std::string** name, InferenceTensor** tensor);
+
+    Status addParameter(const char* parameterName, OVMS_DataType datatype, const void* data);
+    const InferenceParameter* getParameter(uint32_t id) const;
 
     const std::string& getServableName() const;
     model_version_t getServableVersion() const;
+    uint32_t getOutputCount() const;
+    uint32_t getParameterCount() const;
 
     Status setId();
     Status getId();
     InferenceParameter* getInferenceParameter(const char* name);
+    // TODO this can be removed with prediction tests overhaul
+    void Clear();
 };
 }  // namespace ovms
diff --git a/src/inferencetensor.cpp b/src/inferencetensor.cpp
index d3519542..7b6e79ed 100644
--- a/src/inferencetensor.cpp
+++ b/src/inferencetensor.cpp
@@ -27,17 +27,18 @@ InferenceTensor::InferenceTensor(InferenceTensor&& rhs) :
     datatype(std::move(rhs.datatype)),
     shape(std::move(rhs.shape)),
     buffer(std::move(rhs.buffer)) {}
-InferenceTensor::InferenceTensor(DataType datatype, const size_t* shape, size_t dimCount) :
+InferenceTensor::InferenceTensor(OVMS_DataType datatype, const size_t* shape, size_t dimCount) :
     datatype(datatype),
     shape(shape, shape + dimCount) {}
 Status InferenceTensor::setBuffer(const void* addr, size_t byteSize, BufferType bufferType, std::optional<uint32_t> deviceId, bool createCopy) {
     if (nullptr != buffer) {
         return StatusCode::DOUBLE_BUFFER_SET;
-    }
+    }  // TODO validate against byteSize == 0
     buffer = std::make_unique<Buffer>(addr, byteSize, bufferType, deviceId, createCopy);
     return StatusCode::OK;
 }
-DataType InferenceTensor::getDataType() const {
+
+OVMS_DataType InferenceTensor::getDataType() const {
     return this->datatype;
 }
 const shape_t& InferenceTensor::getShape() const {
diff --git a/src/inferencetensor.hpp b/src/inferencetensor.hpp
index f08306f0..113ff63d 100644
--- a/src/inferencetensor.hpp
+++ b/src/inferencetensor.hpp
@@ -26,12 +26,12 @@ class Buffer;
 class Status;
 
 class InferenceTensor {
-    const DataType datatype;
+    const OVMS_DataType datatype;
     shape_t shape;
     std::unique_ptr<Buffer> buffer;
 
 public:
-    InferenceTensor(DataType datatype, const size_t* shape, size_t dimCount);
+    InferenceTensor(OVMS_DataType datatype, const size_t* shape, size_t dimCount);
     ~InferenceTensor();
     InferenceTensor(InferenceTensor&&);
     InferenceTensor(const InferenceTensor&) = delete;
@@ -39,7 +39,7 @@ public:
     InferenceTensor& operator=(const InferenceTensor&&);
     Status setBuffer(const void* addr, size_t byteSize, BufferType bufferType, std::optional<uint32_t> deviceId, bool createCopy = false);
     Status removeBuffer();
-    DataType getDataType() const;
+    OVMS_DataType getDataType() const;
     const shape_t& getShape() const;
     const Buffer* const getBuffer() const;
 };
diff --git a/src/main3.cpp b/src/main3.cpp
index bd0ee9b3..28e0cb18 100644
--- a/src/main3.cpp
+++ b/src/main3.cpp
@@ -13,11 +13,54 @@
 // See the License for the specific language governing permissions and
 // limitations under the License.
 //*****************************************************************************
-#include <iostream>
+// #include <chrono>
+// #include <iostream>
+// #include <thread>
+
+#include <signal.h>
+#include <stdio.h>
 
 #include "pocapi.hpp"
 
+namespace {
+volatile sig_atomic_t shutdown_request = 0;
+}
+
+static void onInterrupt(int status) {
+    shutdown_request = 1;
+}
+
+static void onTerminate(int status) {
+    shutdown_request = 1;
+}
+
+static void onIllegal(int status) {
+    shutdown_request = 2;
+}
+
+static void installSignalHandlers() {
+    static struct sigaction sigIntHandler;
+    sigIntHandler.sa_handler = onInterrupt;
+    sigemptyset(&sigIntHandler.sa_mask);
+    sigIntHandler.sa_flags = 0;
+    sigaction(SIGINT, &sigIntHandler, NULL);
+
+    static struct sigaction sigTermHandler;
+    sigTermHandler.sa_handler = onTerminate;
+    sigemptyset(&sigTermHandler.sa_mask);
+    sigTermHandler.sa_flags = 0;
+    sigaction(SIGTERM, &sigTermHandler, NULL);
+
+    static struct sigaction sigIllHandler;
+    sigIllHandler.sa_handler = onIllegal;
+    sigemptyset(&sigIllHandler.sa_mask);
+    sigIllHandler.sa_flags = 0;
+    sigaction(SIGILL, &sigIllHandler, NULL);
+}
+
 int main(int argc, char** argv) {
+    installSignalHandlers();
+
     OVMS_ServerGeneralOptions* go = 0;
     OVMS_ServerMultiModelOptions* mmo = 0;
     OVMS_Server* srv;
@@ -28,18 +71,38 @@ int main(int argc, char** argv) {
 
     OVMS_ServerGeneralOptionsSetGrpcPort(go, 11337);
     OVMS_ServerGeneralOptionsSetRestPort(go, 11338);
+
+    OVMS_ServerGeneralOptionsSetLogLevel(go, OVMS_LOG_DEBUG);
     OVMS_ServerMultiModelOptionsSetConfigPath(mmo, "/ovms/src/test/c_api/config.json");
 
     OVMS_Status* res = OVMS_ServerStartFromConfigurationFile(srv, go, mmo);
 
+    if (res) {
+        // TODO: Better error handling?
+        fprintf(stderr, "Error starting the server\n");
+        OVMS_ServerDelete(srv);
+        OVMS_ServerMultiModelOptionsDelete(mmo);
+        OVMS_ServerGeneralOptionsDelete(go);
+        return 1;
+    }
+
+    fprintf(stdout, "Server ready for inference\n");
+
+    // infer 1
+    // infer 2
+    // infer 3
+
+    // Application loop if required (C++):
+    // while (shutdown_request == 0) {
+    //     std::this_thread::sleep_for(std::chrono::milliseconds(200));
+    // }
+
+    fprintf(stdout, "No more job to be done, will shut down\n");
+
     OVMS_ServerDelete(srv);
     OVMS_ServerMultiModelOptionsDelete(mmo);
     OVMS_ServerGeneralOptionsDelete(go);
 
-    if (res == 0) {
-        std::cout << "Finish with success" << std::endl;
-    } else {
-        std::cout << "Finish with fail" << std::endl;
-    }
+    fprintf(stdout, "main() exit\n");
     return 0;
 }
diff --git a/src/modelinstance.cpp b/src/modelinstance.cpp
index 6481bf0e..cc5a8b8f 100644
--- a/src/modelinstance.cpp
+++ b/src/modelinstance.cpp
@@ -54,9 +54,11 @@
 namespace {
 enum : unsigned int {
     GET_INFER_REQUEST,
+    PREPROCESS,
     DESERIALIZE,
     PREDICTION,
     SERIALIZE,
+    POSTPROCESS,
     TIMER_END
 };
 }  // namespace
@@ -1111,20 +1113,25 @@ void ModelInstance::unloadModelComponents() {
     }
 }
 
+const std::set<std::string>& ModelInstance::getOptionalInputNames() {
+    static const std::set<std::string> optionalInputNames = {};
+    return optionalInputNames;
+}
+
 template <typename RequestType>
 const Status ModelInstance::validate(const RequestType* request) {
     OVMS_PROFILE_FUNCTION();
-    static const std::set<std::string> optionalInputNames = {};
     return request_validation_utils::validate(
         *request,
         getInputsInfo(),
         getName(),
         getVersion(),
-        optionalInputNames,
+        this->getOptionalInputNames(),
         getModelConfig().getBatchingMode(),
         getModelConfig().getShapes());
 }
 
+template const Status ModelInstance::validate(const InferenceRequest* request);
 template const Status ModelInstance::validate(const ::KFSRequest* request);
 template const Status ModelInstance::validate(const tensorflow::serving::PredictRequest* request);
 
@@ -1154,57 +1161,28 @@ Status ModelInstance::performInference(ov::InferRequest& inferRequest) {
     return StatusCode::OK;
 }
 
-#include <cstring>
-Status ModelInstance::infer(float* data, float* output) {
-    OVMS_PROFILE_FUNCTION();
-    Timer<TIMER_END> timer;
-    using std::chrono::microseconds;
-    timer.start(GET_INFER_REQUEST);
-    ExecutingStreamIdGuard executingStreamIdGuard(getInferRequestsQueue(), this->getMetricReporter());
-    int executingInferId = executingStreamIdGuard.getId();
-    ov::InferRequest& inferRequest = executingStreamIdGuard.getInferRequest();
-    timer.stop(GET_INFER_REQUEST);
-    SPDLOG_DEBUG("Getting infer req duration in model {}, version {}, nireq {}: {:.3f} ms",
-        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(GET_INFER_REQUEST) / 1000);
-    timer.start(DESERIALIZE);
-    static ov::Shape shape{1, 10};
-    ov::element::Type precision = ov::element::Type_t::f32;
-    ov::Tensor tensor(precision,
-        shape,
-        (void*)data);
-    inferRequest.set_tensor("b", tensor);
-    timer.stop(DESERIALIZE);
-    SPDLOG_DEBUG("Deserialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(DESERIALIZE) / 1000);
-    timer.start(PREDICTION);
-    auto status = performInference(inferRequest);
-    timer.stop(PREDICTION);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Prediction duration in model {}, version {}, nireq {}: {:.3f} ms",
-        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREDICTION) / 1000);
-    timer.start(SERIALIZE);
-    auto otensor = inferRequest.get_tensor("a");
-    std::memcpy((void*)output, otensor.data(), otensor.get_byte_size());
-    timer.stop(SERIALIZE);
-    SPDLOG_DEBUG("Serialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(SERIALIZE) / 1000);
-    return StatusCode::OK;
-}
-
-Status ModelInstance::infer(const tensorflow::serving::PredictRequest* requestProto,
-    tensorflow::serving::PredictResponse* responseProto,
+template <typename RequestType, typename ResponseType>
+Status ModelInstance::infer(const RequestType* requestProto,
+    ResponseType* responseProto,
     std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr) {
     OVMS_PROFILE_FUNCTION();
     Timer<TIMER_END> timer;
     using std::chrono::microseconds;
 
-    auto status = validate(requestProto);
+    auto requestProcessor = createRequestProcessor(requestProto, responseProto);  // request, response passed only to deduce type
+    auto status = requestProcessor->extractRequestParameters(requestProto);
+    if (!status.ok())
+        return status;
+    status = validate(requestProto);
     auto requestBatchSize = getRequestBatchSize(requestProto, this->getBatchSizeIndex());
     auto requestShapes = getRequestShapes(requestProto);
     status = reloadModelIfRequired(status, requestBatchSize, requestShapes, modelUnloadGuardPtr);
     if (!status.ok())
         return status;
+    status = requestProcessor->prepare();
+    if (!status.ok())
+        return status;
+
     timer.start(GET_INFER_REQUEST);
     OVMS_PROFILE_SYNC_BEGIN("getInferRequest");
     ExecutingStreamIdGuard executingStreamIdGuard(getInferRequestsQueue(), this->getMetricReporter());
@@ -1215,61 +1193,15 @@ Status ModelInstance::infer(const tensorflow::serving::PredictRequest* requestPr
     double getInferRequestTime = timer.elapsed<microseconds>(GET_INFER_REQUEST);
     OBSERVE_IF_ENABLED(this->getMetricReporter().waitForInferReqTime, getInferRequestTime);
     SPDLOG_DEBUG("Getting infer req duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, getInferRequestTime / 1000);
-
-    timer.start(DESERIALIZE);
-    InputSink<ov::InferRequest&> inputSink(inferRequest);
-    bool isPipeline = false;
-    status = deserializePredictRequest<ConcreteTensorProtoDeserializator>(*requestProto, getInputsInfo(), inputSink, isPipeline);
-    timer.stop(DESERIALIZE);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Deserialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(DESERIALIZE) / 1000);
+        getName(), getVersion(), executingInferId, getInferRequestTime / 1000);
 
-    timer.start(PREDICTION);
-    status = performInference(inferRequest);
-    timer.stop(PREDICTION);
+    timer.start(PREPROCESS);
+    status = requestProcessor->preInferenceProcessing(inferRequest);
+    timer.stop(PREPROCESS);
     if (!status.ok())
         return status;
-    SPDLOG_DEBUG("Prediction duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREDICTION) / 1000);
-
-    timer.start(SERIALIZE);
-    OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    status = serializePredictResponse(outputGetter, getOutputsInfo(), responseProto, getTensorInfoName);
-    timer.stop(SERIALIZE);
-    if (!status.ok())
-        return status;
-
-    SPDLOG_DEBUG("Serialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(SERIALIZE) / 1000);
-
-    return StatusCode::OK;
-}
-
-Status ModelInstance::infer(const ::KFSRequest* requestProto,
-    ::KFSResponse* responseProto,
-    std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr) {
-    OVMS_PROFILE_FUNCTION();
-    Timer<TIMER_END> timer;
-    using std::chrono::microseconds;
-
-    auto status = validate(requestProto);
-    auto requestBatchSize = getRequestBatchSize(requestProto, this->getBatchSizeIndex());
-    auto requestShapes = getRequestShapes(requestProto);
-    status = reloadModelIfRequired(status, requestBatchSize, requestShapes, modelUnloadGuardPtr);
-    if (!status.ok())
-        return status;
-    timer.start(GET_INFER_REQUEST);
-    ExecutingStreamIdGuard executingStreamIdGuard(getInferRequestsQueue(), this->getMetricReporter());
-    int executingInferId = executingStreamIdGuard.getId();
-    ov::InferRequest& inferRequest = executingStreamIdGuard.getInferRequest();
-    timer.stop(GET_INFER_REQUEST);
-    double getInferRequestTime = timer.elapsed<microseconds>(GET_INFER_REQUEST);
-    OBSERVE_IF_ENABLED(this->getMetricReporter().waitForInferReqTime, getInferRequestTime);
-    SPDLOG_DEBUG("Getting infer req duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_name(), getVersion(), executingInferId, getInferRequestTime / 1000);
+    SPDLOG_DEBUG("Preprocessing duration in model {}, version {}, nireq {}: {:.3f} ms",
+        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREPROCESS) / 1000);
 
     timer.start(DESERIALIZE);
     InputSink<ov::InferRequest&> inputSink(inferRequest);
@@ -1279,7 +1211,7 @@ Status ModelInstance::infer(const ::KFSRequest* requestProto,
     if (!status.ok())
         return status;
     SPDLOG_DEBUG("Deserialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_name(), getVersion(), executingInferId, timer.elapsed<microseconds>(DESERIALIZE) / 1000);
+        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(DESERIALIZE) / 1000);
 
     timer.start(PREDICTION);
     status = performInference(inferRequest);
@@ -1287,24 +1219,34 @@ Status ModelInstance::infer(const ::KFSRequest* requestProto,
     if (!status.ok())
         return status;
     SPDLOG_DEBUG("Prediction duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_name(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREDICTION) / 1000);
+        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREDICTION) / 1000);
 
     timer.start(SERIALIZE);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    status = serializePredictResponse(outputGetter, getOutputsInfo(), responseProto, getTensorInfoName, useSharedOutputContent(requestProto));
+    status = serializePredictResponse(outputGetter, getName(), getVersion(), getOutputsInfo(), responseProto, getTensorInfoName);
     timer.stop(SERIALIZE);
     if (!status.ok())
         return status;
-
-    responseProto->set_model_name(getName());
-    responseProto->set_model_version(std::to_string(getVersion()));
-
     SPDLOG_DEBUG("Serialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_name(), getVersion(), executingInferId, timer.elapsed<microseconds>(SERIALIZE) / 1000);
+        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(SERIALIZE) / 1000);
 
-    return StatusCode::OK;
-}
+    timer.start(POSTPROCESS);
+    status = requestProcessor->postInferenceProcessing(responseProto, inferRequest);
+    timer.stop(POSTPROCESS);
+    if (!status.ok())
+        return status;
+    SPDLOG_DEBUG("Postprocessing duration in model {}, version {}, nireq {}: {:.3f} ms",
+        getName(), getVersion(), executingInferId, timer.elapsed<microseconds>(POSTPROCESS) / 1000);
 
+    status = requestProcessor->release();
+    return status;
+}
+template Status ModelInstance::infer<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>(const tensorflow::serving::PredictRequest* requestProto,
+    tensorflow::serving::PredictResponse* responseProto,
+    std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr);
+template Status ModelInstance::infer(const ::KFSRequest* requestProto,
+    ::KFSResponse* responseProto,
+    std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr);
 const size_t ModelInstance::getBatchSizeIndex() const {
     const auto& inputItr = this->inputsInfo.cbegin();
     if (inputItr == this->inputsInfo.cend()) {
@@ -1322,4 +1264,33 @@ uint32_t ModelInstance::getNumOfStreams() const {
     return compiledModel->get_property(ov::num_streams);
 }
 
+std::unique_ptr<RequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>> ModelInstance::createRequestProcessor(const tensorflow::serving::PredictRequest*, tensorflow::serving::PredictResponse*) {
+    return std::make_unique<RequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>>();
+}
+std::unique_ptr<RequestProcessor<KFSRequest, KFSResponse>> ModelInstance::createRequestProcessor(const KFSRequest*, KFSResponse*) {
+    return std::make_unique<RequestProcessor<KFSRequest, KFSResponse>>();
+}
+std::unique_ptr<RequestProcessor<InferenceRequest, InferenceResponse>> ModelInstance::createRequestProcessor(const InferenceRequest*, InferenceResponse*) {
+    return std::make_unique<RequestProcessor<InferenceRequest, InferenceResponse>>();
+}
+
+template Status ModelInstance::infer<InferenceRequest, InferenceResponse>(InferenceRequest const*, InferenceResponse*, std::unique_ptr<ModelInstanceUnloadGuard>&);
+
+template <typename RequestType, typename ResponseType>
+RequestProcessor<RequestType, ResponseType>::RequestProcessor() = default;
+template <typename RequestType, typename ResponseType>
+RequestProcessor<RequestType, ResponseType>::~RequestProcessor() = default;
+template <typename RequestType, typename ResponseType>
+Status RequestProcessor<RequestType, ResponseType>::extractRequestParameters(const RequestType* request) { return StatusCode::OK; }
+template <typename RequestType, typename ResponseType>
+Status RequestProcessor<RequestType, ResponseType>::prepare() { return StatusCode::OK; }
+template <typename RequestType, typename ResponseType>
+Status RequestProcessor<RequestType, ResponseType>::preInferenceProcessing(ov::InferRequest& inferRequest) { return StatusCode::OK; }
+template <typename RequestType, typename ResponseType>
+Status RequestProcessor<RequestType, ResponseType>::postInferenceProcessing(ResponseType* response, ov::InferRequest& inferRequest) { return StatusCode::OK; }
+template <typename RequestType, typename ResponseType>
+Status RequestProcessor<RequestType, ResponseType>::release() { return StatusCode::OK; }
+
+template class RequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>;
+template class RequestProcessor<KFSRequest, KFSResponse>;
 }  // namespace ovms
diff --git a/src/modelinstance.hpp b/src/modelinstance.hpp
index ef2a62e1..f701867f 100644
--- a/src/modelinstance.hpp
+++ b/src/modelinstance.hpp
@@ -19,18 +19,15 @@
 #include <functional>
 #include <map>
 #include <memory>
+#include <set>
 #include <sstream>
 #include <string>
 #include <vector>
 
 #include <openvino/openvino.hpp>
 
-#pragma GCC diagnostic push
-#pragma GCC diagnostic ignored "-Wall"
-#include "tensorflow/core/framework/tensor.h"
-#include "tensorflow_serving/apis/prediction_service.grpc.pb.h"
-#pragma GCC diagnostic pop
-
+#include "inferencerequest.hpp"
+#include "inferenceresponse.hpp"
 #include "kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "model_metric_reporter.hpp"
 #include "modelchangesubscription.hpp"
@@ -39,12 +36,15 @@
 #include "modelversionstatus.hpp"
 #include "ovinferrequestsqueue.hpp"
 #include "tensorinfo.hpp"
+#include "tfs_frontend/tfs_utils.hpp"
 
 namespace ovms {
 class MetricRegistry;
 class ModelInstanceUnloadGuard;
 class PipelineDefinition;
 class Status;
+template <typename T1, typename T2>
+struct RequestProcessor;
 
 class DynamicModelParameter {
 public:
@@ -543,17 +543,28 @@ public:
 
     Status performInference(ov::InferRequest& inferRequest);
 
-    virtual Status infer(const tensorflow::serving::PredictRequest* requestProto,
-        tensorflow::serving::PredictResponse* responseProto,
-        std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr);
-    virtual Status infer(const ::KFSRequest* requestProto,
-        ::KFSResponse* responseProto,
+    template <typename RequestType, typename ResponseType>
+    Status infer(const RequestType* requestProto,
+        ResponseType* responseProto,
         std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr);
 
     ModelMetricReporter& getMetricReporter() const { return *this->reporter; }
 
     uint32_t getNumOfStreams() const;
 
-    Status infer(float* data, float* output);
+    virtual std::unique_ptr<RequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>> createRequestProcessor(const tensorflow::serving::PredictRequest*, tensorflow::serving::PredictResponse*);
+    virtual std::unique_ptr<RequestProcessor<KFSRequest, KFSResponse>> createRequestProcessor(const KFSRequest*, KFSResponse*);
+    virtual std::unique_ptr<RequestProcessor<InferenceRequest, InferenceResponse>> createRequestProcessor(const InferenceRequest*, InferenceResponse*);
+    virtual const std::set<std::string>& getOptionalInputNames();
+};
+template <typename RequestType, typename ResponseType>
+struct RequestProcessor {
+    RequestProcessor();
+    virtual ~RequestProcessor();
+    virtual Status extractRequestParameters(const RequestType* request);
+    virtual Status prepare();
+    virtual Status preInferenceProcessing(ov::InferRequest& inferRequest);
+    virtual Status postInferenceProcessing(ResponseType* response, ov::InferRequest& inferRequest);
+    virtual Status release();
 };
 }  // namespace ovms
diff --git a/src/modelmanager.cpp b/src/modelmanager.cpp
index 099a8084..0a9335d8 100644
--- a/src/modelmanager.cpp
+++ b/src/modelmanager.cpp
@@ -152,7 +152,7 @@ Status ModelManager::start(const Config& config) {
     sequenceCleaupIntervalMinutes = config.sequenceCleanerPollWaitMinutes();
     resourcesCleanupIntervalSec = config.resourcesCleanerPollWaitSeconds();
     if (resourcesCleanupIntervalSec < 1) {
-        SPDLOG_LOGGER_WARN(modelmanager_logger, "Parameter: custom_node_resources_cleaner_interval has to be greater than 0. Applying default value(1 second)");
+        SPDLOG_LOGGER_WARN(modelmanager_logger, "Parameter: custom_node_resources_cleaner_interval_seconds has to be greater than 0. Applying default value(1 second)");
         resourcesCleanupIntervalSec = 1;
     }
     Status status;
diff --git a/src/pocapi.cpp b/src/pocapi.cpp
index 3f6f0ce5..c5b1edd0 100644
--- a/src/pocapi.cpp
+++ b/src/pocapi.cpp
@@ -16,67 +16,605 @@
 #include "pocapi.hpp"
 
 #include <cstdint>
+#include <memory>
 #include <string>
 
-#include "poc_api_impl.hpp"
+#include "buffer.hpp"
+#include "inferenceparameter.hpp"
+#include "inferencerequest.hpp"
+#include "inferenceresponse.hpp"
+#include "inferencetensor.hpp"
+#include "model_service.hpp"
+#include "modelinstance.hpp"
+#include "modelinstanceunloadguard.hpp"
+#include "modelmanager.hpp"
+#include "profiler.hpp"
+#include "servablemanagermodule.hpp"
+#include "server.hpp"
+#include "server_options.hpp"
+#include "status.hpp"
+#include "timer.hpp"
+
+using ovms::Buffer;
+using ovms::InferenceParameter;
+using ovms::InferenceRequest;
+using ovms::InferenceResponse;
+using ovms::InferenceTensor;
+using ovms::ModelInstanceUnloadGuard;
+using ovms::ModelManager;
+using ovms::ServableManagerModule;
+using ovms::Server;
+using ovms::Status;
+using ovms::StatusCode;
+using ovms::Timer;
+using std::chrono::microseconds;
 
 OVMS_Status* OVMS_ServerGeneralOptionsNew(OVMS_ServerGeneralOptions** options) {
-    *options = (OVMS_ServerGeneralOptions*)new ovms::GeneralOptionsImpl;
-    return 0;
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    *options = reinterpret_cast<OVMS_ServerGeneralOptions*>(new ovms::GeneralOptionsImpl);
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerGeneralOptionsDelete(OVMS_ServerGeneralOptions* options) {
-    delete (ovms::GeneralOptionsImpl*)options;
-    return 0;
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    delete reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerMultiModelOptionsNew(OVMS_ServerMultiModelOptions** options) {
-    *options = (OVMS_ServerMultiModelOptions*)new ovms::MultiModelOptionsImpl;
-    return 0;
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    *options = reinterpret_cast<OVMS_ServerMultiModelOptions*>(new ovms::MultiModelOptionsImpl);
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerMultiModelOptionsDelete(OVMS_ServerMultiModelOptions* options) {
-    delete (ovms::MultiModelOptionsImpl*)options;
-    return 0;
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    delete reinterpret_cast<ovms::MultiModelOptionsImpl*>(options);
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerNew(OVMS_Server** server) {
-    *server = (OVMS_Server*)new ovms::ServerImpl;
-    return 0;
+    // Create new server once multi server configuration becomes possible.
+    if (server == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_SERVER));
+    }
+    *server = reinterpret_cast<OVMS_Server*>(&ovms::Server::instance());
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerDelete(OVMS_Server* server) {
-    delete (ovms::ServerImpl*)server;
-    return 0;
+    if (server == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_SERVER));
+    }
+    ovms::Server* srv = reinterpret_cast<ovms::Server*>(server);
+    srv->shutdownModules();
+    // delete passed in ptr once multi server configuration is done
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerStartFromConfigurationFile(OVMS_Server* server,
     OVMS_ServerGeneralOptions* general_options,
     OVMS_ServerMultiModelOptions* multi_model_specific_options) {
-    ovms::ServerImpl* srv = (ovms::ServerImpl*)server;
-    ovms::GeneralOptionsImpl* go = (ovms::GeneralOptionsImpl*)general_options;
-    ovms::MultiModelOptionsImpl* mmo = (ovms::MultiModelOptionsImpl*)multi_model_specific_options;
+    if (server == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_SERVER));
+    }
+    if (general_options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (multi_model_specific_options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::Server* srv = reinterpret_cast<ovms::Server*>(server);
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(general_options);
+    ovms::MultiModelOptionsImpl* mmo = reinterpret_cast<ovms::MultiModelOptionsImpl*>(multi_model_specific_options);
     std::int64_t res = srv->start(go, mmo);
-    return (OVMS_Status*)res;  // TODO: Return proper OVMS_Status instead of a raw status code
+    return (OVMS_Status*)res;  // TODO: Return proper OVMS_Status instead of a raw status code in error handling PR
 }
 
 OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcPort(OVMS_ServerGeneralOptions* options,
-    uint64_t grpcPort) {
-    ovms::GeneralOptionsImpl* go = (ovms::GeneralOptionsImpl*)options;
+    uint32_t grpcPort) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
     go->grpcPort = grpcPort;
-    return 0;
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerGeneralOptionsSetRestPort(OVMS_ServerGeneralOptions* options,
-    uint64_t restPort) {
-    ovms::GeneralOptionsImpl* go = (ovms::GeneralOptionsImpl*)options;
+    uint32_t restPort) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
     go->restPort = restPort;
-    return 0;
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcWorkers(OVMS_ServerGeneralOptions* options,
+    uint32_t grpc_workers) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->grpcWorkers = grpc_workers;
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcBindAddress(OVMS_ServerGeneralOptions* options,
+    const char* grpc_bind_address) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (grpc_bind_address == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->grpcBindAddress.assign(grpc_bind_address);
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetRestWorkers(OVMS_ServerGeneralOptions* options,
+    uint32_t rest_workers) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->restWorkers = rest_workers;
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetRestBindAddress(OVMS_ServerGeneralOptions* options,
+    const char* rest_bind_address) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (rest_bind_address == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->restBindAddress.assign(rest_bind_address);
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcChannelArguments(OVMS_ServerGeneralOptions* options,
+    const char* grpc_channel_arguments) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (grpc_channel_arguments == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->grpcChannelArguments.assign(grpc_channel_arguments);
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetFileSystemPollWaitSeconds(OVMS_ServerGeneralOptions* options,
+    uint32_t seconds) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->filesystemPollWaitSeconds = seconds;
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetSequenceCleanerPollWaitMinutes(OVMS_ServerGeneralOptions* options,
+    uint32_t minutes) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->sequenceCleanerPollWaitMinutes = minutes;
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetCustomNodeResourcesCleanerIntervalSeconds(OVMS_ServerGeneralOptions* options,
+    uint32_t seconds) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->resourcesCleanerPollWaitSeconds = seconds;
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetCpuExtensionPath(OVMS_ServerGeneralOptions* options,
+    const char* cpu_extension_path) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (cpu_extension_path == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->cpuExtensionLibraryPath.assign(cpu_extension_path);
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetCacheDir(OVMS_ServerGeneralOptions* options,
+    const char* cache_dir) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (cache_dir == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->cacheDir.assign(cache_dir);
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetLogLevel(OVMS_ServerGeneralOptions* options,
+    OVMS_LogLevel log_level) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    switch (log_level) {
+    case OVMS_LOG_INFO:
+        go->logLevel = "INFO";
+        break;
+    case OVMS_LOG_ERROR:
+        go->logLevel = "ERROR";
+        break;
+    case OVMS_LOG_DEBUG:
+        go->logLevel = "DEBUG";
+        break;
+    case OVMS_LOG_TRACE:
+        go->logLevel = "TRACE";
+        break;
+    case OVMS_LOG_WARNING:
+        go->logLevel = "WARNING";
+        break;
+    default:
+        // TODO: Return error in error handling PR
+        break;
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_ServerGeneralOptionsSetLogPath(OVMS_ServerGeneralOptions* options,
+    const char* log_path) {
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (log_path == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::GeneralOptionsImpl* go = reinterpret_cast<ovms::GeneralOptionsImpl*>(options);
+    go->logPath.assign(log_path);
+    return nullptr;
 }
 
 OVMS_Status* OVMS_ServerMultiModelOptionsSetConfigPath(OVMS_ServerMultiModelOptions* options,
     const char* config_path) {
-    ovms::MultiModelOptionsImpl* mmo = (ovms::MultiModelOptionsImpl*)options;
-    mmo->configPath = std::string(config_path);
-    return 0;
+    if (options == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_OPTIONS));
+    }
+    if (config_path == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));  // TODO name->string
+    }
+    ovms::MultiModelOptionsImpl* mmo = reinterpret_cast<ovms::MultiModelOptionsImpl*>(options);
+    mmo->configPath.assign(config_path);
+    return nullptr;
+}
+// inference API
+OVMS_Status* OVMS_InferenceRequestNew(OVMS_InferenceRequest** request, const char* servableName, uint32_t servableVersion) {
+    // TODO should we allow to create requests to not yet loaded models?
+    if (request == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (servableName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    *request = reinterpret_cast<OVMS_InferenceRequest*>(new InferenceRequest(servableName, servableVersion));
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestDelete(OVMS_InferenceRequest* request) {
+    if (request == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    delete reinterpret_cast<InferenceRequest*>(request);
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestAddInput(OVMS_InferenceRequest* req, const char* inputName, OVMS_DataType datatype, const uint64_t* shape, uint32_t dimCount) {
+    if (req == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (inputName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    if (shape == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_TABLE));
+    }
+    InferenceRequest* request = reinterpret_cast<InferenceRequest*>(req);
+    auto status = request->addInput(inputName, datatype, shape, dimCount);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestInputSetData(OVMS_InferenceRequest* req, const char* inputName, void* data, size_t bufferSize, BufferType bufferType, uint32_t deviceId) {
+    if (req == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (inputName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    if (data == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_DATA));
+    }
+    InferenceRequest* request = reinterpret_cast<InferenceRequest*>(req);
+    auto status = request->setInputBuffer(inputName, data, bufferSize, bufferType, deviceId);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestAddParameter(OVMS_InferenceRequest* req, const char* parameterName, OVMS_DataType datatype, const void* data, size_t byteSize) {
+    if (req == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (parameterName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    if (data == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_DATA));
+    }
+    InferenceRequest* request = reinterpret_cast<InferenceRequest*>(req);
+    auto status = request->addParameter(parameterName, datatype, data);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestRemoveParameter(OVMS_InferenceRequest* req, const char* parameterName) {
+    if (req == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (parameterName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    InferenceRequest* request = reinterpret_cast<InferenceRequest*>(req);
+    auto status = request->removeParameter(parameterName);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestRemoveInput(OVMS_InferenceRequest* req, const char* inputName) {
+    if (req == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (inputName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    InferenceRequest* request = reinterpret_cast<InferenceRequest*>(req);
+    auto status = request->removeInput(inputName);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceRequestInputRemoveData(OVMS_InferenceRequest* req, const char* inputName) {
+    if (req == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (inputName == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    InferenceRequest* request = reinterpret_cast<InferenceRequest*>(req);
+    auto status = request->removeInputBuffer(inputName);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceResponseGetOutput(OVMS_InferenceResponse* res, uint32_t id, const char** name, OVMS_DataType* datatype, const uint64_t** shape, uint32_t* dimCount, const void** data, size_t* bytesize, BufferType* bufferType, uint32_t* deviceId) {
+    if (res == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_RESPONSE));
+    }
+    if (name == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_STRING));
+    }
+    if (datatype == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    if (shape == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_TABLE));
+    }
+    if (dimCount == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    if (data == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_DATA));
+    }
+    if (bytesize == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    if (bufferType == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    if (deviceId == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    InferenceResponse* response = reinterpret_cast<InferenceResponse*>(res);
+    const InferenceTensor* tensor = nullptr;
+    const std::string* cppName;
+    auto status = response->getOutput(id, &cppName, &tensor);
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+    if ((tensor == nullptr) ||
+        (cppName == nullptr)) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::INTERNAL_ERROR, "InferenceResponse returned nullptr tensor or name"));
+    }
+    const Buffer* buffer = tensor->getBuffer();
+    if (nullptr == buffer) {
+        return reinterpret_cast<OVMS_Status*>(new Status(ovms::StatusCode::INTERNAL_ERROR, "InferenceResponse has tensor without buffer"));
+    }
+    *name = cppName->c_str();
+    *datatype = tensor->getDataType();
+    *shape = tensor->getShape().data();
+    *dimCount = tensor->getShape().size();
+    *bufferType = buffer->getBufferType();
+    *deviceId = buffer->getDeviceId().value_or(0);  // TODO how discriminate betwen undefined & actual device 0
+    // possibly it is not neccessary to discriminate
+    *data = buffer->data();
+    *bytesize = buffer->getByteSize();
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceResponseGetOutputCount(OVMS_InferenceResponse* res, uint32_t* count) {
+    if (res == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_RESPONSE));
+    }
+    if (count == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    InferenceResponse* response = reinterpret_cast<InferenceResponse*>(res);
+    *count = response->getOutputCount();
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceResponseGetParameterCount(OVMS_InferenceResponse* res, uint32_t* count) {
+    if (res == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_RESPONSE));
+    }
+    if (count == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    InferenceResponse* response = reinterpret_cast<InferenceResponse*>(res);
+    *count = response->getParameterCount();
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceResponseGetParameter(OVMS_InferenceResponse* res, uint32_t id, OVMS_DataType* datatype, const void** data) {
+    if (res == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_RESPONSE));
+    }
+    if (datatype == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_NUMBER));
+    }
+    if (data == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_DATA));
+    }
+    InferenceResponse* response = reinterpret_cast<InferenceResponse*>(res);
+    const InferenceParameter* parameter = response->getParameter(id);
+    if (nullptr == parameter) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_PARAMETER_FOR_REMOVAL));
+    }
+    *datatype = parameter->getDataType();
+    *data = parameter->getData();
+    return nullptr;
+}
+
+OVMS_Status* OVMS_InferenceResponseDelete(OVMS_InferenceResponse* res) {
+    if (res == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_RESPONSE));
+    }
+    InferenceResponse* response = reinterpret_cast<InferenceResponse*>(res);
+    delete response;
+    return nullptr;
+}
+
+namespace {
+enum : unsigned int {
+    TOTAL,
+    TIMER_END
+};
+
+static Status getModelInstance(ovms::Server& server, const InferenceRequest* request, std::shared_ptr<ovms::ModelInstance>& modelInstance,
+    std::unique_ptr<ModelInstanceUnloadGuard>& modelInstanceUnloadGuardPtr) {
+    OVMS_PROFILE_FUNCTION();
+    auto& modelManager = dynamic_cast<const ServableManagerModule*>(server.getModule(ovms::SERVABLE_MANAGER_MODULE_NAME))->getServableManager();
+    return modelManager.getModelInstance(request->getServableName(), request->getServableVersion(), modelInstance, modelInstanceUnloadGuardPtr);
+}
+}  // namespace
+OVMS_Status* OVMS_Inference(OVMS_Server* serverPtr, OVMS_InferenceRequest* request, OVMS_InferenceResponse** response) {
+    OVMS_PROFILE_FUNCTION();
+    using std::chrono::microseconds;
+    Timer<TIMER_END> timer;
+    timer.start(TOTAL);
+    if (serverPtr == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_SERVER));
+    }
+    if (request == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_REQUEST));
+    }
+    if (response == nullptr) {
+        return reinterpret_cast<OVMS_Status*>(new Status(StatusCode::NONEXISTENT_RESPONSE));
+    }
+    auto req = reinterpret_cast<ovms::InferenceRequest*>(request);
+    ovms::Server& server = *reinterpret_cast<ovms::Server*>(serverPtr);
+    std::unique_ptr<ovms::InferenceResponse> res(new ovms::InferenceResponse(req->getServableName(), req->getServableVersion()));
+
+    SPDLOG_DEBUG("Processing C-API request for model: {}; version: {}",
+        req->getServableName(),
+        req->getServableVersion());
+
+    std::shared_ptr<ovms::ModelInstance> modelInstance;
+    //   std::unique_ptr<ovms::Pipeline> pipelinePtr;
+
+    std::unique_ptr<ModelInstanceUnloadGuard> modelInstanceUnloadGuard;
+    auto status = getModelInstance(server, req, modelInstance, modelInstanceUnloadGuard);
+
+    if (status == StatusCode::MODEL_NAME_MISSING) {
+        SPDLOG_DEBUG("Requested model: {} does not exist. Searching for pipeline with that name...", req->getServableName());
+        // status = getPipeline(req, response, pipelinePtr);
+        status = Status(StatusCode::NOT_IMPLEMENTED, "Inference with DAG not supported with C-API in preview");
+    }
+    if (!status.ok()) {
+        if (modelInstance) {
+            //    INCREMENT_IF_ENABLED(modelInstance->getMetricReporter().reqFailGrpcPredict);
+        }
+        SPDLOG_INFO("Getting modelInstance or pipeline failed. {}", status.string());
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+
+    // ExecutionContext executionContext{
+    //   ExecutionContext::Interface::CAPI,
+    //  ExecutionContext::Method::Inference};
+
+    // if (pipelinePtr) {
+    //       status = pipelinePtr->execute(executionContext);
+    // INCREMENT_IF_ENABLED(pipelinePtr->getMetricReporter().getInferRequestMetric(executionContext, status.ok()));
+    //   } else {
+    status = modelInstance->infer(req, res.get(), modelInstanceUnloadGuard);
+    //   INCREMENT_IF_ENABLED(modelInstance->getMetricReporter().getInferRequestMetric(executionContext, status.ok()));
+    //}
+
+    if (!status.ok()) {
+        return reinterpret_cast<OVMS_Status*>(new Status(status));
+    }
+
+    timer.stop(TOTAL);
+    double reqTotal = timer.elapsed<microseconds>(TOTAL);
+    // if (pipelinePtr) {
+    //  OBSERVE_IF_ENABLED(pipelinePtr->getMetricReporter().reqTimeGrpc, reqTotal);
+    //  } else {
+    //   OBSERVE_IF_ENABLED(modelInstance->getMetricReporter().reqTimeGrpc, reqTotal);
+    // }
+    SPDLOG_DEBUG("Total C-API req processing time: {} ms", reqTotal / 1000);
+    *response = reinterpret_cast<OVMS_InferenceResponse*>(res.release());
+    return nullptr;
+    // return grpc::Status::OK;
 }
diff --git a/src/pocapi.hpp b/src/pocapi.hpp
index ec8e1a12..7fd15eed 100644
--- a/src/pocapi.hpp
+++ b/src/pocapi.hpp
@@ -17,6 +17,8 @@
 #include <stddef.h>
 #include <stdint.h>  //  For precise data types
 
+// TODO extern C
+
 struct OVMS_Server;
 struct OVMS_Status;
 
@@ -24,7 +26,7 @@ struct OVMS_ServerGeneralOptions;
 struct OVMS_ServerMultiModelOptions;
 
 // TODO reuse this in precision.hpp
-enum DataType {
+enum OVMS_DataType {
     OVMS_DATATYPE_BF16,
     OVMS_DATATYPE_FP64,
     OVMS_DATATYPE_FP32,
@@ -61,13 +63,13 @@ struct OVMS_Status;
 struct OVMS_InferenceRequest;
 struct OVMS_InferenceResponse;
 
-typedef enum OVMSSERVER_loglevel_enum {
-    OVMSSERVER_LOG_TRACE,
-    OVMSSERVER_LOG_DEBUG,
-    OVMSSERVER_LOG_INFO,
-    OVMSSERVER_LOG_WARNING,
-    OVMSSERVER_LOG_ERROR
-} OVMSSERVER_LogLevel;
+typedef enum OVMS_LogLevel_enum {
+    OVMS_LOG_TRACE,
+    OVMS_LOG_DEBUG,
+    OVMS_LOG_INFO,
+    OVMS_LOG_WARNING,
+    OVMS_LOG_ERROR
+} OVMS_LogLevel;
 
 ////
 //// OVMS_ServerGeneralOptions
@@ -80,40 +82,60 @@ OVMS_Status* OVMS_ServerGeneralOptionsDelete(OVMS_ServerGeneralOptions* options)
 
 // --port
 OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcPort(OVMS_ServerGeneralOptions* options,
-    uint64_t grpcPort);
+    uint32_t grpc_port);
 
 // --rest_port
 OVMS_Status* OVMS_ServerGeneralOptionsSetRestPort(OVMS_ServerGeneralOptions* options,
-    uint64_t restPort);
+    uint32_t rest_port);
 
-// --log_level
-OVMS_Status* OVMS_ServerGeneralOptionsSetLogLevel(OVMS_ServerGeneralOptions* options,
-    OVMSSERVER_LogLevel log_level);
+// --grpc_workers
+OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcWorkers(OVMS_ServerGeneralOptions* options,
+    uint32_t grpc_workers);
 
-// --log_path
-OVMS_Status* OVMS_ServerGeneralOptionsSetLogPath(OVMS_ServerGeneralOptions* options,
-    const char* log_path);
+// --grpc_bind_address
+OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcBindAddress(OVMS_ServerGeneralOptions* options,
+    const char* grpc_bind_address);
+
+// --rest_workers
+OVMS_Status* OVMS_ServerGeneralOptionsSetRestWorkers(OVMS_ServerGeneralOptions* options,
+    uint32_t rest_workers);
+
+// --rest_bind_address
+OVMS_Status* OVMS_ServerGeneralOptionsSetRestBindAddress(OVMS_ServerGeneralOptions* options,
+    const char* rest_bind_address);
+
+// --grpc_channel_arguments
+OVMS_Status* OVMS_ServerGeneralOptionsSetGrpcChannelArguments(OVMS_ServerGeneralOptions* options,
+    const char* grpc_channel_arguments);
 
 // --file_system_poll_wait_seconds
 OVMS_Status* OVMS_ServerGeneralOptionsSetFileSystemPollWaitSeconds(OVMS_ServerGeneralOptions* options,
-    uint64_t file_system_poll_wait_seconds);
+    uint32_t seconds);
 
 // --sequence_cleaner_poll_wait_minutes
 OVMS_Status* OVMS_ServerGeneralOptionsSetSequenceCleanerPollWaitMinutes(OVMS_ServerGeneralOptions* options,
-    uint64_t sequence_cleaner_poll_wait_minutes);
+    uint32_t minutes);
 
-// --custom_node_resources_cleaner_interval
-OVMS_Status* OVMS_ServerGeneralOptionsSetCustomNodeResourcesCleanerInterval(OVMS_ServerGeneralOptions* options,
-    uint64_t custom_node_resources_cleaner_interval);  // TODO: Should include seconds or minutes in the name
-
-// --cache_dir
-void OVMS_ServerGeneralOptionsSetCacheDir(OVMS_ServerGeneralOptions* options,
-    const char* cache_dir);
+// --custom_node_resources_cleaner_interval_seconds
+OVMS_Status* OVMS_ServerGeneralOptionsSetCustomNodeResourcesCleanerIntervalSeconds(OVMS_ServerGeneralOptions* options,
+    uint32_t seconds);
 
 // --cpu_extension
 OVMS_Status* OVMS_ServerGeneralOptionsSetCpuExtensionPath(OVMS_ServerGeneralOptions* options,
     const char* cpu_extension_path);
 
+// --cache_dir
+OVMS_Status* OVMS_ServerGeneralOptionsSetCacheDir(OVMS_ServerGeneralOptions* options,
+    const char* cache_dir);
+
+// --log_level
+OVMS_Status* OVMS_ServerGeneralOptionsSetLogLevel(OVMS_ServerGeneralOptions* options,
+    OVMS_LogLevel log_level);
+
+// --log_path
+OVMS_Status* OVMS_ServerGeneralOptionsSetLogPath(OVMS_ServerGeneralOptions* options,
+    const char* log_path);
+
 ////
 //// OVMS_ServerMultiModelOptions
 //// Options for starting multi model server controlled by config.json file
@@ -146,27 +168,26 @@ OVMS_Status* OVMS_ServerStartFromConfigurationFile(OVMS_Server* server,
 OVMS_Status* OVMS_ServerStop(OVMS_Server* server);
 
 // OVMS_InferenceRequest
-OVMS_Status* OVMS_InferenceRequestNew(char* modelName, uint32_t servableVersion);
+OVMS_Status* OVMS_InferenceRequestNew(OVMS_InferenceRequest** request, const char* servableName, uint32_t servableVersion);  // TODO add passing server here
 OVMS_Status* OVMS_InferenceRequestDelete(OVMS_InferenceRequest* response);
-OVMS_Status* OVMS_InferenceRequestAddInput(OVMS_InferenceRequest* request, char* inputName, DataType datatype, uint64_t* shape, uint32_t dimCount);
-OVMS_Status* OVMS_InferenceRequestAddInputRaw(OVMS_InferenceRequest* request, char* inputName, DataType datatype);  // TODO consider no datatype & handle the parameters
+
+OVMS_Status* OVMS_InferenceRequestAddInput(OVMS_InferenceRequest* request, const char* inputName, OVMS_DataType datatype, const uint64_t* shape, uint32_t dimCount);
+OVMS_Status* OVMS_InferenceRequestAddInputRaw(OVMS_InferenceRequest* request, const char* inputName, OVMS_DataType datatype);  // TODO consider no datatype & handle the parameters NOT IMPLEMENTED
+
 // ownership of data needs to be maintained during inference
-OVMS_Status* OVMS_InferenceRequestInputSetData(OVMS_InferenceRequest* request, char* inputName, void* data, size_t bufferSize, BufferType bufferType, uint32_t deviceId);
-OVMS_Status* OVMS_InferenceRequestInputRemoveData(OVMS_InferenceRequest* request, char* inputName);
-OVMS_Status* OVMS_InferenceRequestRemoveInput(OVMS_InferenceRequest* request, char* inputName);  // this will allow for reuse of request but with different input data
+OVMS_Status* OVMS_InferenceRequestInputSetData(OVMS_InferenceRequest* request, const char* inputName, void* data, size_t bufferSize, BufferType bufferType, uint32_t deviceId);
+OVMS_Status* OVMS_InferenceRequestInputRemoveData(OVMS_InferenceRequest* request, const char* inputName);
+OVMS_Status* OVMS_InferenceRequestRemoveInput(OVMS_InferenceRequest* request, const char* inputName);  // this will allow for reuse of request but with different input data
 OVMS_Status* OVMS_InferenceRequestRemoveAllInputs(OVMS_InferenceRequest* request);
 OVMS_Status* OVMS_InferenceRequestAddRequestedOutput(OVMS_InferenceRequest* request, char* inputName);  // TODO consider the other way around - add not usefull outputs
-OVMS_Status* OVMS_InferenceRequestAddParameter(OVMS_InferenceRequest* request, char* paramaterName, DataType datatype, void* data, size_t byteSize);
+OVMS_Status* OVMS_InferenceRequestAddParameter(OVMS_InferenceRequest* request, const char* parameterName, OVMS_DataType datatype, const void* data, size_t byteSize);
+OVMS_Status* OVMS_InferenceRequestRemoveParameter(OVMS_InferenceRequest* request, const char* parameterName);
 
 // OVMS_Inference Response
 OVMS_Status* OVMS_InferenceResponseGetOutputCount(OVMS_InferenceResponse* response, uint32_t* count);
-OVMS_Status* OVMS_InferenceResponseOutput(OVMS_InferenceResponse* response, uint32_t id, char* name, DataType* datatype, uint64_t* shape, uint32_t dimCount, BufferType* bufferType, uint32_t* deviceId, void** data);
-OVMS_Status* OVMS_InferenceResponseDelete(OVMS_InferenceResponse* response);
+OVMS_Status* OVMS_InferenceResponseGetOutput(OVMS_InferenceResponse* response, uint32_t id, const char** name, OVMS_DataType* datatype, const uint64_t** shape, uint32_t* dimCount, const void** data, size_t* bytesize, BufferType* bufferType, uint32_t* deviceId);
 OVMS_Status* OVMS_InferenceResponseGetParameterCount(OVMS_InferenceResponse* response, uint32_t* count);
-OVMS_Status* OVMS_InferenceResponseGetParameter(OVMS_InferenceResponse* response, uint32_t id, DataType* datatype, void** data);
-
-OVMS_Status* OVMS_Inference(OVMS_InferenceRequest* request, OVMS_InferenceResponse** response);
+OVMS_Status* OVMS_InferenceResponseGetParameter(OVMS_InferenceResponse* response, uint32_t id, OVMS_DataType* datatype, const void** data);
+OVMS_Status* OVMS_InferenceResponseDelete(OVMS_InferenceResponse* response);
 
-// POCAPI to be removed
-int OVMS_Start(int argc, char** argv);
-void OVMS_Infer(char* name, float* data, float* output);
+OVMS_Status* OVMS_Inference(OVMS_Server* server, OVMS_InferenceRequest* request, OVMS_InferenceResponse** response);
diff --git a/src/pocapiinternal.cpp b/src/pocapiinternal.cpp
index 47c4c950..6137983d 100644
--- a/src/pocapiinternal.cpp
+++ b/src/pocapiinternal.cpp
@@ -16,7 +16,7 @@
 #include "pocapiinternal.hpp"
 
 namespace ovms {
-DataType getPrecisionAsOVMSDataType(Precision precision) {
+OVMS_DataType getPrecisionAsOVMSDataType(Precision precision) {
     switch (precision) {
     case Precision::BF16:
         return OVMS_DATATYPE_BF16;
@@ -66,7 +66,7 @@ DataType getPrecisionAsOVMSDataType(Precision precision) {
         return OVMS_DATATYPE_UNDEFINED;
     }
 }
-Precision getOVMSDataTypeAsPrecision(DataType datatype) {
+Precision getOVMSDataTypeAsPrecision(OVMS_DataType datatype) {
     switch (datatype) {
     case OVMS_DATATYPE_BF16:
         return Precision::BF16;
diff --git a/src/pocapiinternal.hpp b/src/pocapiinternal.hpp
index a90af3e3..e0303221 100644
--- a/src/pocapiinternal.hpp
+++ b/src/pocapiinternal.hpp
@@ -18,6 +18,6 @@
 #include "precision.hpp"
 
 namespace ovms {
-DataType getPrecisionAsOVMSDataType(Precision precision);
-Precision getOVMSDataTypeAsPrecision(DataType datatype);
+OVMS_DataType getPrecisionAsOVMSDataType(Precision precision);
+Precision getOVMSDataTypeAsPrecision(OVMS_DataType datatype);
 }  // namespace ovms
diff --git a/src/predict_request_validation_utils.cpp b/src/predict_request_validation_utils.cpp
index 3377afde..0b43cd22 100644
--- a/src/predict_request_validation_utils.cpp
+++ b/src/predict_request_validation_utils.cpp
@@ -23,9 +23,14 @@
 
 #include <spdlog/spdlog.h>
 
+#include "buffer.hpp"
+#include "capi_frontend/capi_utils.hpp"
+#include "inferencerequest.hpp"
+#include "inferencetensor.hpp"
 #include "kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "kfs_frontend/kfs_utils.hpp"
 #include "modelconfig.hpp"
+#include "pocapiinternal.hpp"
 #include "profiler.hpp"
 #include "status.hpp"
 #include "tfs_frontend/tfs_utils.hpp"
@@ -56,6 +61,10 @@ dimension_value_t RequestShapeInfo<TFSInputTensorType, TFSShapeType>::getDim(siz
     return tensor.tensor_shape().dim(i).size();
 }
 template <>
+dimension_value_t RequestShapeInfo<InferenceTensor, shape_t>::getDim(size_t i) {
+    return tensor.getShape()[i];
+}
+template <>
 size_t RequestShapeInfo<KFSTensorInputProto, KFSShapeType>::getShapeSize() {
     return tensor.shape().size();
 }
@@ -64,6 +73,10 @@ size_t RequestShapeInfo<TFSInputTensorType, TFSShapeType>::getShapeSize() {
     return tensor.tensor_shape().dim_size();
 }
 template <>
+size_t RequestShapeInfo<InferenceTensor, shape_t>::getShapeSize() {
+    return tensor.getShape().size();
+}
+template <>
 const TFSShapeType& RequestShapeInfo<TFSInputTensorType, TFSShapeType>::getShape() {
     return tensor.tensor_shape();
 }
@@ -71,7 +84,10 @@ template <>
 const KFSShapeType& RequestShapeInfo<KFSTensorInputProto, KFSShapeType>::getShape() {
     return tensor.shape();
 }
-
+template <>
+const shape_t& RequestShapeInfo<InferenceTensor, shape_t>::getShape() {
+    return tensor.getShape();
+}
 template <typename RequestType, typename InputTensorType, typename InputIterator, typename ShapeType>
 class RequestValidator {
     const RequestType& request;
@@ -86,6 +102,8 @@ class RequestValidator {
 
     RequestValidator() = delete;
 
+    const std::string* currentlyValidatedName;
+
     const std::string& getCurrentlyValidatedInputName() const;
     const InputTensorType& getInputFromIt(const InputIterator& it) const;
 
@@ -102,6 +120,7 @@ public:
         batchingMode(batchingMode),
         shapeInfo(shapeInfo) {}
 
+    Status validateInferenceTensorBufferType(const InferenceTensor& it) const;
     Status validateNumberOfInputs() const;
     Status validateAndGetInput(const RequestType& request, const std::string& name, InputIterator& it, size_t& bufferId);
     Status checkIfShapeValuesNegative(const InputTensorType& proto) const;
@@ -109,7 +128,7 @@ public:
     Status checkBatchSizeMismatch(const InputTensorType& proto, const Dimension& servableBatchSize, const size_t batchSizeIndex, Status& finalStatus, Mode batchingMode, Mode shapeMode) const;
     Status checkBinaryBatchSizeMismatch(const InputTensorType& proto, const Dimension& servableBatchSize, Status& finalStatus, Mode batchingMode, Mode shapeMode) const;
     Status checkShapeMismatch(const InputTensorType& proto, const ovms::TensorInfo& inputInfo, const size_t batchSizeIndex, Status& finalStatus, Mode batchingMode, Mode shapeMode) const;
-    Status validateTensorContentSize(const InputTensorType& proto, ovms::Precision expectedPrecision, size_t bufferId) const;
+    Status validateTensorContent(const InputTensorType& proto, ovms::Precision expectedPrecision, size_t bufferId) const;
     Status validateNumberOfShapeDimensions(const ovms::TensorInfo& inputInfo, const InputTensorType& proto) const;
     Status validatePrecision(const ovms::TensorInfo& inputInfo, const InputTensorType& proto) const;
     bool checkIfBinaryInputUsed(const InputTensorType& proto, const std::string inputName) const;
@@ -138,6 +157,11 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
     return StatusCode::OK;
 }
 
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validateRequestCoherency() const {
+    return StatusCode::OK;
+}
+
 template <>
 Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::validateNumberOfInputs() const {
     size_t expectedNumberOfInputs = inputsInfo.size();
@@ -163,22 +187,29 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
 
 template <>
 const std::string& RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::getCurrentlyValidatedInputName() const {
-    return it->name();
+    return *currentlyValidatedName;
 }
-
 template <>
 const std::string& RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::getCurrentlyValidatedInputName() const {
-    return it->first;
+    return *currentlyValidatedName;
 }
+template <>
+const std::string& RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::getCurrentlyValidatedInputName() const {
+    return *currentlyValidatedName;
+}
+
 template <>
 const KFSTensorInputProto& RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::getInputFromIt(const KFSInputTensorIteratorType& it) const {
     return *it;
 }
-
 template <>
 const TFSInputTensorType& RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::getInputFromIt(const TFSInputTensorIteratorType& it) const {
     return it->second;
 }
+template <>
+const InferenceTensor& RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::getInputFromIt(const InferenceTensor* const& it) const {
+    return *it;
+}
 
 template <>
 Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::validateNumberOfInputs() const {
@@ -196,20 +227,33 @@ Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIterat
     SPDLOG_DEBUG("[servable name: {} version: {}] Invalid number of inputs - {}", servableName, servableVersion, details);
     return Status(StatusCode::INVALID_NO_OF_INPUTS, details);
 }
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validateNumberOfInputs() const {
+    size_t expectedNumberOfInputs = inputsInfo.size();
+    if (request.getInputsSize() > 0 && expectedNumberOfInputs == static_cast<size_t>(request.getInputsSize())) {
+        return StatusCode::OK;
+    }
+    std::stringstream ss;
+    ss << "Expected: " << expectedNumberOfInputs << "; Actual: " << request.getInputsSize();
+    const std::string details = ss.str();
+    SPDLOG_DEBUG("[servable name: {} version: {}] Invalid number of inputs - {}", servableName, servableVersion, details);
+    return Status(StatusCode::INVALID_NO_OF_INPUTS, details);
+}
 
 template <>
 Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::validateAndGetInput(const TFSRequestType& request, const std::string& name, TFSInputTensorIteratorType& it, size_t& bufferId) {
     it = request.inputs().find(name);
     if (it != request.inputs().end()) {
+        currentlyValidatedName = &name;
         return StatusCode::OK;
     }
+    currentlyValidatedName = nullptr;
     std::stringstream ss;
     ss << "Required input: " << name;
     const std::string details = ss.str();
     SPDLOG_DEBUG("[servable name: {} version: {}] Missing input with specific name - {}", servableName, servableVersion, details);
     return Status(StatusCode::INVALID_MISSING_INPUT, details);
 }
-
 template <>
 Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::validateAndGetInput(const KFSRequest& request, const std::string& name, KFSInputTensorIteratorType& it, size_t& bufferId) {
     it = request.inputs().begin();
@@ -222,8 +266,25 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
         ++bufferId;
     }
     if (it != request.inputs().end()) {
+        currentlyValidatedName = &name;
+        return StatusCode::OK;
+    }
+    currentlyValidatedName = nullptr;
+    std::stringstream ss;
+    ss << "Required input: " << name;
+    const std::string details = ss.str();
+    SPDLOG_DEBUG("[servable name: {} version: {}] Missing input with specific name - {}", servableName, servableVersion, details);
+    return Status(StatusCode::INVALID_MISSING_INPUT, details);
+}
+
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validateAndGetInput(const InferenceRequest& request, const std::string& name, const InferenceTensor*& it, size_t& bufferId) {
+    if (request.getInput(name.c_str(), &it) != StatusCode::NONEXISTENT_TENSOR) {
+        currentlyValidatedName = &name;
         return StatusCode::OK;
     }
+
+    currentlyValidatedName = nullptr;
     std::stringstream ss;
     ss << "Required input: " << name;
     const std::string details = ss.str();
@@ -259,7 +320,6 @@ Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIterat
     }
     return StatusCode::OK;
 }
-
 template <>
 Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::validateNumberOfBinaryInputShapeDimensions(const KFSTensorInputProto& proto) const {
     RequestShapeInfo<KFSTensorInputProto, KFSShapeType> rsi(proto);
@@ -272,6 +332,18 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
     }
     return StatusCode::OK;
 }
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validateNumberOfBinaryInputShapeDimensions(const InferenceTensor& tensor) const {
+    RequestShapeInfo<InferenceTensor, shape_t> rsi(tensor);
+    if (rsi.getShapeSize() != 1) {
+        std::stringstream ss;
+        ss << "Expected number of binary input shape dimensions: 1; Actual: " << rsi.getShapeSize() << "; input name: " << getCurrentlyValidatedInputName();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid number of shape dimensions - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, details);
+    }
+    return StatusCode::OK;
+}
 
 template <typename RequestType, typename InputTensorType, typename InputIteratorType, typename ShapeType>
 Status RequestValidator<RequestType, InputTensorType, InputIteratorType, ShapeType>::checkBatchSizeMismatch(const InputTensorType& proto, const Dimension& servableBatchSize, const size_t batchSizeIndex, Status& finalStatus, Mode batchingMode, Mode shapeMode) const {
@@ -342,6 +414,31 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
     }
     return StatusCode::OK;
 }
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::checkBinaryBatchSizeMismatch(const InferenceTensor& tensor, const Dimension& servableBatchSize, Status& finalStatus, Mode batchingMode, Mode shapeMode) const {
+    RequestShapeInfo<InferenceTensor, shape_t> rsi(tensor);
+    if (rsi.getDim(0) <= 0) {
+        std::stringstream ss;
+        ss << "Batch size must be positive; input name: " << getCurrentlyValidatedInputName();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid batch size - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_BATCH_SIZE, details);
+    }
+    if (servableBatchSize.match(rsi.getDim(0))) {
+        return StatusCode::OK;
+    }
+    if (batchingMode == AUTO) {
+        finalStatus = StatusCode::BATCHSIZE_CHANGE_REQUIRED;
+        return StatusCode::OK;
+    } else if (shapeMode != AUTO) {
+        std::stringstream ss;
+        ss << "Expected: " << servableBatchSize.toString() << "; Actual: " << tensor.getBuffer()->getByteSize() << "; input name: " << getCurrentlyValidatedInputName();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid batch size - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_BATCH_SIZE, details);
+    }
+    return StatusCode::OK;
+}
 
 template <typename RequestType, typename InputTensorType, typename IteratorType, typename ShapeType>
 Status RequestValidator<RequestType, InputTensorType, IteratorType, ShapeType>::checkShapeMismatch(const InputTensorType& proto, const ovms::TensorInfo& inputInfo, const size_t batchSizeIndex, Status& finalStatus, Mode batchingMode, Mode shapeMode) const {
@@ -387,8 +484,41 @@ Status RequestValidator<RequestType, InputTensorType, IteratorType, ShapeType>::
     return StatusCode::OK;
 }
 
+template <typename RequestType, typename InputTensorType, typename IteratorType, typename ShapeType>
+Status RequestValidator<RequestType, InputTensorType, IteratorType, ShapeType>::validateInferenceTensorBufferType(const InferenceTensor& it) const {
+    const Buffer* buffer = it.getBuffer();
+    const BufferType bufType = buffer->getBufferType();
+    if (bufType < BufferType::OVMS_BUFFERTYPE_CPU || bufType > BufferType::OVMS_BUFFERTYPE_HDDL) {
+        std::stringstream ss;
+        ss << "Required input ";
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Has invalid buffer type for input with specific name - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_BUFFER_TYPE, details);
+
+    } else {
+        // Remove this when other buffer types are supported
+        if (bufType != BufferType::OVMS_BUFFERTYPE_CPU) {
+            std::stringstream ss;
+            ss << "Required input ";
+            const std::string details = ss.str();
+            SPDLOG_DEBUG("[servable name: {} version: {}] Has invalid buffer type for input with specific name - {}", servableName, servableVersion, details);
+            return Status(StatusCode::INVALID_BUFFER_TYPE, details);
+        }
+    }
+
+    if (buffer->getBufferType() == BufferType::OVMS_BUFFERTYPE_CPU && buffer->getDeviceId() != std::nullopt && buffer->getDeviceId() != 0) {
+        std::stringstream ss;
+        ss << "Required input ";
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Has invalid device id for buffer, input with specific name - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_DEVICE_ID, details);
+    }
+
+    return StatusCode::OK;
+}
+
 template <>
-Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::validateTensorContentSize(const TFSInputTensorType& proto, ovms::Precision expectedPrecision, size_t bufferId) const {
+Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::validateTensorContent(const TFSInputTensorType& proto, ovms::Precision expectedPrecision, size_t bufferId) const {
     /*
     int8        data in request.tensor_content
     uint8       data in request.tensor_content
@@ -490,7 +620,7 @@ static size_t getElementsCount(const KFSTensorInputProto& proto, ovms::Precision
 }
 
 template <>
-Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::validateTensorContentSize(const KFSTensorInputProto& proto, ovms::Precision expectedPrecision, size_t bufferId) const {
+Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::validateTensorContent(const KFSTensorInputProto& proto, ovms::Precision expectedPrecision, size_t bufferId) const {
     size_t expectedValueCount = 1;
     for (int i = 0; i < proto.shape().size(); i++) {
         expectedValueCount *= proto.shape()[i];
@@ -518,6 +648,33 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
     }
     return StatusCode::OK;
 }
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validateTensorContent(const InferenceTensor& tensor, ovms::Precision expectedPrecision, size_t bufferId) const {
+    const Buffer* buffer = tensor.getBuffer();
+    if (nullptr == buffer) {
+        std::stringstream ss;
+        ss << "Servable: " << servableName
+           << "; version: " << servableVersion
+           << "; is missing buffer for tensor: " << bufferId;
+        const std::string details = ss.str();
+        SPDLOG_DEBUG(details);
+        return Status(StatusCode::INVALID_CONTENT_SIZE, details);  // TODO separate code?
+    }
+    size_t expectedValueCount = 1;
+    for (size_t i = 0; i < tensor.getShape().size(); i++) {
+        expectedValueCount *= tensor.getShape()[i];
+    }
+    size_t expectedContentSize = expectedValueCount * ov::element::Type(ovmsPrecisionToIE2Precision(expectedPrecision)).size();
+    if (expectedContentSize != buffer->getByteSize()) {
+        std::stringstream ss;
+        ss << "Expected: " << expectedContentSize << " bytes; Actual: " << buffer->getByteSize() << " bytes; input name: " << getCurrentlyValidatedInputName();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid content size of tensor - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_CONTENT_SIZE, details);
+    }
+
+    return validateInferenceTensorBufferType(tensor);
+}
 
 template <>
 Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::validateNumberOfShapeDimensions(const ovms::TensorInfo& inputInfo, const TFSInputTensorType& proto) const {
@@ -552,6 +709,22 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
     }
     return StatusCode::OK;
 }
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validateNumberOfShapeDimensions(const ovms::TensorInfo& inputInfo, const InferenceTensor& tensor) const {
+    // Network and request must have the same number of shape dimensions, higher than 0
+    const auto& shape = inputInfo.getShape();
+    if (tensor.getShape().size() <= 0 ||
+        shape.size() != static_cast<size_t>(tensor.getShape().size())) {
+        std::stringstream ss;
+        ss << "Expected: " << shape.toString()
+           << "; Actual: " << tensorShapeToString(tensor.getShape())
+           << "; input name: " << getCurrentlyValidatedInputName();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid number of shape dimensions - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, details);
+    }
+    return StatusCode::OK;
+}
 
 template <>
 Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIteratorType, TFSShapeType>::validatePrecision(const ovms::TensorInfo& inputInfo, const TFSInputTensorType& proto) const {
@@ -566,7 +739,6 @@ Status RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIterat
     }
     return StatusCode::OK;
 }
-
 template <>
 Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::validatePrecision(const ovms::TensorInfo& inputInfo, const KFSTensorInputProto& proto) const {
     if (proto.datatype() != ovmsPrecisionToKFSPrecision(inputInfo.getPrecision())) {
@@ -580,6 +752,19 @@ Status RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorT
     }
     return StatusCode::OK;
 }
+template <>
+Status RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::validatePrecision(const ovms::TensorInfo& inputInfo, const InferenceTensor& tensor) const {
+    if (tensor.getDataType() != getPrecisionAsOVMSDataType(inputInfo.getPrecision())) {
+        std::stringstream ss;
+        ss << "Expected: " << inputInfo.getPrecisionAsString()
+           << "; Actual: " << tensor.getDataType()
+           << "; input name: " << getCurrentlyValidatedInputName();
+        const std::string details = ss.str();
+        SPDLOG_DEBUG("[servable name: {} version: {}] Invalid precision - {}", servableName, servableVersion, details);
+        return Status(StatusCode::INVALID_PRECISION, details);
+    }
+    return StatusCode::OK;
+}
 
 static Mode getShapeMode(const shapes_info_map_t& shapeInfo, const std::string& name) {
     if (shapeInfo.size() == 0) {
@@ -603,7 +788,6 @@ bool RequestValidator<TFSRequestType, TFSInputTensorType, TFSInputTensorIterator
     }
     return false;
 }
-
 template <>
 bool RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>::checkIfBinaryInputUsed(const KFSTensorInputProto& proto, const std::string inputName) const {
     if (proto.datatype() == "BYTES") {
@@ -612,6 +796,11 @@ bool RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorTyp
     }
     return false;
 }
+template <>
+bool RequestValidator<ovms::InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>::checkIfBinaryInputUsed(const InferenceTensor& tensor, const std::string inputName) const {
+    // TODO no strig no bytes currently, will implement one of those types with binary input.
+    return false;
+}
 
 template <typename RequestType, typename InputTensorType, typename IteratorType, typename ShapeType>
 Status RequestValidator<RequestType, InputTensorType, IteratorType, ShapeType>::validate() {
@@ -672,7 +861,7 @@ Status RequestValidator<RequestType, InputTensorType, IteratorType, ShapeType>::
         status = checkShapeMismatch(proto, *inputInfo, batchIndex.value(), finalStatus, batchingMode, shapeMode);
         if (!status.ok())
             return status;
-        status = validateTensorContentSize(proto, inputInfo->getPrecision(), bufferId);
+        status = validateTensorContent(proto, inputInfo->getPrecision(), bufferId);
         if (!status.ok())
             return status;
     }
@@ -690,5 +879,11 @@ Status validate(const KFSRequest& request, const tensor_map_t& inputsInfo, const
     OVMS_PROFILE_FUNCTION();
     return RequestValidator<KFSRequest, KFSTensorInputProto, KFSInputTensorIteratorType, KFSShapeType>(request, inputsInfo, servableName, servableVersion, optionalAllowedInputNames, batchingMode, shapeInfo).validate();
 }
+
+template <>
+Status validate(const InferenceRequest& request, const tensor_map_t& inputsInfo, const std::string& servableName, const model_version_t servableVersion, const std::set<std::string>& optionalAllowedInputNames, const Mode batchingMode, const shapes_info_map_t& shapeInfo) {
+    OVMS_PROFILE_FUNCTION();
+    return RequestValidator<InferenceRequest, InferenceTensor, const InferenceTensor*, shape_t>(request, inputsInfo, servableName, servableVersion, optionalAllowedInputNames, batchingMode, shapeInfo).validate();
+}
 }  // namespace request_validation_utils
 }  // namespace ovms
diff --git a/src/prediction_service_utils.cpp b/src/prediction_service_utils.cpp
index 255215ad..506e334a 100644
--- a/src/prediction_service_utils.cpp
+++ b/src/prediction_service_utils.cpp
@@ -19,6 +19,8 @@
 
 #include "deserialization.hpp"
 #include "executingstreamidguard.hpp"
+#include "inferencerequest.hpp"
+#include "inferencetensor.hpp"
 #include "modelinstance.hpp"
 #include "modelinstanceunloadguard.hpp"
 #include "modelmanager.hpp"
@@ -96,6 +98,18 @@ std::map<std::string, shape_t> getRequestShapes(const tensorflow::serving::Predi
     }
     return requestShapes;
 }
+std::optional<Dimension> getRequestBatchSize(const InferenceRequest* request, const size_t batchSizeIndex) {
+    size_t bs = 0;
+    auto status = request->getBatchSize(bs, batchSizeIndex);
+    if (!status.ok()) {
+        return std::nullopt;  // TODO sth different?
+    }
+    return bs;
+}
+
+std::map<std::string, shape_t> getRequestShapes(const InferenceRequest* request) {
+    return request->getRequestShapes();
+}
 
 bool useSharedOutputContent(const tensorflow::serving::PredictRequest* request) {
     return true;
diff --git a/src/prediction_service_utils.hpp b/src/prediction_service_utils.hpp
index 41badb69..3c91645e 100644
--- a/src/prediction_service_utils.hpp
+++ b/src/prediction_service_utils.hpp
@@ -27,14 +27,17 @@
 #include "shape.hpp"
 
 namespace ovms {
+class InferenceRequest;
 
 std::optional<Dimension> getRequestBatchSize(const ::KFSRequest* request, const size_t batchSizeIndex);
-
 std::map<std::string, shape_t> getRequestShapes(const ::KFSRequest* request);
 
 std::optional<Dimension> getRequestBatchSize(const tensorflow::serving::PredictRequest* request, const size_t batchSizeIndex);
 std::map<std::string, shape_t> getRequestShapes(const tensorflow::serving::PredictRequest* request);
 
+std::optional<Dimension> getRequestBatchSize(const InferenceRequest* request, const size_t batchSizeIndex);
+std::map<std::string, shape_t> getRequestShapes(const InferenceRequest* request);
+
 bool useSharedOutputContent(const tensorflow::serving::PredictRequest* request);
 bool useSharedOutputContent(const ::inference::ModelInferRequest* request);
 }  // namespace ovms
diff --git a/src/serialization.hpp b/src/serialization.hpp
index 10d52bc2..d6637011 100644
--- a/src/serialization.hpp
+++ b/src/serialization.hpp
@@ -28,6 +28,7 @@
 #pragma GCC diagnostic pop
 
 #include "inferenceresponse.hpp"
+#include "inferencetensor.hpp"
 #include "kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "pocapiinternal.hpp"
 #include "profiler.hpp"
@@ -35,7 +36,6 @@
 #include "tensorinfo.hpp"
 
 namespace ovms {
-class InferenceTensor;
 
 template <typename T>
 class OutputGetter {
@@ -87,6 +87,8 @@ const std::string& getOutputMapKeyName(const std::string& first, const TensorInf
 template <typename T>
 Status serializePredictResponse(
     OutputGetter<T>& outputGetter,
+    const std::string& servableName,
+    model_version_t servableVersion,
     const tensor_map_t& outputMap,
     tensorflow::serving::PredictResponse* response,
     outputNameChooser_t outputNameChooser,
@@ -112,12 +114,16 @@ Status serializePredictResponse(
 template <typename T>
 Status serializePredictResponse(
     OutputGetter<T>& outputGetter,
+    const std::string& servableName,
+    model_version_t servableVersion,
     const tensor_map_t& outputMap,
     ::KFSResponse* response,
     outputNameChooser_t outputNameChooser,
     bool useSharedOutputContent = true) {
     OVMS_PROFILE_FUNCTION();
     Status status;
+    response->set_model_name(servableName);
+    response->set_model_version(std::to_string(servableVersion));
     ProtoGetter<::KFSResponse*, ::KFSResponse::InferOutputTensor&> protoGetter(response);
     for (const auto& [outputName, outputInfo] : outputMap) {
         ov::Tensor tensor;
@@ -141,12 +147,14 @@ Status serializePredictResponse(
 template <typename T>
 Status serializePredictResponse(
     OutputGetter<T>& outputGetter,
+    const std::string& servableName,
+    model_version_t servableVersion,
     const tensor_map_t& outputMap,
     InferenceResponse* response,
     outputNameChooser_t outputNameChooser) {
     OVMS_PROFILE_FUNCTION();
     Status status;
-    ProtoGetter<InferenceResponse*, InferenceTensor&> protoGetter(response);
+    uint32_t outputId = 0;
     for (const auto& [outputName, outputInfo] : outputMap) {
         ov::Tensor tensor;
         status = outputGetter.get(outputNameChooser(outputName, *outputInfo), tensor);
@@ -203,7 +211,9 @@ Status serializePredictResponse(
                 outputName, response->getServableName(), response->getServableVersion());
             return StatusCode::INTERNAL_ERROR;
         }
-        status = response->getOutput(outputInfo->getMappedName().c_str(), &outputTensor);
+        const std::string* outputNameFromCapiTensor = nullptr;
+        status = response->getOutput(outputId, &outputNameFromCapiTensor, &outputTensor);
+        ++outputId;  // TODO C-API test serialization 2 outputs
         if (!status.ok()) {
             SPDLOG_ERROR("Cannot serialize output with name:{} for servable name:{}; version:{}; error: cannot find inserted input",
                 outputName, response->getServableName(), response->getServableVersion());
diff --git a/src/server.cpp b/src/server.cpp
index fb35a7f4..31c25e6c 100644
--- a/src/server.cpp
+++ b/src/server.cpp
@@ -45,10 +45,10 @@
 #include "metric_module.hpp"
 #include "model_service.hpp"
 #include "modelmanager.hpp"
-#include "poc_api_impl.hpp"
 #include "prediction_service.hpp"
 #include "profiler.hpp"
 #include "servablemanagermodule.hpp"
+#include "server_options.hpp"
 #include "stringutils.hpp"
 #include "version.hpp"
 
@@ -119,7 +119,7 @@ static void onIllegal(int status) {
     shutdown_request = 2;
 }
 
-static void installSignalHandlers(ovms::Server& server) {
+static void installSignalHandlers() {
     static struct sigaction sigIntHandler;
     sigIntHandler.sa_handler = onInterrupt;
     sigemptyset(&sigIntHandler.sa_mask);
@@ -325,36 +325,41 @@ void Server::shutdownModules() {
 
 // OVMS Start
 int Server::start(int argc, char** argv) {
+    installSignalHandlers();
+
     ovms::CLIParser parser;
     ovms::GeneralOptionsImpl go;
     ovms::MultiModelOptionsImpl mmo;
     parser.parse(argc, argv);
     parser.prepare(&go, &mmo);
-    return start(&go, &mmo);
+
+    int ret = start(&go, &mmo);
+    ModulesShutdownGuard shutdownGuard(*this);
+    if (ret != 0) {
+        return ret;
+    }
+    while (!shutdown_request) {
+        std::this_thread::sleep_for(std::chrono::milliseconds(200));
+    }
+    if (shutdown_request == 2) {
+        SPDLOG_ERROR("Illegal operation. OVMS started on unsupported device");
+    }
+    SPDLOG_INFO("Shutting down");
+
+    return EXIT_SUCCESS;
 }
 
 // C-API Start
 int Server::start(GeneralOptionsImpl* go, MultiModelOptionsImpl* mmo) {
-    ovms::Server& server = ovms::Server::instance();
-    installSignalHandlers(server);
     try {
         auto& config = ovms::Config::instance();
         if (!config.parse(go, mmo))
             return EX_USAGE;
         configure_logger(config.logLevel(), config.logPath());
         logConfig(config);
-        ModulesShutdownGuard shutdownGuard(*this);
         auto retCode = this->startModules(config);
         if (retCode)
             return retCode;
-
-        while (!shutdown_request) {
-            std::this_thread::sleep_for(std::chrono::milliseconds(200));
-        }
-        if (shutdown_request == 2) {
-            SPDLOG_ERROR("Illegal operation. OVMS started on unsupported device");
-        }
-        SPDLOG_INFO("Shutting down");
     } catch (std::exception& e) {
         SPDLOG_ERROR("Exception catch: {} - will now terminate.", e.what());
         return EXIT_FAILURE;
@@ -364,5 +369,4 @@ int Server::start(GeneralOptionsImpl* go, MultiModelOptionsImpl* mmo) {
     }
     return EXIT_SUCCESS;
 }
-
 }  // namespace ovms
diff --git a/src/poc_api_impl.hpp b/src/server_options.hpp
similarity index 88%
rename from src/poc_api_impl.hpp
rename to src/server_options.hpp
index a4255930..b8418d14 100644
--- a/src/poc_api_impl.hpp
+++ b/src/server_options.hpp
@@ -21,12 +21,12 @@
 namespace ovms {
 
 struct GeneralOptionsImpl {
-    uint64_t grpcPort = 9178;
-    uint64_t restPort = 0;
+    uint32_t grpcPort = 9178;
+    uint32_t restPort = 0;
+    uint32_t grpcWorkers = 1;
     std::string grpcBindAddress = "0.0.0.0";
+    std::optional<uint32_t> restWorkers;
     std::string restBindAddress = "0.0.0.0";
-    uint grpcWorkers = 1;
-    std::optional<uint> restWorkers;
     bool metricsEnabled = false;
     std::string metricsList;
     std::string cpuExtensionLibraryPath;
@@ -36,7 +36,7 @@ struct GeneralOptionsImpl {
     std::string tracePath;
 #endif
     std::string grpcChannelArguments;
-    uint filesystemPollWaitSeconds = 1;
+    uint32_t filesystemPollWaitSeconds = 1;
     uint32_t sequenceCleanerPollWaitMinutes = 5;
     uint32_t resourcesCleanerPollWaitSeconds = 1;
     std::string cacheDir;
@@ -60,8 +60,4 @@ struct MultiModelOptionsImpl {
     std::string configPath;
 };
 
-struct ServerImpl {
-    int start(GeneralOptionsImpl*, MultiModelOptionsImpl*);
-};
-
 }  // namespace ovms
diff --git a/src/statefulmodelinstance.cpp b/src/statefulmodelinstance.cpp
index 8a237bff..fcceb4fb 100644
--- a/src/statefulmodelinstance.cpp
+++ b/src/statefulmodelinstance.cpp
@@ -35,15 +35,15 @@ const std::set<std::string> StatefulModelInstance::SPECIAL_INPUT_NAMES{"sequence
 
 const Status StatefulModelInstance::extractSequenceId(const tensorflow::TensorProto& proto, uint64_t& sequenceId) {
     if (!proto.tensor_shape().dim_size()) {
-        SPDLOG_DEBUG("[Model: {} version: {}] Sequence id tensor proto does not contain tensor shape information", getName(), getVersion());
+        SPDLOG_DEBUG("Sequence id tensor proto does not contain tensor shape information");
         return StatusCode::SPECIAL_INPUT_NO_TENSOR_SHAPE;
     } else if (proto.tensor_shape().dim_size() != 1) {
-        SPDLOG_DEBUG("[Model: {} version: {}] Sequence id tensor proto shape has invalid number of dimensions. Expecting shape with one dimension", getName(), getVersion());
+        SPDLOG_DEBUG("Sequence id tensor proto shape has invalid number of dimensions. Expecting shape with one dimension");
         return Status(StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, "Required shape for sequence_id is: (1)");
     }
 
     if (proto.tensor_shape().dim(0).size() != 1) {
-        SPDLOG_DEBUG("[Model: {} version: {}] Sequence id tensor proto shape has invalid shape. Expecting shape: (1)", getName(), getVersion());
+        SPDLOG_DEBUG("Sequence id tensor proto shape has invalid shape. Expecting shape: (1)");
         return Status(StatusCode::INVALID_SHAPE, "Required shape for sequence_id is: (1)");
     }
 
@@ -56,15 +56,15 @@ const Status StatefulModelInstance::extractSequenceId(const tensorflow::TensorPr
 
 const Status StatefulModelInstance::extractSequenceControlInput(const tensorflow::TensorProto& proto, uint32_t& sequenceControlInput) {
     if (proto.tensor_shape().dim_size() == 0) {
-        SPDLOG_DEBUG("[Model: {} version: {}] Sequence control tensor proto does not contain tensor shape information", getName(), getVersion());
+        SPDLOG_DEBUG("Sequence control tensor proto does not contain tensor shape information");
         return StatusCode::SPECIAL_INPUT_NO_TENSOR_SHAPE;
     } else if (proto.tensor_shape().dim_size() != 1) {
-        SPDLOG_DEBUG("[Model: {} version: {}] Sequence control tensor proto shape has invalid number of dimensions. Expecting shape with one dimension.", getName(), getVersion());
+        SPDLOG_DEBUG("Sequence control tensor proto shape has invalid number of dimensions. Expecting shape with one dimension.");
         return Status(StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, "Required shape for sequence_control_input is: (1)");
     }
 
     if (proto.tensor_shape().dim(0).size() != 1) {
-        SPDLOG_DEBUG("[Model: {} version: {}] Sequence control tensor proto shape has invalid shape. Expecting shape: (1)", getName(), getVersion());
+        SPDLOG_DEBUG("Sequence control tensor proto shape has invalid shape. Expecting shape: (1)");
         return Status(StatusCode::INVALID_SHAPE, "Required shape for sequence_control_input is: (1)");
     }
 
@@ -150,7 +150,7 @@ Status StatefulModelInstance::loadOVCompiledModel(const ModelConfig& config) {
 }
 
 template <>
-const Status StatefulModelInstance::validateSpecialKeys(const tensorflow::serving::PredictRequest* request, SequenceProcessingSpec& sequenceProcessingSpec) {
+const Status StatefulModelInstance::extractSpecialKeys(const tensorflow::serving::PredictRequest* request, SequenceProcessingSpec& sequenceProcessingSpec) {
     uint64_t sequenceId = 0;
     uint32_t sequenceControlInput = 0;
     Status status;
@@ -180,118 +180,84 @@ const Status StatefulModelInstance::validateSpecialKeys(const tensorflow::servin
     return StatusCode::OK;
 }
 
-template <typename RequestType>
-const Status StatefulModelInstance::validate(const RequestType* request, SequenceProcessingSpec& sequenceProcessingSpec) {
-    OVMS_PROFILE_FUNCTION();
-    auto status = validateSpecialKeys(request, sequenceProcessingSpec);
-    if (!status.ok())
-        return status;
-
-    return request_validation_utils::validate(
-        *request,
-        getInputsInfo(),
-        getName(),
-        getVersion(),
-        SPECIAL_INPUT_NAMES,
-        getModelConfig().getBatchingMode(),
-        getModelConfig().getShapes());
+const std::set<std::string>& StatefulModelInstance::getOptionalInputNames() {
+    return SPECIAL_INPUT_NAMES;
 }
-
-Status StatefulModelInstance::infer(const tensorflow::serving::PredictRequest* requestProto,
-    tensorflow::serving::PredictResponse* responseProto,
-    std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr) {
+template <>
+StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>::StatefulRequestProcessor(SequenceManager& sequenceManager) :
+    sequenceManager(sequenceManager) {
+}
+template <>
+Status StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>::extractRequestParameters(const tensorflow::serving::PredictRequest* request) {
     OVMS_PROFILE_FUNCTION();
-    enum : unsigned int {
-        GET_INFER_REQUEST,
-        PREPROCESS,
-        DESERIALIZE,
-        PREDICTION,
-        SERIALIZE,
-        POSTPROCESS,
-        TIMER_END
-    };
-    Timer<TIMER_END> timer;
-    using std::chrono::microseconds;
-    SequenceProcessingSpec sequenceProcessingSpec;
-    auto status = validate(requestProto, sequenceProcessingSpec);
-    if (!status.ok())
-        return status;
-
-    std::unique_lock<std::mutex> sequenceManagerLock(sequenceManager->getMutex());
-    status = sequenceManager->processRequestedSpec(sequenceProcessingSpec);
+    auto status = StatefulModelInstance::extractSpecialKeys(request, sequenceProcessingSpec);
+    return status;
+}
+template <>
+Status StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>::prepare() {
+    sequenceManagerLock = std::make_unique<std::unique_lock<std::mutex>>(sequenceManager.getMutex());
+    auto status = sequenceManager.processRequestedSpec(sequenceProcessingSpec);
     if (!status.ok())
         return status;
-    const uint64_t sequenceId = sequenceProcessingSpec.getSequenceId();
-    if (!sequenceManager->sequenceExists(sequenceId))
+    this->sequenceId = sequenceProcessingSpec.getSequenceId();
+    if (!sequenceManager.sequenceExists(this->sequenceId))
         return StatusCode::INTERNAL_ERROR;
-    Sequence& sequence = sequenceManager->getSequence(sequenceId);
-
-    std::unique_lock<std::mutex> sequenceLock(sequence.getMutex());
-    sequenceManagerLock.unlock();
-
-    timer.start(GET_INFER_REQUEST);
-    ExecutingStreamIdGuard executingStreamIdGuard(getInferRequestsQueue(), this->getMetricReporter());
-    int executingInferId = executingStreamIdGuard.getId();
-    ov::InferRequest& inferRequest = executingStreamIdGuard.getInferRequest();
-    timer.stop(GET_INFER_REQUEST);
-    double getInferRequestTime = timer.elapsed<microseconds>(GET_INFER_REQUEST);
-    OBSERVE_IF_ENABLED(this->getMetricReporter().waitForInferReqTime, getInferRequestTime);
-    SPDLOG_DEBUG("Getting infer req duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, getInferRequestTime / 1000);
-
-    timer.start(PREPROCESS);
-    status = preInferenceProcessing(inferRequest, sequence, sequenceProcessingSpec);
-    timer.stop(PREPROCESS);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Preprocessing duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREPROCESS) / 1000);
+    // TODO should be able to search & get in one go
+    sequence = &sequenceManager.getSequence(this->sequenceId);
 
-    timer.start(DESERIALIZE);
-    InputSink<ov::InferRequest&> inputSink(inferRequest);
-    bool isPipeline = false;
-    status = deserializePredictRequest<ConcreteTensorProtoDeserializator>(*requestProto, getInputsInfo(), inputSink, isPipeline);
-    timer.stop(DESERIALIZE);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Deserialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(DESERIALIZE) / 1000);
-
-    timer.start(PREDICTION);
-    status = performInference(inferRequest);
-    timer.stop(PREDICTION);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Prediction duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(PREDICTION) / 1000);
-
-    timer.start(SERIALIZE);
-    OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    status = serializePredictResponse(outputGetter, getOutputsInfo(), responseProto, getTensorInfoName);
-    timer.stop(SERIALIZE);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Serialization duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(SERIALIZE) / 1000);
-
-    timer.start(POSTPROCESS);
-    status = postInferenceProcessing(responseProto, inferRequest, sequence, sequenceProcessingSpec);
-    timer.stop(POSTPROCESS);
-    if (!status.ok())
-        return status;
-    SPDLOG_DEBUG("Postprocessing duration in model {}, version {}, nireq {}: {:.3f} ms",
-        requestProto->model_spec().name(), getVersion(), executingInferId, timer.elapsed<microseconds>(POSTPROCESS) / 1000);
-
-    sequenceLock.unlock();
+    sequenceLock = std::make_unique<std::unique_lock<std::mutex>>(sequence->getMutex());
+    sequenceManagerLock->unlock();
+    return StatusCode::OK;
+}
+template <>
+Status StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>::preInferenceProcessing(ov::InferRequest& inferRequest) {
+    if (sequenceProcessingSpec.getSequenceControlInput() == SEQUENCE_START) {
+        // On SEQUENCE_START reset memory state of infer request to default
+        for (auto&& state : inferRequest.query_state()) {
+            state.reset();
+        }
+    } else {
+        // For next requests in the sequence set infer request memory state to the last state saved by the sequence
+        const sequence_memory_state_t& sequenceMemoryState = sequence->getMemoryState();
+        for (auto&& state : inferRequest.query_state()) {
+            auto stateName = state.get_name();
+            if (!sequenceMemoryState.count(stateName))
+                return StatusCode::INTERNAL_ERROR;
+            state.set_state(sequenceMemoryState.at(stateName));
+        }
+    }
+    return StatusCode::OK;
+}
+template <>
+Status StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>::postInferenceProcessing(tensorflow::serving::PredictResponse* response, ov::InferRequest& inferRequest) {
+    // Reset inferRequest states on SEQUENCE_END
     if (sequenceProcessingSpec.getSequenceControlInput() == SEQUENCE_END) {
-        sequenceManagerLock.lock();
-        status = sequenceManager->removeSequence(sequenceId);
-        if (!status.ok())
-            return status;
+        SPDLOG_DEBUG("Received SEQUENCE_END signal. Reseting model state");
+        for (auto&& state : inferRequest.query_state()) {
+            state.reset();
+        }
+    } else {
+        auto modelState = inferRequest.query_state();
+        sequence->updateMemoryState(modelState);
     }
-
+    // Include sequence_id in server response
+    auto& tensorProto = (*response->mutable_outputs())["sequence_id"];
+    tensorProto.mutable_tensor_shape()->add_dim()->set_size(1);
+    tensorProto.set_dtype(tensorflow::DataType::DT_UINT64);
+    tensorProto.add_uint64_val(sequenceProcessingSpec.getSequenceId());
     return StatusCode::OK;
 }
+template <>
+Status StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>::release() {
+    SPDLOG_DEBUG("Received SEQUENCE_END signal. Removing sequence");
+    sequenceLock->unlock();
+    Status status;
+    if (sequenceProcessingSpec.getSequenceControlInput() == SEQUENCE_END) {
+        sequenceManagerLock->lock();
+        status = sequenceManager.removeSequence(this->sequenceId);
+    }
+    return status;
+}
 
 const Status StatefulModelInstance::preInferenceProcessing(ov::InferRequest& inferRequest, Sequence& sequence,
     SequenceProcessingSpec& sequenceProcessingSpec) {
@@ -334,4 +300,8 @@ const Status StatefulModelInstance::postInferenceProcessing(tensorflow::serving:
 
     return StatusCode::OK;
 }
+
+std::unique_ptr<RequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>> StatefulModelInstance::createRequestProcessor(const tensorflow::serving::PredictRequest*, tensorflow::serving::PredictResponse*) {
+    return std::make_unique<StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>>(*this->getSequenceManager());
+}
 }  // namespace ovms
diff --git a/src/statefulmodelinstance.hpp b/src/statefulmodelinstance.hpp
index 4f1e3ddb..69b5afd3 100644
--- a/src/statefulmodelinstance.hpp
+++ b/src/statefulmodelinstance.hpp
@@ -22,10 +22,9 @@
 #include "global_sequences_viewer.hpp"
 #include "modelinstance.hpp"
 #include "sequence_manager.hpp"
+#include "sequence_processing_spec.hpp"
 
 namespace ovms {
-class MetricRegistry;
-class MetricConfig;
 class ModelConfig;
 class StatefulModelInstance : public ModelInstance {
     static const std::set<std::string> SPECIAL_INPUT_NAMES;
@@ -44,9 +43,9 @@ public:
         return this->sequenceManager;
     }
 
-    const Status extractSequenceId(const tensorflow::TensorProto& proto, uint64_t& sequenceId);
+    static const Status extractSequenceId(const tensorflow::TensorProto& proto, uint64_t& sequenceId);
 
-    const Status extractSequenceControlInput(const tensorflow::TensorProto& proto, uint32_t& sequenceControlInput);
+    static const Status extractSequenceControlInput(const tensorflow::TensorProto& proto, uint32_t& sequenceControlInput);
     /*
     Performs pre inference operations:
         - for SEQUENCE_START control input - reset InferRequest memory state
@@ -67,10 +66,6 @@ public:
     const Status postInferenceProcessing(tensorflow::serving::PredictResponse* response,
         ov::InferRequest& inferRequest, Sequence& sequence, SequenceProcessingSpec& sequenceProcessingSpec);
 
-    Status infer(const tensorflow::serving::PredictRequest* requestProto,
-        tensorflow::serving::PredictResponse* responseProto,
-        std::unique_ptr<ModelInstanceUnloadGuard>& modelUnloadGuardPtr) override;
-
     Status loadModel(const ModelConfig& config) override;
 
     Status reloadModel(const ModelConfig& config, const DynamicModelParameter& parameter = DynamicModelParameter()) override;
@@ -88,15 +83,32 @@ protected:
 
     GlobalSequencesViewer* globalSequencesViewer;
 
-    template <typename RequestType>
-    const Status validate(const RequestType* request, SequenceProcessingSpec& processingSpec);
-
     Status loadModelImpl(const ModelConfig& config, const DynamicModelParameter& parameter = DynamicModelParameter()) override;
 
     Status loadOVCompiledModel(const ModelConfig& config) override;
 
-private:
+public:
     template <typename RequestType>
-    const Status validateSpecialKeys(const RequestType* request, SequenceProcessingSpec& sequenceProcessingSpec);
+    static const Status extractSpecialKeys(const RequestType* request, SequenceProcessingSpec& sequenceProcessingSpec);
+
+    std::unique_ptr<RequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse>> createRequestProcessor(const tensorflow::serving::PredictRequest*, tensorflow::serving::PredictResponse*) override;
+    const std::set<std::string>& getOptionalInputNames() override;
+};
+
+template <typename RequestType, typename ResponseType>
+struct StatefulRequestProcessor : public RequestProcessor<RequestType, ResponseType> {
+    SequenceManager& sequenceManager;
+    std::unique_ptr<std::unique_lock<std::mutex>> sequenceManagerLock;
+    std::unique_ptr<std::unique_lock<std::mutex>> sequenceLock;
+    SequenceProcessingSpec sequenceProcessingSpec;
+    Sequence* sequence;
+    uint64_t sequenceId;
+
+    StatefulRequestProcessor(SequenceManager& sequenceManager);
+    Status extractRequestParameters(const RequestType* request) override;
+    Status prepare() override;
+    Status preInferenceProcessing(ov::InferRequest& inferRequest) override;
+    Status postInferenceProcessing(ResponseType* response, ov::InferRequest& inferRequest) override;
+    Status release() override;
 };
 }  // namespace ovms
diff --git a/src/status.cpp b/src/status.cpp
index f500292c..c283a57a 100644
--- a/src/status.cpp
+++ b/src/status.cpp
@@ -94,6 +94,8 @@ const std::unordered_map<const StatusCode, const std::string> Status::statusMess
     {StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS, "Invalid number of shape dimensions"},
     {StatusCode::INVALID_BATCH_SIZE, "Invalid input batch size"},
     {StatusCode::INVALID_SHAPE, "Invalid input shape"},
+    {StatusCode::INVALID_BUFFER_TYPE, "Invalid input buffer type"},
+    {StatusCode::INVALID_DEVICE_ID, "Invalid input buffer device id"},
     {StatusCode::INVALID_PRECISION, "Invalid input precision"},
     {StatusCode::INVALID_VALUE_COUNT, "Invalid number of values in tensor proto container"},
     {StatusCode::INVALID_CONTENT_SIZE, "Invalid content size of tensor proto"},
@@ -264,11 +266,19 @@ const std::unordered_map<const StatusCode, const std::string> Status::statusMess
     {StatusCode::DOUBLE_BUFFER_SET, "Cannot set buffer more than once to the same tensor"},
     {StatusCode::DOUBLE_TENSOR_INSERT, "Cannot insert more than one tensor with the same name"},
     {StatusCode::DOUBLE_PARAMETER_INSERT, "Cannot insert more than one parameter with the same name"},
+    {StatusCode::NONEXISTENT_BUFFER_FOR_REMOVAL, "Tried to remove nonexisting buffer"},
+    {StatusCode::NONEXISTENT_DATA, "Tried to use nonexisting data"},
+    {StatusCode::NONEXISTENT_STRING, "Tried to use nonexisting string"},
+    {StatusCode::NONEXISTENT_NUMBER, "Tried to use nonexisting number"},
+    {StatusCode::NONEXISTENT_OPTIONS, "Tried to use nonexisting options"},
+    {StatusCode::NONEXISTENT_PARAMETER_FOR_REMOVAL, "Tried to remove nonexisting parameter"},
+    {StatusCode::NONEXISTENT_RESPONSE, "Tried to use nonexisting response"},
+    {StatusCode::NONEXISTENT_REQUEST, "Tried to use nonexisting request"},
+    {StatusCode::NONEXISTENT_SERVER, "Tried to use nonexisting server"},
+    {StatusCode::NONEXISTENT_TABLE, "Tried to use nonexisting table"},
     {StatusCode::NONEXISTENT_TENSOR, "Tried to get nonexisting tensor"},
     {StatusCode::NONEXISTENT_TENSOR_FOR_SET_BUFFER, "Tried to set buffer for nonexisting tensor"},
     {StatusCode::NONEXISTENT_TENSOR_FOR_REMOVE_BUFFER, "Tried to remove buffer for nonexisting tensor"},
     {StatusCode::NONEXISTENT_TENSOR_FOR_REMOVAL, "Tried to remove nonexisting tensor"},
-    {StatusCode::NONEXISTENT_BUFFER_FOR_REMOVAL, "Tried to remove nonexisting buffer"},
-    {StatusCode::NONEXISTENT_PARAMETER_FOR_REMOVAL, "Tried to remove nonexisting parameter"},
 };
 }  // namespace ovms
diff --git a/src/status.hpp b/src/status.hpp
index 6c7a26a7..947df37f 100644
--- a/src/status.hpp
+++ b/src/status.hpp
@@ -97,6 +97,8 @@ enum class StatusCode {
     INVALID_VALUE_COUNT,            /*!< Invalid value count error status for uint16 and half float data types */
     INVALID_CONTENT_SIZE,           /*!< Invalid content size error status for types using tensor_content() */
     INVALID_MESSAGE_STRUCTURE,      /*!< Buffers can't be both in raw_input_content & input tensor content */
+    INVALID_BUFFER_TYPE,            /*!< Invalid buffer type */
+    INVALID_DEVICE_ID,              /*!< Invalid buffer device id */
 
     // Deserialization
     OV_UNSUPPORTED_DESERIALIZATION_PRECISION, /*!< Unsupported deserialization precision, theoretically should never be returned since ModelInstance::validation checks against model precision */
@@ -275,12 +277,20 @@ enum class StatusCode {
     DOUBLE_BUFFER_SET,
     DOUBLE_TENSOR_INSERT,
     DOUBLE_PARAMETER_INSERT,
+    NONEXISTENT_BUFFER_FOR_REMOVAL,
+    NONEXISTENT_DATA,
+    NONEXISTENT_STRING,
+    NONEXISTENT_NUMBER,
+    NONEXISTENT_OPTIONS,
+    NONEXISTENT_PARAMETER_FOR_REMOVAL,  // rename to non existen parameter
+    NONEXISTENT_SERVER,
+    NONEXISTENT_RESPONSE,
+    NONEXISTENT_REQUEST,
+    NONEXISTENT_TABLE,
     NONEXISTENT_TENSOR,
     NONEXISTENT_TENSOR_FOR_SET_BUFFER,
     NONEXISTENT_TENSOR_FOR_REMOVE_BUFFER,
     NONEXISTENT_TENSOR_FOR_REMOVAL,
-    NONEXISTENT_BUFFER_FOR_REMOVAL,
-    NONEXISTENT_PARAMETER_FOR_REMOVAL,
 
     STATUS_CODE_END
 };
diff --git a/src/test/c_api/config_standard_dummy.json b/src/test/c_api/config_standard_dummy.json
new file mode 100644
index 00000000..cb1c3463
--- /dev/null
+++ b/src/test/c_api/config_standard_dummy.json
@@ -0,0 +1,8 @@
+{
+    "model_config_list": [
+        {"config": {
+                "name": "dummy",
+                "base_path": "/ovms/src/test/dummy",
+                "shape": "(1, 10)"}}
+    ]
+}
diff --git a/src/test/c_api_tests.cpp b/src/test/c_api_tests.cpp
index b1669785..7f9266f3 100644
--- a/src/test/c_api_tests.cpp
+++ b/src/test/c_api_tests.cpp
@@ -16,27 +16,509 @@
 
 #include <gtest/gtest.h>
 
-#include "../config.hpp"
-#include "../poc_api_impl.hpp"
+// TODO we should not include classes from OVMS here
+// consider how to workaround test_utils
+#include "../inferenceresponse.hpp"
+#include "../pocapi.hpp"
+#include "test_utils.hpp"
 
 using namespace ovms;
+using testing::ElementsAreArray;
 
-class CapiConfigTest : public ::testing::Test {
-protected:
-    void SetUp() override {
+static void testDefaultSingleModelOptions(MultiModelOptionsImpl* mmo) {
+    EXPECT_EQ(mmo->modelName, "");
+    EXPECT_EQ(mmo->modelPath, "");
+    EXPECT_EQ(mmo->batchSize, "");
+    EXPECT_EQ(mmo->shape, "");
+    EXPECT_EQ(mmo->layout, "");
+    EXPECT_EQ(mmo->modelVersionPolicy, "");
+    EXPECT_EQ(mmo->nireq, 0);
+    EXPECT_EQ(mmo->targetDevice, "");
+    EXPECT_EQ(mmo->pluginConfig, "");
+    EXPECT_EQ(mmo->stateful, std::nullopt);
+    EXPECT_EQ(mmo->lowLatencyTransformation, std::nullopt);
+    EXPECT_EQ(mmo->maxSequenceNumber, std::nullopt);
+    EXPECT_EQ(mmo->idleSequenceCleanup, std::nullopt);
+}
+
+TEST(CApiConfigTest, MultiModelConfiguration) {
+    OVMS_ServerGeneralOptions* _go = 0;
+    OVMS_ServerMultiModelOptions* _mmo = 0;
+
+    ASSERT_EQ(OVMS_ServerGeneralOptionsNew(&_go), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsNew(&_mmo), nullptr);
+    ASSERT_NE(_go, nullptr);
+    ASSERT_NE(_mmo, nullptr);
+
+    GeneralOptionsImpl* go = reinterpret_cast<GeneralOptionsImpl*>(_go);
+    MultiModelOptionsImpl* mmo = reinterpret_cast<MultiModelOptionsImpl*>(_mmo);
+
+    // Test default values
+    EXPECT_EQ(go->grpcPort, 9178);
+    EXPECT_EQ(go->restPort, 0);
+    EXPECT_EQ(go->grpcWorkers, 1);
+    EXPECT_EQ(go->grpcBindAddress, "0.0.0.0");
+    EXPECT_EQ(go->restWorkers, std::nullopt);
+    EXPECT_EQ(go->restBindAddress, "0.0.0.0");
+    EXPECT_EQ(go->metricsEnabled, false);
+    EXPECT_EQ(go->metricsList, "");
+    EXPECT_EQ(go->cpuExtensionLibraryPath, "");
+    EXPECT_EQ(go->logLevel, "INFO");
+    EXPECT_EQ(go->logPath, "");
+    // trace path  // not tested since it is not supported in C-API
+    EXPECT_EQ(go->grpcChannelArguments, "");
+    EXPECT_EQ(go->filesystemPollWaitSeconds, 1);
+    EXPECT_EQ(go->sequenceCleanerPollWaitMinutes, 5);
+    EXPECT_EQ(go->resourcesCleanerPollWaitSeconds, 1);
+    EXPECT_EQ(go->cacheDir, "");
+
+    testDefaultSingleModelOptions(mmo);
+    EXPECT_EQ(mmo->configPath, "");
+
+    // Set non default values
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcPort(_go, 5555), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetRestPort(_go, 6666), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcWorkers(_go, 30), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcBindAddress(_go, "2.2.2.2"), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetRestWorkers(_go, 31), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetRestBindAddress(_go, "3.3.3.3"), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcChannelArguments(_go, "grpcargs"), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetFileSystemPollWaitSeconds(_go, 2), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetSequenceCleanerPollWaitMinutes(_go, 3), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetCustomNodeResourcesCleanerIntervalSeconds(_go, 4), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetCpuExtensionPath(_go, "/ovms/src/test"), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetCacheDir(_go, "/tmp/cache"), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetLogLevel(_go, OVMS_LOG_TRACE), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetLogPath(_go, "/logs"), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsSetConfigPath(_mmo, "/config"), nullptr);
+
+    // Test non default values
+    EXPECT_EQ(go->grpcPort, 5555);
+    EXPECT_EQ(go->restPort, 6666);
+    EXPECT_EQ(go->grpcWorkers, 30);
+    EXPECT_EQ(go->grpcBindAddress, "2.2.2.2");
+    EXPECT_EQ(go->restWorkers, 31);
+    EXPECT_EQ(go->restBindAddress, "3.3.3.3");
+    // EXPECT_EQ(go->metricsEnabled, false);  // TODO: enable testing once metrics will be configurable via api
+    // EXPECT_EQ(go->metricsList, "");
+    EXPECT_EQ(go->cpuExtensionLibraryPath, "/ovms/src/test");
+    EXPECT_EQ(go->logLevel, "TRACE");
+    EXPECT_EQ(go->logPath, "/logs");
+    // trace path  // not tested since it is not supported in C-API
+    EXPECT_EQ(go->grpcChannelArguments, "grpcargs");
+    EXPECT_EQ(go->filesystemPollWaitSeconds, 2);
+    EXPECT_EQ(go->sequenceCleanerPollWaitMinutes, 3);
+    EXPECT_EQ(go->resourcesCleanerPollWaitSeconds, 4);
+    EXPECT_EQ(go->cacheDir, "/tmp/cache");
+
+    testDefaultSingleModelOptions(mmo);
+    EXPECT_EQ(mmo->configPath, "/config");
+
+    // Test config parser
+    ConstructorEnabledConfig cfg;
+    ASSERT_TRUE(cfg.parse(go, mmo));
+    EXPECT_EQ(cfg.port(), 5555);
+    EXPECT_EQ(cfg.restPort(), 6666);
+    EXPECT_EQ(cfg.grpcWorkers(), 30);
+    EXPECT_EQ(cfg.grpcBindAddress(), "2.2.2.2");
+    EXPECT_EQ(cfg.restWorkers(), 31);
+    EXPECT_EQ(cfg.restBindAddress(), "3.3.3.3");
+    // EXPECT_EQ(go->metricsEnabled, false);  // TODO: enable testing once metrics will be configurable via api
+    // EXPECT_EQ(go->metricsList, "");
+    EXPECT_EQ(cfg.cpuExtensionLibraryPath(), "/ovms/src/test");
+    EXPECT_EQ(cfg.logLevel(), "TRACE");
+    EXPECT_EQ(cfg.logPath(), "/logs");
+    // trace path  // not tested since it is not supported in C-API
+    EXPECT_EQ(cfg.grpcChannelArguments(), "grpcargs");
+    EXPECT_EQ(cfg.filesystemPollWaitSeconds(), 2);
+    EXPECT_EQ(cfg.sequenceCleanerPollWaitMinutes(), 3);
+    EXPECT_EQ(cfg.resourcesCleanerPollWaitSeconds(), 4);
+    EXPECT_EQ(cfg.cacheDir(), "/tmp/cache");
+
+    EXPECT_EQ(cfg.modelName(), "");
+    EXPECT_EQ(cfg.modelPath(), "");
+    EXPECT_EQ(cfg.batchSize(), "0");
+    EXPECT_EQ(cfg.shape(), "");
+    EXPECT_EQ(cfg.layout(), "");
+    EXPECT_EQ(cfg.modelVersionPolicy(), "");
+    EXPECT_EQ(cfg.nireq(), 0);
+    EXPECT_EQ(cfg.targetDevice(), "CPU");
+    EXPECT_EQ(cfg.pluginConfig(), "");
+    EXPECT_FALSE(cfg.stateful());
+    EXPECT_FALSE(cfg.lowLatencyTransformation());
+    EXPECT_EQ(cfg.maxSequenceNumber(), ovms::DEFAULT_MAX_SEQUENCE_NUMBER);
+    EXPECT_TRUE(cfg.idleSequenceCleanup());
+
+    EXPECT_EQ(cfg.configPath(), "/config");
+
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsDelete(_mmo), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsDelete(_go), nullptr);
+}
+
+TEST(CApiConfigTest, SingleModelConfiguration) {
+    GTEST_SKIP() << "Use C-API to initialize in next stages, currently not supported";
+}
+
+TEST(CApiStartTest, InitializingMultipleServers) {
+    OVMS_Server* srv1 = 0;
+    OVMS_Server* srv2 = 0;
+
+    ASSERT_EQ(OVMS_ServerNew(&srv1), nullptr);
+    ASSERT_EQ(OVMS_ServerNew(&srv2), nullptr);
+    ASSERT_EQ(srv1, srv2);
+    ASSERT_EQ(OVMS_ServerDelete(srv1), nullptr);
+}
+
+TEST(CApiStartTest, StartFlow) {
+    OVMS_Server* srv = 0;
+    OVMS_ServerGeneralOptions* go = 0;
+    OVMS_ServerMultiModelOptions* mmo = 0;
+
+    ASSERT_EQ(OVMS_ServerNew(&srv), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsNew(&go), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsNew(&mmo), nullptr);
+
+    ASSERT_NE(srv, nullptr);
+    ASSERT_NE(go, nullptr);
+    ASSERT_NE(mmo, nullptr);
+
+    // Cannot start due to configuration error
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcPort(go, 5555), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetRestPort(go, 5555), nullptr);  // The same port
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsSetConfigPath(mmo, "/ovms/src/test/c_api/config.json"), nullptr);
+
+    // Expect fail
+    // TODO: Check exact error code and details once error reporting becomes ready
+    ASSERT_NE(OVMS_ServerStartFromConfigurationFile(srv, go, mmo), nullptr);
+
+    // Fix and expect ok
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetRestPort(go, 6666), nullptr);  // Different port
+    ASSERT_EQ(OVMS_ServerStartFromConfigurationFile(srv, go, mmo), nullptr);
+
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsDelete(mmo), nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsDelete(go), nullptr);
+    ASSERT_EQ(OVMS_ServerDelete(srv), nullptr);
+}
+
+class CapiInference : public ::testing::Test {};
+
+TEST_F(CapiInference, Basic) {
+    // request creation
+    OVMS_InferenceRequest* request{nullptr};
+    OVMS_Status* status = OVMS_InferenceRequestNew(&request, "dummy", 1);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_NE(nullptr, request);
+
+    // adding input
+    status = OVMS_InferenceRequestAddInput(request, DUMMY_MODEL_INPUT_NAME, OVMS_DATATYPE_FP32, DUMMY_MODEL_SHAPE.data(), DUMMY_MODEL_SHAPE.size());
+    ASSERT_EQ(nullptr, status);
+    // setting buffer
+    std::array<float, DUMMY_MODEL_INPUT_SIZE> data{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
+    uint32_t notUsedNum = 0;
+    status = OVMS_InferenceRequestInputSetData(request, DUMMY_MODEL_INPUT_NAME, reinterpret_cast<void*>(data.data()), sizeof(float) * data.size(), OVMS_BUFFERTYPE_CPU, notUsedNum);
+    ASSERT_EQ(nullptr, status);
+    // add parameters
+    const uint64_t sequenceId{42};
+    status = OVMS_InferenceRequestAddParameter(request, "sequence_id", OVMS_DATATYPE_U64, reinterpret_cast<const void*>(&sequenceId), sizeof(sequenceId));
+    ASSERT_EQ(nullptr, status);
+    // 2nd time should get error
+    status = OVMS_InferenceRequestAddParameter(request, "sequence_id", OVMS_DATATYPE_U64, reinterpret_cast<const void*>(&sequenceId), sizeof(sequenceId));
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    const uint32_t sequenceControl{1};  // SEQUENCE_START
+    status = OVMS_InferenceRequestAddParameter(request, "sequence_control_input", OVMS_DATATYPE_U32, reinterpret_cast<const void*>(&sequenceControl), sizeof(sequenceControl));
+    ASSERT_EQ(nullptr, status);
+
+    //////////////////
+    //  INFERENCE
+    //////////////////
+    // remove when C-API start implemented
+    std::string port = "9000";
+    randomizePort(port);
+    // prepare options
+    OVMS_ServerGeneralOptions* go = 0;
+    OVMS_ServerMultiModelOptions* mmo = 0;
+    ASSERT_EQ(OVMS_ServerGeneralOptionsNew(&go), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsNew(&mmo), nullptr);
+    ASSERT_NE(go, nullptr);
+    ASSERT_NE(mmo, nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcPort(go, std::stoi(port)), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsSetConfigPath(mmo, "/ovms/src/test/c_api/config_standard_dummy.json"), nullptr);
+
+    OVMS_Server* cserver = nullptr;
+    ASSERT_EQ(OVMS_ServerNew(&cserver), nullptr);
+    ASSERT_EQ(OVMS_ServerStartFromConfigurationFile(cserver, go, mmo), nullptr);
+
+    OVMS_InferenceResponse* response = nullptr;
+    status = OVMS_Inference(cserver, request, &response);
+    ASSERT_EQ(nullptr, status);
+    // verify GetOutputCount
+    uint32_t outputCount = 42;
+    status = OVMS_InferenceResponseGetOutputCount(response, &outputCount);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(outputCount, 1);
+    // verify GetParameterCount
+    uint32_t parameterCount = 42;
+    status = OVMS_InferenceResponseGetParameterCount(response, &parameterCount);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(0, parameterCount);
+    // verify GetOutput
+    const void* voutputData;
+    size_t bytesize = 42;
+    uint32_t outputId = 0;
+    OVMS_DataType datatype = (OVMS_DataType)199;
+    const uint64_t* shape{nullptr};
+    uint32_t dimCount = 42;
+    BufferType bufferType = (BufferType)199;
+    uint32_t deviceId = 42;
+    const char* outputName{nullptr};
+    status = OVMS_InferenceResponseGetOutput(response, outputId, &outputName, &datatype, &shape, &dimCount, &voutputData, &bytesize, &bufferType, &deviceId);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(std::string(DUMMY_MODEL_OUTPUT_NAME), outputName);
+    EXPECT_EQ(datatype, OVMS_DATATYPE_FP32);
+    EXPECT_EQ(dimCount, 2);
+    EXPECT_EQ(bufferType, OVMS_BUFFERTYPE_CPU);
+    EXPECT_EQ(deviceId, 0);
+
+    for (size_t i = 0; i < DUMMY_MODEL_SHAPE.size(); ++i) {
+        EXPECT_EQ(DUMMY_MODEL_SHAPE[i], shape[i]) << "Different at:" << i << " place.";
     }
-};
+    const float* outputData = reinterpret_cast<const float*>(voutputData);
+    ASSERT_EQ(bytesize, sizeof(float) * DUMMY_MODEL_INPUT_SIZE);
+    for (size_t i = 0; i < data.size(); ++i) {
+        EXPECT_EQ(data[i] + 1, outputData[i]) << "Different at:" << i << " place.";
+    }
+
+    ///////////////
+    // CLEANUP
+    ///////////////
+    // cleanup response
+    status = OVMS_InferenceResponseDelete(response);
+    ASSERT_EQ(nullptr, status);
+    // cleanup request
+    // here we will add additional inputs to verify 2 ways of cleanup
+    // - direct call to remove whole request
+    // - separate calls to remove partial data
+    //
+    // here we will just add inputs to remove them later
+    // one original will be removed together with buffer during whole request removal
+    // one will be removed together with request but without buffer attached
+    // one will be removed with buffer directly
+    // one will be removed without buffer directly
+    status = OVMS_InferenceRequestAddInput(request, "INPUT_WITHOUT_BUFFER_REMOVED_WITH_REQUEST", OVMS_DATATYPE_FP32, DUMMY_MODEL_SHAPE.data(), DUMMY_MODEL_SHAPE.size());
+    ASSERT_EQ(nullptr, status);
+    status = OVMS_InferenceRequestAddInput(request, "INPUT_WITH_BUFFER_REMOVED_DIRECTLY", OVMS_DATATYPE_FP32, DUMMY_MODEL_SHAPE.data(), DUMMY_MODEL_SHAPE.size());
+    ASSERT_EQ(nullptr, status);
+    status = OVMS_InferenceRequestAddInput(request, "INPUT_WITHOUT_BUFFER_REMOVED_DIRECTLY", OVMS_DATATYPE_FP32, DUMMY_MODEL_SHAPE.data(), DUMMY_MODEL_SHAPE.size());
+    ASSERT_EQ(nullptr, status);
+    status = OVMS_InferenceRequestInputSetData(request, "INPUT_WITH_BUFFER_REMOVED_DIRECTLY", reinterpret_cast<void*>(data.data()), sizeof(float) * data.size(), OVMS_BUFFERTYPE_CPU, notUsedNum);
+    ASSERT_EQ(nullptr, status);
+    // we will add buffer and remove it to check separate buffer removal
+    status = OVMS_InferenceRequestInputSetData(request, "INPUT_WITHOUT_BUFFER_REMOVED_DIRECTLY", reinterpret_cast<void*>(data.data()), sizeof(float) * data.size(), OVMS_BUFFERTYPE_CPU, notUsedNum);
+    ASSERT_EQ(nullptr, status);
+
+    status = OVMS_InferenceRequestInputRemoveData(request, "INPUT_WITHOUT_BUFFER_REMOVED_DIRECTLY");
+    ASSERT_EQ(nullptr, status);
+    // second time we should get error
+    status = OVMS_InferenceRequestInputRemoveData(request, "INPUT_WITHOUT_BUFFER_REMOVED_DIRECTLY");
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    status = OVMS_InferenceRequestRemoveInput(request, "INPUT_WITHOUT_BUFFER_REMOVED_DIRECTLY");
+    ASSERT_EQ(nullptr, status);
+    status = OVMS_InferenceRequestRemoveInput(request, "INPUT_WITH_BUFFER_REMOVED_DIRECTLY");
+    ASSERT_EQ(nullptr, status);
+    // we will remove 1 of two parameters
+    status = OVMS_InferenceRequestRemoveParameter(request, "sequence_id");
+    ASSERT_EQ(nullptr, status);
+    // 2nd time should report error
+    status = OVMS_InferenceRequestRemoveParameter(request, "sequence_id");
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+
+    status = OVMS_InferenceRequestRemoveInput(request, "NONEXISTENT_TENSOR");
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    status = OVMS_InferenceRequestDelete(request);
+    ASSERT_EQ(nullptr, status);
+
+    ASSERT_EQ(OVMS_ServerDelete(cserver), nullptr);
+}
+
+TEST_F(CapiInference, NegativeInference) {
+    // first start OVMS
+    std::string port = "9000";
+    randomizePort(port);
+    // prepare options
+    OVMS_ServerGeneralOptions* go = 0;
+    OVMS_ServerMultiModelOptions* mmo = 0;
+    ASSERT_EQ(OVMS_ServerGeneralOptionsNew(&go), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsNew(&mmo), nullptr);
+    ASSERT_NE(go, nullptr);
+    ASSERT_NE(mmo, nullptr);
+    ASSERT_EQ(OVMS_ServerGeneralOptionsSetGrpcPort(go, std::stoi(port)), nullptr);
+    ASSERT_EQ(OVMS_ServerMultiModelOptionsSetConfigPath(mmo, "/ovms/src/test/c_api/config_standard_dummy.json"), nullptr);
 
-TEST_F(CapiConfigTest, Parse) {
-    GeneralOptionsImpl go;
-    MultiModelOptionsImpl mmo;
+    OVMS_Server* cserver = nullptr;
+    ASSERT_EQ(OVMS_ServerNew(&cserver), nullptr);
+    ASSERT_NE(OVMS_ServerStartFromConfigurationFile(nullptr, go, mmo), nullptr);
+    ASSERT_NE(OVMS_ServerStartFromConfigurationFile(cserver, nullptr, mmo), nullptr);
+    ASSERT_NE(OVMS_ServerStartFromConfigurationFile(cserver, go, nullptr), nullptr);
+    ASSERT_EQ(OVMS_ServerStartFromConfigurationFile(cserver, go, mmo), nullptr);
+    // TODO ensure cleanup
+    // TODO add check for server ready strict in 2022.3 not available in C-API
 
-    go.grpcPort = 123;
-    go.restPort = 234;
-    mmo.configPath = "/path/config.json";
+    OVMS_InferenceRequest* request{nullptr};
+    OVMS_InferenceResponse* response = nullptr;
+    OVMS_Status* status = OVMS_InferenceRequestNew(&request, "dummy", 1);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_NE(nullptr, request);
+    // negative no inputs
+    status = OVMS_Inference(cserver, request, &response);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+
+    // negative no input buffer
+    status = OVMS_InferenceRequestAddInput(request, DUMMY_MODEL_INPUT_NAME, OVMS_DATATYPE_FP32, DUMMY_MODEL_SHAPE.data(), DUMMY_MODEL_SHAPE.size());
+    ASSERT_EQ(nullptr, status);
+    status = OVMS_Inference(cserver, request, &response);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+
+    // setting buffer
+    std::array<float, DUMMY_MODEL_INPUT_SIZE> data{0, 1, 2, 3, 4, 5, 6, 7, 8, 9};
+    uint32_t notUsedNum = 0;
+    status = OVMS_InferenceRequestInputSetData(request, DUMMY_MODEL_INPUT_NAME, reinterpret_cast<void*>(data.data()), sizeof(float) * data.size(), OVMS_BUFFERTYPE_CPU, notUsedNum);
+    ASSERT_EQ(nullptr, status);
+    // add parameters
+    const uint64_t sequenceId{42};
+    status = OVMS_InferenceRequestAddParameter(request, "sequence_id", OVMS_DATATYPE_U64, reinterpret_cast<const void*>(&sequenceId), sizeof(sequenceId));
+    ASSERT_EQ(nullptr, status);
+    // 2nd time should get error
+    status = OVMS_InferenceRequestAddParameter(request, "sequence_id", OVMS_DATATYPE_U64, reinterpret_cast<const void*>(&sequenceId), sizeof(sequenceId));
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    const uint32_t sequenceControl{1};  // SEQUENCE_START
+    status = OVMS_InferenceRequestAddParameter(request, "sequence_control_input", OVMS_DATATYPE_U32, reinterpret_cast<const void*>(&sequenceControl), sizeof(sequenceControl));
+    ASSERT_EQ(nullptr, status);
+
+    // verify passing nullptrs
+    status = OVMS_Inference(nullptr, request, &response);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); FIXME
+    status = OVMS_Inference(cserver, nullptr, &response);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); FIXME
+    status = OVMS_Inference(cserver, request, nullptr);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); FIXME
+
+    ASSERT_NE(OVMS_ServerDelete(nullptr), nullptr);
+    ASSERT_EQ(OVMS_ServerDelete(cserver), nullptr);
+    ASSERT_NE(OVMS_ServerDelete(nullptr), nullptr);
+}
+// TODO negative test -> validate at the infer stage
+// TODO reuse request after inference
+namespace {
+const std::string MODEL_NAME{"SomeModelName"};
+const uint64_t MODEL_VERSION{42};
+const std::string PARAMETER_NAME{"sequence_id"};  // TODO check if in ovms there is such constant
+const OVMS_DataType PARAMETER_DATATYPE{OVMS_DATATYPE_I32};
+
+const uint32_t PARAMETER_VALUE{13};
+const uint32_t PRIORITY{7};
+const uint64_t REQUEST_ID{3};
+
+const std::string INPUT_NAME{"NOT_RANDOM_NAME"};
+const ovms::shape_t INPUT_SHAPE{1, 3, 220, 230};
+const std::array<float, DUMMY_MODEL_INPUT_SIZE> INPUT_DATA{1, 2, 3, 4, 5, 6, 7, 8, 9, 0};
+constexpr size_t INPUT_DATA_BYTESIZE{INPUT_DATA.size() * sizeof(float)};
+const OVMS_DataType DATATYPE{OVMS_DATATYPE_FP32};
+}  // namespace
+
+TEST_F(CapiInference, ResponseRetrieval) {
+    auto cppResponse = std::make_unique<InferenceResponse>(MODEL_NAME, MODEL_VERSION);
+    // add output
+    std::array<size_t, 2> cppOutputShape{1, DUMMY_MODEL_INPUT_SIZE};
+    auto cppStatus = cppResponse->addOutput(INPUT_NAME.c_str(), DATATYPE, cppOutputShape.data(), cppOutputShape.size());
+    ASSERT_EQ(cppStatus, StatusCode::OK) << cppStatus.string();
+    InferenceTensor* cpptensor = nullptr;
+    const std::string* cppOutputName;
+    cppStatus = cppResponse->getOutput(0, &cppOutputName, &cpptensor);
+    ASSERT_EQ(cppStatus, StatusCode::OK) << cppStatus.string();
+
+    // save data into output (it should have it's own copy in contrast to request)
+    bool createCopy = true;
+    cppStatus = cpptensor->setBuffer(INPUT_DATA.data(), INPUT_DATA_BYTESIZE, OVMS_BUFFERTYPE_CPU, std::nullopt, createCopy);
+    ASSERT_EQ(cppStatus, StatusCode::OK) << cppStatus.string();
+    // add parameter to response
+    uint64_t seqId = 666;
+    cppStatus = cppResponse->addParameter("sequence_id", OVMS_DATATYPE_U64, reinterpret_cast<void*>(&seqId));
+    ASSERT_EQ(cppStatus, StatusCode::OK) << cppStatus.string();
+    ///////////////////////////
+    // now response is prepared so we can test C-API
+    ///////////////////////////
+    OVMS_InferenceResponse* response = reinterpret_cast<OVMS_InferenceResponse*>(cppResponse.get());
+    uint32_t outputCount = 42;
+    auto status = OVMS_InferenceResponseGetOutputCount(response, &outputCount);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(outputCount, 1);
+
+    uint32_t parameterCount = 42;
+    status = OVMS_InferenceResponseGetParameterCount(response, &parameterCount);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(1, parameterCount);
+    // verify get Parameter
+    OVMS_DataType parameterDatatype = OVMS_DATATYPE_FP32;
+    const void* parameterData{nullptr};
+    status = OVMS_InferenceResponseGetParameter(response, 0, &parameterDatatype, &parameterData);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(parameterDatatype, OVMS_DATATYPE_U64);
+    EXPECT_EQ(0, std::memcmp(parameterData, (void*)&seqId, sizeof(seqId)));
+    // verify get Output
+    const void* voutputData;
+    size_t bytesize = 42;
+    uint32_t outputId = 0;
+    OVMS_DataType datatype = (OVMS_DataType)199;
+    const uint64_t* shape{nullptr};
+    uint32_t dimCount = 42;
+    BufferType bufferType = (BufferType)199;
+    uint32_t deviceId = 42;
+    const char* outputName{nullptr};
+    status = OVMS_InferenceResponseGetOutput(response, outputId + 42123, &outputName, &datatype, &shape, &dimCount, &voutputData, &bytesize, &bufferType, &deviceId);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    status = OVMS_InferenceResponseGetOutput(response, outputId, &outputName, &datatype, &shape, &dimCount, &voutputData, &bytesize, &bufferType, &deviceId);
+    ASSERT_EQ(nullptr, status);
+    ASSERT_EQ(INPUT_NAME, outputName);
+    EXPECT_EQ(datatype, OVMS_DATATYPE_FP32);
+    EXPECT_EQ(dimCount, 2);
+    EXPECT_EQ(bufferType, OVMS_BUFFERTYPE_CPU);
+    EXPECT_EQ(deviceId, 0);
+
+    for (size_t i = 0; i < cppOutputShape.size(); ++i) {
+        EXPECT_EQ(cppOutputShape[i], shape[i]) << "Different at:" << i << " place.";
+    }
+    const float* outputData = reinterpret_cast<const float*>(voutputData);
+    ASSERT_EQ(bytesize, sizeof(float) * DUMMY_MODEL_INPUT_SIZE);
+    for (size_t i = 0; i < INPUT_DATA.size(); ++i) {
+        EXPECT_EQ(INPUT_DATA[i], outputData[i]) << "Different at:" << i << " place.";
+    }
 
-    ovms::Config::instance().parse(&go, &mmo);
-    EXPECT_EQ(ovms::Config::instance().port(), 123);
-    EXPECT_EQ(ovms::Config::instance().restPort(), 234);
-    EXPECT_EQ(ovms::Config::instance().configPath(), "/path/config.json");
+    // test negative scenario with getting output without buffer
+    cppStatus = cppResponse->addOutput("outputWithNoBuffer", DATATYPE, cppOutputShape.data(), cppOutputShape.size());
+    ASSERT_EQ(cppStatus, StatusCode::OK) << cppStatus.string();
+    status = OVMS_InferenceResponseGetOutput(response, outputId + 1, &outputName, &datatype, &shape, &dimCount, &voutputData, &bytesize, &bufferType, &deviceId);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    // negative scenario nonexistsing parameter
+    status = OVMS_InferenceResponseGetParameter(response, 123, &parameterDatatype, &parameterData);
+    ASSERT_NE(nullptr, status);
+    // OVMS_StatusDelete(status); // FIXME(dkalinow)
+    // final cleanup
+    // we release unique_ptr ownership here so that we can free it safely via C-API
+    cppResponse.release();
+    status = OVMS_InferenceResponseDelete(response);
+    ASSERT_EQ(nullptr, status);
 }
+// TODO make cleaner error codes reporting
+// todo decide either use remove or delete for consistency
diff --git a/src/test/capi_predict_validation_test.cpp b/src/test/capi_predict_validation_test.cpp
new file mode 100644
index 00000000..6c4209c2
--- /dev/null
+++ b/src/test/capi_predict_validation_test.cpp
@@ -0,0 +1,843 @@
+//*****************************************************************************
+// Copyright 2022 Intel Corporation
+//
+// Licensed under the Apache License, Version 2.0 (the "License");
+// you may not use this file except in compliance with the License.
+// You may obtain a copy of the License at
+//
+//     http://www.apache.org/licenses/LICENSE-2.0
+//
+// Unless required by applicable law or agreed to in writing, software
+// distributed under the License is distributed on an "AS IS" BASIS,
+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+// See the License for the specific language governing permissions and
+// limitations under the License.
+//*****************************************************************************
+
+#include <string>
+
+#include <gmock/gmock.h>
+#include <gtest/gtest.h>
+
+#include "../buffer.hpp"
+#include "../inferencerequest.hpp"
+#include "../modelconfig.hpp"
+#include "../predict_request_validation_utils.hpp"
+#include "test_utils.hpp"
+
+using ::testing::NiceMock;
+using ::testing::Return;
+using ::testing::ReturnRef;
+
+using ovms::InferenceRequest;
+
+#pragma GCC diagnostic push
+#pragma GCC diagnostic ignored "-Wunused-variable"
+
+class CAPIPredictValidation : public ::testing::Test {
+protected:
+    std::unique_ptr<ov::Core> ieCore;
+    std::unique_ptr<NiceMock<MockedMetadataModelIns>> instance;
+    ovms::InferenceRequest request{"model_name", 1};
+    ovms::ModelConfig modelConfig{"model_name", "model_path"};
+    ovms::tensor_map_t servableInputs;
+    bool createCopy{false};
+    uint32_t decrementBufferSize{0};
+    std::vector<float> requestData{10000000};
+
+    void SetUp() override {
+        ieCore = std::make_unique<ov::Core>();
+        instance = std::make_unique<NiceMock<MockedMetadataModelIns>>(*ieCore);
+        std::iota(requestData.begin(), requestData.end(), 1.0);
+
+        servableInputs = ovms::tensor_map_t({
+            {"Input_FP32_1_224_224_3_NHWC",
+                std::make_shared<ovms::TensorInfo>("Input_FP32_1_3_224_224_NHWC", ovms::Precision::FP32, ovms::shape_t{1, 224, 224, 3}, ovms::Layout{"NHWC"})},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::make_shared<ovms::TensorInfo>("Input_U8_1_3_62_62_NCHW", ovms::Precision::U8, ovms::shape_t{1, 3, 62, 62}, ovms::Layout{"NCHW"})},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::make_shared<ovms::TensorInfo>("Input_I64_1_6_128_128_16_NCDHW", ovms::Precision::I64, ovms::shape_t{1, 6, 128, 128, 16}, ovms::Layout{"NCDHW"})},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::make_shared<ovms::TensorInfo>("Input_U16_1_2_8_4_NCHW", ovms::Precision::U16, ovms::shape_t{1, 2, 8, 4}, ovms::Layout{"NCHW"})},
+        });
+
+        ON_CALL(*instance, getInputsInfo()).WillByDefault(ReturnRef(servableInputs));
+        ON_CALL(*instance, getBatchSize()).WillByDefault(Return(1));
+        ON_CALL(*instance, getModelConfig()).WillByDefault(ReturnRef(modelConfig));
+
+        preparePredictRequest(request,
+            {{"Input_FP32_1_224_224_3_NHWC",
+                 std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+                {"Input_U8_1_3_62_62_NCHW",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+                {"Input_I64_1_6_128_128_16_NCDHW",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+                {"Input_U16_1_2_8_4_NCHW",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+            requestData);
+    }
+};
+
+TEST_F(CAPIPredictValidation, ValidRequest) {
+    auto status = instance->mockValidate(&request);
+    EXPECT_TRUE(status.ok()) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, InvalidPrecision) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, static_cast<ovms::Precision>(99)}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_PRECISION) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestNotEnoughInputs) {
+    request.removeInput("Input_U16_1_2_8_4_NCHW");
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_NO_OF_INPUTS) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestTooManyInputs) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+            std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_NO_OF_INPUTS) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestMissingInputName) {
+    preparePredictRequest(request,
+        {{"BadInput_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_MISSING_INPUT);
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongInputName) {
+    request.removeInput("Input_U16_1_2_8_4_NCHW");
+    preparePredictRequest(request,
+        {{"BADInput_FP32_1_224_224_3_NHWC",
+            std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_NO_OF_INPUTS) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestTooManyShapeDimensions) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestNotEnoughShapeDimensions) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62, 5}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16, 6}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4, 5}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_NO_OF_SHAPE_DIMENSIONS) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongBatchSize) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{2, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);  // dim(0) is batch size
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BATCH_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongBatchSizeAuto) {
+    modelConfig.setBatchingParams("auto");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{2, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::BATCHSIZE_CHANGE_REQUIRED) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongAndCorrectBatchSizeAuto) {
+    modelConfig.setBatchingParams("auto");
+
+    // First is incorrect, second is correct
+    preparePredictRequest(request, {{"im_data", {{3, 3, 800, 1344}, ovms::Precision::FP32}}, {"im_info", {{1, 3}, ovms::Precision::FP32}}}, requestData);
+
+    servableInputs.clear();
+    servableInputs = ovms::tensor_map_t{
+        {"im_data", std::make_shared<ovms::TensorInfo>("im_data", ovms::Precision::FP32, ovms::shape_t{1, 3, 800, 1344}, ovms::Layout{"NCHW"})},
+        {"im_info", std::make_shared<ovms::TensorInfo>("im_info", ovms::Precision::FP32, ovms::shape_t{1, 3}, ovms::Layout{"NC"})},
+    };
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::BATCHSIZE_CHANGE_REQUIRED);
+
+    preparePredictRequest(request, {{"im_data", {{1, 3, 800, 1344}, ovms::Precision::FP32}}, {"im_info", {{3, 3}, ovms::Precision::FP32}}}, requestData);
+
+    status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::BATCHSIZE_CHANGE_REQUIRED) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongAndCorrectShapeAuto) {
+    modelConfig.parseShapeParameter("auto");
+    preparePredictRequest(request, {{"im_data", {{1, 3, 900, 1344}, ovms::Precision::FP32}}, {"im_info", {{1, 3}, ovms::Precision::FP32}}}, requestData);
+
+    // First is incorrect, second is correct
+    servableInputs.clear();
+    servableInputs = ovms::tensor_map_t{
+        {"im_data", std::make_shared<ovms::TensorInfo>("im_data", ovms::Precision::FP32, ovms::shape_t{1, 3, 800, 1344}, ovms::Layout{"NCHW"})},
+        {"im_info", std::make_shared<ovms::TensorInfo>("im_info", ovms::Precision::FP32, ovms::shape_t{1, 3}, ovms::Layout{"NC"})},
+    };
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED) << status.string();
+
+    // First is correct, second is incorrect
+    preparePredictRequest(request, {{"im_data", {{1, 3, 800, 1344}, ovms::Precision::FP32}}, {"im_info", {{1, 6}, ovms::Precision::FP32}}}, requestData);
+
+    status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED) << status.string();
+}
+TEST_F(CAPIPredictValidation, RequestValidBatchSizeAuto) {
+    modelConfig.setBatchingParams("auto");
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::OK) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValues) {
+    modelConfig.setBatchingParams("auto");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 17}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_SHAPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesTwoInputsOneWrong) {  // one input fails validation, request denied
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"auto\"}");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 17}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_SHAPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesAuto) {
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"auto\"}");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 61, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesAutoTwoInputs) {
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"auto\", \"Input_U16_1_2_8_4_NCHW\": \"auto\"}");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 61, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 2, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED);
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesAutoNoNamedInput) {
+    modelConfig.parseShapeParameter("auto");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 214, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 61, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 1, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 2, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED);
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesAutoFirstDim) {
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"auto\"}");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED);
+}
+
+TEST_F(CAPIPredictValidation, RequestValidShapeValuesTwoInputsFixed) {
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"(1,3,62,62)\", \"Input_U16_1_2_8_4_NCHW\": \"(1,2,8,4)\"}");
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::OK) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesFixed) {
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"(1,3,62,62)\"}");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 4, 63, 63}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_SHAPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongShapeValuesFixedFirstDim) {
+    modelConfig.parseShapeParameter("{\"Input_U8_1_3_62_62_NCHW\": \"(1,3,62,62)\"}");
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{2, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BATCH_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorrectContentSize) {
+    decrementBufferSize = 1;
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_CONTENT_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorrectInputWithNoBuffer) {
+    servableInputs = ovms::tensor_map_t({{"Input_FP32_1_1_1_1_NHWC",
+        std::make_shared<ovms::TensorInfo>("Input_FP32_1_3_224_224_NHWC", ovms::Precision::FP32, ovms::shape_t{1, 1, 1, 1}, ovms::Layout{"NHWC"})}});
+    ON_CALL(*instance, getInputsInfo()).WillByDefault(ReturnRef(servableInputs));
+
+    InferenceRequest request("NOT_USED", 42);
+    std::array<size_t, 4> shape{1, 1, 1, 1};
+    request.addInput("Input_FP32_1_1_1_1_NHWC", OVMS_DATATYPE_FP32, shape.data(), shape.size());
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_CONTENT_SIZE) << status.string();  // TODO change retcode?
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorrectContentSizeZero) {
+    decrementBufferSize = 602112;
+
+    servableInputs = ovms::tensor_map_t({{"Input_FP32_1_224_224_3_NHWC",
+        std::make_shared<ovms::TensorInfo>("Input_FP32_1_3_224_224_NHWC", ovms::Precision::FP32, ovms::shape_t{1, 224, 224, 3}, ovms::Layout{"NHWC"})}});
+    ON_CALL(*instance, getInputsInfo()).WillByDefault(ReturnRef(servableInputs));
+
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+            std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}}},
+        requestData, decrementBufferSize);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_CONTENT_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorrectBufferType) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize, static_cast<BufferType>(999));
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BUFFER_TYPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestNegativeBufferType) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize, static_cast<BufferType>(-22));
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BUFFER_TYPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorectDeviceId) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize, BufferType::OVMS_BUFFERTYPE_CPU, 1);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_DEVICE_ID) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorectBufferType) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize, BufferType::OVMS_BUFFERTYPE_GPU);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BUFFER_TYPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestCorectDeviceId) {
+    GTEST_SKIP() << "Enable when Other buffer types are supported";
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize, BufferType::OVMS_BUFFERTYPE_GPU, 1);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::OK) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestNotNullDeviceId) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize, BufferType::OVMS_BUFFERTYPE_CPU, 1);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_DEVICE_ID) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorrectContentSizeBatchAuto) {
+    modelConfig.setBatchingParams("auto");
+    decrementBufferSize = 1;
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_CONTENT_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestIncorrectContentSizeShapeAuto) {
+    modelConfig.parseShapeParameter("auto");
+    decrementBufferSize = 1;
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::U8}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData, decrementBufferSize);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_CONTENT_SIZE) << status.string();
+}
+
+class CAPIPredictValidationInputTensorContent : public ::testing::TestWithParam<ovms::Precision> {
+protected:
+    std::unique_ptr<ov::Core> ieCore;
+    std::unique_ptr<NiceMock<MockedMetadataModelIns>> instance;
+    ovms::InferenceRequest request{"model_name", 1};
+
+    ovms::ModelConfig modelConfig{"model_name", "model_path"};
+    ovms::tensor_map_t servableInputs;
+    std::vector<float> requestData{10000000};
+
+    void SetUp() override {
+        ieCore = std::make_unique<ov::Core>();
+        instance = std::make_unique<NiceMock<MockedMetadataModelIns>>(*ieCore);
+        std::iota(requestData.begin(), requestData.end(), 1.0);
+    }
+};
+
+TEST_P(CAPIPredictValidationInputTensorContent, RequestCorrectContentSizeInputTensorContent) {
+    ovms::Precision testedPrecision = GetParam();
+    const std::string inputName = "someName";
+    servableInputs = ovms::tensor_map_t({
+        {inputName,
+            std::make_shared<ovms::TensorInfo>(inputName, testedPrecision, ovms::shape_t{1, 224, 224, 3}, ovms::Layout{"NHWC"})},
+    });
+    ON_CALL(*instance, getInputsInfo()).WillByDefault(ReturnRef(servableInputs));
+    ON_CALL(*instance, getBatchSize()).WillByDefault(Return(1));
+    ON_CALL(*instance, getModelConfig()).WillByDefault(ReturnRef(modelConfig));
+    preparePredictRequest(request,
+        {{inputName,
+            std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, testedPrecision}}},
+        requestData,  // data,
+        false);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::OK) << status.string();
+}
+
+TEST_F(CAPIPredictValidation, RequestWrongPrecision) {
+    preparePredictRequest(request,
+        {{"Input_FP32_1_224_224_3_NHWC",
+             std::tuple<ovms::shape_t, ovms::Precision>{{1, 224, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_1_3_62_62_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 3, 62, 62}, ovms::Precision::Q78}},
+            {"Input_I64_1_6_128_128_16_NCDHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 6, 128, 128, 16}, ovms::Precision::I64}},
+            {"Input_U16_1_2_8_4_NCHW",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 2, 8, 4}, ovms::Precision::U16}}},
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_PRECISION) << status.string();
+}
+
+class CAPIPredictValidationArbitraryBatchPosition : public CAPIPredictValidation {
+protected:
+    void SetUp() override {
+        CAPIPredictValidation::SetUp();
+
+        servableInputs = ovms::tensor_map_t({
+            {"Input_FP32_224_224_3_1_HWCN",
+                std::make_shared<ovms::TensorInfo>("Input_FP32_224_224_3_1_HWCN", ovms::Precision::FP32, ovms::shape_t{224, 224, 3, 1}, ovms::Layout{"HWCN"})},
+            {"Input_U8_3_1_128_CNH",
+                std::make_shared<ovms::TensorInfo>("Input_U8_3_1_128_CNH", ovms::Precision::U8, ovms::shape_t{3, 1, 128}, ovms::Layout{"CNH"})},
+        });
+
+        preparePredictRequest(request,
+            {
+                {"Input_FP32_224_224_3_1_HWCN",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{224, 224, 3, 1}, ovms::Precision::FP32}},
+                {"Input_U8_3_1_128_CNH",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{3, 1, 128}, ovms::Precision::U8}},
+            },
+            requestData);
+    }
+};
+
+TEST_F(CAPIPredictValidationArbitraryBatchPosition, Valid) {
+    auto status = instance->mockValidate(&request);
+    EXPECT_TRUE(status.ok());
+}
+
+TEST_F(CAPIPredictValidationArbitraryBatchPosition, RequestWrongBatchSize) {
+    // Edit fourth dimension (N), expect validator to report wrong batch size instead of wrong shape.
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_224_224_3_1_HWCN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{224, 224, 3, 10}, ovms::Precision::FP32}},
+            {"Input_U8_3_1_128_CNH",
+                std::tuple<ovms::shape_t, ovms::Precision>{{3, 1, 128}, ovms::Precision::U8}},
+        },
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BATCH_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidationArbitraryBatchPosition, RequestWrongBatchSizeAuto) {
+    modelConfig.setBatchingParams("auto");
+    // Edit fourth dimension (N), expect validator to report batch size change request instead of reshape request.
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_224_224_3_1_HWCN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{224, 224, 3, 10}, ovms::Precision::FP32}},
+            {"Input_U8_3_1_128_CNH",
+                std::tuple<ovms::shape_t, ovms::Precision>{{3, 1, 128}, ovms::Precision::U8}},
+        },
+        requestData);
+
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::BATCHSIZE_CHANGE_REQUIRED) << status.string();
+}
+
+TEST_F(CAPIPredictValidationArbitraryBatchPosition, RequestWrongShapeValues) {
+    // Edit first dimension (H), expect validator to report wrong shape instead of wrong batch size.
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_224_224_3_1_HWCN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{221, 224, 3, 1}, ovms::Precision::FP32}},
+            {"Input_U8_3_1_128_CNH",
+                std::tuple<ovms::shape_t, ovms::Precision>{{3, 1, 128}, ovms::Precision::U8}},
+        },
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_SHAPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidationArbitraryBatchPosition, RequestWrongShapeValuesAuto) {
+    modelConfig.parseShapeParameter("auto");
+    // Edit first dimension (H), expect validator to report reshape request instead of requesting batch size change.
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_224_224_3_1_HWCN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{10, 224, 3, 1}, ovms::Precision::FP32}},
+            {"Input_U8_3_1_128_CNH",
+                std::tuple<ovms::shape_t, ovms::Precision>{{3, 1, 128}, ovms::Precision::U8}},
+        },
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::RESHAPE_REQUIRED) << status.string();
+}
+
+class CAPIPredictValidationDynamicModel : public CAPIPredictValidation {
+protected:
+    void SetUp() override {
+        CAPIPredictValidation::SetUp();
+
+        servableInputs = ovms::tensor_map_t({{"Input_FP32_any_224:512_224:512_3_NHWC",
+                                                 std::make_shared<ovms::TensorInfo>("Input_FP32_any_224:512_224:512_3_NHWC", ovms::Precision::FP32, ovms::Shape{ovms::Dimension::any(), {224, 512}, {224, 512}, 3}, ovms::Layout{"NHWC"})},
+            {"Input_U8_100:200_any_CN",
+                std::make_shared<ovms::TensorInfo>("Input_U8_100:200_any_CN", ovms::Precision::U8, ovms::Shape{{100, 200}, ovms::Dimension::any()}, ovms::Layout{"CN"})}});
+
+        ON_CALL(*instance, getBatchSize()).WillByDefault(Return(ovms::Dimension::any()));
+
+        const ovms::dimension_value_t requestBatchSize = 16;
+        preparePredictRequest(request,
+            {
+                {"Input_FP32_any_224:512_224:512_3_NHWC",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{requestBatchSize, 300, 320, 3}, ovms::Precision::FP32}},
+                {"Input_U8_100:200_any_CN",
+                    std::tuple<ovms::shape_t, ovms::Precision>{{101, requestBatchSize}, ovms::Precision::U8}},
+            },
+            requestData);
+    }
+};
+
+TEST_F(CAPIPredictValidationDynamicModel, ValidRequest) {
+    auto status = instance->mockValidate(&request);
+    EXPECT_TRUE(status.ok());
+}
+
+TEST_F(CAPIPredictValidationDynamicModel, RequestBatchNotInRangeFirstPosition) {
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_any_224:512_224:512_3_NHWC",
+                std::tuple<ovms::shape_t, ovms::Precision>{{16, 300, 320, 3}, ovms::Precision::FP32}},
+            {"Input_U8_100:200_any_CN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{101, 16}, ovms::Precision::U8}},
+        },
+        requestData);
+
+    servableInputs["Input_FP32_any_224:512_224:512_3_NHWC"] = std::make_shared<ovms::TensorInfo>("Input_FP32_any_224:512_224:512_3_NHWC", ovms::Precision::FP32, ovms::Shape{{1, 5}, {224, 512}, {224, 512}, 3}, ovms::Layout{"NHWC"});
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BATCH_SIZE);
+}
+
+TEST_F(CAPIPredictValidationDynamicModel, RequestDimensionNotInRangeFirstPosition) {
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_any_224:512_224:512_3_NHWC",
+                std::tuple<ovms::shape_t, ovms::Precision>{{16, 300, 320, 3}, ovms::Precision::FP32}},
+            {"Input_U8_100:200_any_CN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{98, 1}, ovms::Precision::U8}},
+        },
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_SHAPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidationDynamicModel, RequestBatchNotInRangeSecondPosition) {
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_any_224:512_224:512_3_NHWC",
+                std::tuple<ovms::shape_t, ovms::Precision>{{16, 300, 320, 3}, ovms::Precision::FP32}},
+            {"Input_U8_100:200_any_CN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{100, 98}, ovms::Precision::U8}},
+        },
+        requestData);
+    servableInputs["Input_U8_100:200_any_CN"] = std::make_shared<ovms::TensorInfo>("Input_U8_100:200_any_CN", ovms::Precision::U8, ovms::Shape{{100, 200}, {1, 5}}, ovms::Layout{"CN"});
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_BATCH_SIZE) << status.string();
+}
+
+TEST_F(CAPIPredictValidationDynamicModel, RequestDimensionNotInRangeSecondPosition) {
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_any_224:512_224:512_3_NHWC",
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, 223, 224, 3}, ovms::Precision::FP32}},
+            {"Input_U8_100:200_any_CN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{101, 16}, ovms::Precision::U8}},
+        },
+        requestData);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_SHAPE) << status.string();
+}
+
+TEST_F(CAPIPredictValidationDynamicModel, RequestDimensionInRangeWrongTensorContent) {
+    decrementBufferSize = 1;
+    preparePredictRequest(request,
+        {
+            {"Input_FP32_any_224:512_224:512_3_NHWC",
+                std::tuple<ovms::shape_t, ovms::Precision>{{16, 300, 320, 3}, ovms::Precision::FP32}},
+            {"Input_U8_100:200_any_CN",
+                std::tuple<ovms::shape_t, ovms::Precision>{{101, 16}, ovms::Precision::U8}},
+        },
+        requestData, decrementBufferSize);
+    auto status = instance->mockValidate(&request);
+    EXPECT_EQ(status, ovms::StatusCode::INVALID_CONTENT_SIZE) << status.string();
+}
+
+// TODO: Add request parameters name validation tests
+
+class CAPIPredictValidationPrecision : public ::testing::TestWithParam<ovms::Precision> {
+protected:
+    std::vector<float> requestData{10000000};
+
+    void SetUp() override {
+        std::iota(requestData.begin(), requestData.end(), 1.0);
+        auto precision = ovms::Precision::FP32;
+        mockedInputsInfo[tensorName] = std::make_shared<ovms::TensorInfo>(tensorName, precision, ovms::shape_t{1, DUMMY_MODEL_INPUT_SIZE}, ovms::Layout{"NC"});
+    }
+    ovms::InferenceRequest request{"model_name", 1};
+
+    const char* tensorName = DUMMY_MODEL_INPUT_NAME;
+    ovms::tensor_map_t mockedInputsInfo;
+};
+
+TEST_P(CAPIPredictValidationPrecision, ValidPrecisions) {
+    ovms::Precision testedPrecision = GetParam();
+    mockedInputsInfo[tensorName]->setPrecision(testedPrecision);
+    preparePredictRequest(request,
+        {
+            {tensorName,
+                std::tuple<ovms::shape_t, ovms::Precision>{{1, DUMMY_MODEL_INPUT_SIZE}, testedPrecision}},
+        },
+        requestData);
+    auto status = ovms::request_validation_utils::validate(request, mockedInputsInfo, "dummy", ovms::model_version_t{1});
+    EXPECT_EQ(status, ovms::StatusCode::OK) << "Precision validation failed:"
+                                            << toString(testedPrecision)
+                                            << " should pass validation";
+}
+
+INSTANTIATE_TEST_SUITE_P(
+    Test,
+    CAPIPredictValidationPrecision,
+    ::testing::ValuesIn(SUPPORTED_CAPI_INPUT_PRECISIONS),
+    [](const ::testing::TestParamInfo<CAPIPredictValidationPrecision::ParamType>& info) {
+        return toString(info.param);
+    });
+
+INSTANTIATE_TEST_SUITE_P(
+    Test,
+    CAPIPredictValidationInputTensorContent,
+    ::testing::ValuesIn(SUPPORTED_CAPI_INPUT_PRECISIONS_TENSORINPUTCONTENT),
+    [](const ::testing::TestParamInfo<CAPIPredictValidationPrecision::ParamType>& info) {
+        return toString(info.param);
+    });
+
+#pragma GCC diagnostic pop
diff --git a/src/test/deserialization_tests.cpp b/src/test/deserialization_tests.cpp
index b394535f..9cea22f6 100644
--- a/src/test/deserialization_tests.cpp
+++ b/src/test/deserialization_tests.cpp
@@ -100,7 +100,8 @@ protected:
             Layout{"NC"});
         SetUpTensorProto(getPrecisionAsOVMSDataType(PRECISION));
     }
-    void SetUpTensorProto(DataType dataType) {
+
+    void SetUpTensorProto(OVMS_DataType dataType) {
         std::array<size_t, 2> shape{1, DUMMY_MODEL_INPUT_SIZE};
         tensorCapi = std::make_unique<InferenceTensor>(dataType,
             shape.data(),
diff --git a/src/test/inferencerequest_test.cpp b/src/test/inferencerequest_test.cpp
index d7ccaa4f..7ce2c7e9 100644
--- a/src/test/inferencerequest_test.cpp
+++ b/src/test/inferencerequest_test.cpp
@@ -42,7 +42,7 @@ namespace {
 const std::string MODEL_NAME{"SomeModelName"};
 const uint64_t MODEL_VERSION{42};
 const std::string PARAMETER_NAME{"SEQUENCE_ID"};  // TODO check if in ovms there is such constant
-const DataType PARAMETER_DATATYPE{OVMS_DATATYPE_I32};
+const OVMS_DataType PARAMETER_DATATYPE{OVMS_DATATYPE_I32};
 
 const uint32_t PARAMETER_VALUE{13};
 const uint32_t PRIORITY{7};
@@ -52,7 +52,7 @@ const std::string INPUT_NAME{"NOT_RANDOM_NAME"};
 const ovms::shape_t INPUT_SHAPE{1, 3, 220, 230};
 const std::array<float, 10> INPUT_DATA{1, 2, 3, 4, 5, 6, 7, 8, 9, 0};
 constexpr size_t INPUT_DATA_BYTESIZE{INPUT_DATA.size() * sizeof(float)};
-const DataType DATATYPE{OVMS_DATATYPE_FP32};
+const OVMS_DataType DATATYPE{OVMS_DATATYPE_FP32};
 }  // namespace
 
 TEST(InferenceParameter, CreateParameter) {
@@ -138,16 +138,22 @@ TEST(InferenceResponse, CreateAndReadData) {
     // add output
     auto status = response.addOutput(INPUT_NAME.c_str(), DATATYPE, INPUT_SHAPE.data(), INPUT_SHAPE.size());
     ASSERT_EQ(status, StatusCode::OK) << status.string();
+    EXPECT_EQ(response.getOutputCount(), 1);
     // add 2nd output with the same name should fail
     status = response.addOutput(INPUT_NAME.c_str(), DATATYPE, INPUT_SHAPE.data(), INPUT_SHAPE.size());
     ASSERT_EQ(status, StatusCode::DOUBLE_TENSOR_INSERT) << status.string();
     // get nonexistent output
+    const std::string* wrongOutputName = nullptr;
     InferenceTensor* tensor = nullptr;
-    status = response.getOutput("SOME_NOT_RANDOM_NAME", &tensor);
+    status = response.getOutput(13, &wrongOutputName, &tensor);
     ASSERT_EQ(status, StatusCode::NONEXISTENT_TENSOR) << status.string();
+    ASSERT_EQ(nullptr, tensor);
+    ASSERT_EQ(nullptr, wrongOutputName);
     // get output
-    status = response.getOutput(INPUT_NAME.c_str(), &tensor);
+    const std::string* outputName = nullptr;
+    status = response.getOutput(0, &outputName, &tensor);
     ASSERT_NE(nullptr, tensor);
+    ASSERT_EQ(INPUT_NAME, *outputName);
     ASSERT_EQ(status, StatusCode::OK) << status.string();
     // compare datatype
     ASSERT_EQ(tensor->getDataType(), DATATYPE);
@@ -177,9 +183,11 @@ TEST(InferenceResponse, CreateAndReadData) {
     // verify parameter handling
     status = response.addParameter(PARAMETER_NAME.c_str(), PARAMETER_DATATYPE, reinterpret_cast<const void*>(&PARAMETER_VALUE));
     ASSERT_EQ(status, StatusCode::OK) << status.string();
+    EXPECT_EQ(response.getParameterCount(), 1);
 
-    // add parameter
-    const InferenceParameter* parameter = response.getParameter(PARAMETER_NAME.c_str());
+    const InferenceParameter* parameter = response.getParameter(1);
+    ASSERT_EQ(parameter, nullptr);
+    parameter = response.getParameter(0);
     ASSERT_NE(parameter, nullptr);
     EXPECT_EQ(parameter->getName(), PARAMETER_NAME);
     EXPECT_EQ(parameter->getDataType(), PARAMETER_DATATYPE);
diff --git a/src/test/ovinferrequestqueue_test.cpp b/src/test/ovinferrequestqueue_test.cpp
index c762ceca..a6017cd4 100644
--- a/src/test/ovinferrequestqueue_test.cpp
+++ b/src/test/ovinferrequestqueue_test.cpp
@@ -77,7 +77,7 @@ TEST(OVInferRequestQueue, FullQueue) {
     EXPECT_EQ(reqid, 3);
 }
 
-void inferenceSimulate(ovms::OVInferRequestsQueue& ms, std::vector<int>& tv) {
+static void inferenceSimulate(ovms::OVInferRequestsQueue& ms, std::vector<int>& tv) {
     for (int i = 1; i <= 10; i++) {
         int st = ms.getIdleStream().get();
         int rd = std::rand();
diff --git a/src/test/ovmsconfig_test.cpp b/src/test/ovmsconfig_test.cpp
index 36679207..2e624a33 100644
--- a/src/test/ovmsconfig_test.cpp
+++ b/src/test/ovmsconfig_test.cpp
@@ -25,6 +25,7 @@
 #include "spdlog/spdlog.h"
 
 #include "../config.hpp"
+#include "test_utils.hpp"
 
 using testing::_;
 using testing::ContainerEq;
@@ -322,11 +323,6 @@ TEST_F(OvmsParamsTest, hostname_ip_regex) {
     EXPECT_EQ(ovms::Config::check_hostname_or_ip(too_long), false);
 }
 
-class MockedConfig : public ovms::Config {
-public:
-    MockedConfig() {}
-};
-
 TEST(OvmsConfigTest, positiveMulti) {
     char* n_argv[] = {"ovms",
         "--port", "44",
@@ -338,7 +334,7 @@ TEST(OvmsConfigTest, positiveMulti) {
         "--grpc_channel_arguments", "grpc_channel_args",
         "--file_system_poll_wait_seconds", "2",
         "--sequence_cleaner_poll_wait_minutes", "7",
-        "--custom_node_resources_cleaner_interval", "8",
+        "--custom_node_resources_cleaner_interval_seconds", "8",
         "--cpu_extension", "/ovms",
         "--cache_dir", "/tmp/model_cache",
         "--log_path", "/tmp/log_path",
@@ -346,7 +342,7 @@ TEST(OvmsConfigTest, positiveMulti) {
 
         "--config_path", "/config.json"};
     int arg_count = 31;
-    MockedConfig config;
+    ConstructorEnabledConfig config;
     config.parse(arg_count, n_argv);
 
     EXPECT_EQ(config.port(), 44);
@@ -388,7 +384,7 @@ TEST(OvmsConfigTest, positiveSingle) {
         "2",
         "--sequence_cleaner_poll_wait_minutes",
         "7",
-        "--custom_node_resources_cleaner_interval",
+        "--custom_node_resources_cleaner_interval_seconds",
         "8",
         "--cpu_extension",
         "/ovms",
@@ -427,7 +423,7 @@ TEST(OvmsConfigTest, positiveSingle) {
         "52",
     };
     int arg_count = 55;
-    MockedConfig config;
+    ConstructorEnabledConfig config;
     config.parse(arg_count, n_argv);
 
     EXPECT_EQ(config.port(), 44);
diff --git a/src/test/predict_validation_test.cpp b/src/test/predict_validation_test.cpp
index 7889867d..5f49391e 100644
--- a/src/test/predict_validation_test.cpp
+++ b/src/test/predict_validation_test.cpp
@@ -32,32 +32,17 @@ using ::testing::ReturnRef;
 #pragma GCC diagnostic push
 #pragma GCC diagnostic ignored "-Wunused-variable"
 
-class MockModelInstance : public ovms::ModelInstance {
-public:
-    MockModelInstance(ov::Core& ieCore) :
-        ModelInstance("UNUSED_NAME", 42, ieCore) {}
-    MOCK_METHOD(const ovms::tensor_map_t&, getInputsInfo, (), (const, override));
-    MOCK_METHOD(ovms::Dimension, getBatchSize, (), (const, override));
-    MOCK_METHOD(const ovms::ModelConfig&, getModelConfig, (), (const, override));
-    const ovms::Status mockValidate(const tensorflow::serving::PredictRequest* request) {
-        return validate(request);
-    }
-    const ovms::Status mockValidate(const ::KFSRequest* request) {
-        return validate(request);
-    }
-};
-
 class TfsPredictValidation : public ::testing::Test {
 protected:
     std::unique_ptr<ov::Core> ieCore;
-    std::unique_ptr<NiceMock<MockModelInstance>> instance;
+    std::unique_ptr<NiceMock<MockedMetadataModelIns>> instance;
     tensorflow::serving::PredictRequest request;
     ovms::ModelConfig modelConfig{"model_name", "model_path"};
     ovms::tensor_map_t servableInputs;
 
     void SetUp() override {
         ieCore = std::make_unique<ov::Core>();
-        instance = std::make_unique<NiceMock<MockModelInstance>>(*ieCore);
+        instance = std::make_unique<NiceMock<MockedMetadataModelIns>>(*ieCore);
 
         servableInputs = ovms::tensor_map_t({
             {"Input_FP32_1_224_224_3_NHWC",
@@ -678,14 +663,14 @@ INSTANTIATE_TEST_SUITE_P(
 class KFSPredictValidation : public ::testing::Test {
 protected:
     std::unique_ptr<ov::Core> ieCore;
-    std::unique_ptr<NiceMock<MockModelInstance>> instance;
+    std::unique_ptr<NiceMock<MockedMetadataModelIns>> instance;
     ::KFSRequest request;
     ovms::ModelConfig modelConfig{"model_name", "model_path"};
     ovms::tensor_map_t servableInputs;
 
     void SetUp() override {
         ieCore = std::make_unique<ov::Core>();
-        instance = std::make_unique<NiceMock<MockModelInstance>>(*ieCore);
+        instance = std::make_unique<NiceMock<MockedMetadataModelIns>>(*ieCore);
 
         servableInputs = ovms::tensor_map_t({
             {"Input_FP32_1_224_224_3_NHWC",
@@ -1002,14 +987,14 @@ TEST_F(KFSPredictValidation, RequestIncorrectContentSizeShapeAuto) {
 class KFSPredictValidationInputTensorContent : public ::testing::TestWithParam<ovms::Precision> {
 protected:
     std::unique_ptr<ov::Core> ieCore;
-    std::unique_ptr<NiceMock<MockModelInstance>> instance;
+    std::unique_ptr<NiceMock<MockedMetadataModelIns>> instance;
     ::KFSRequest request;
     ovms::ModelConfig modelConfig{"model_name", "model_path"};
     ovms::tensor_map_t servableInputs;
 
     void SetUp() override {
         ieCore = std::make_unique<ov::Core>();
-        instance = std::make_unique<NiceMock<MockModelInstance>>(*ieCore);
+        instance = std::make_unique<NiceMock<MockedMetadataModelIns>>(*ieCore);
     }
 };
 
@@ -1058,14 +1043,14 @@ TEST_P(KFSPredictValidationInputTensorContent, RequestCorrectContentSizeInputTen
 class KFSPredictValidationInputTensorContentNegative : public ::testing::Test {
 protected:
     std::unique_ptr<ov::Core> ieCore;
-    std::unique_ptr<NiceMock<MockModelInstance>> instance;
+    std::unique_ptr<NiceMock<MockedMetadataModelIns>> instance;
     ::KFSRequest request;
     ovms::ModelConfig modelConfig{"model_name", "model_path"};
     ovms::tensor_map_t servableInputs;
 
     void SetUp() override {
         ieCore = std::make_unique<ov::Core>();
-        instance = std::make_unique<NiceMock<MockModelInstance>>(*ieCore);
+        instance = std::make_unique<NiceMock<MockedMetadataModelIns>>(*ieCore);
 
         servableInputs = ovms::tensor_map_t({
             {"Input_FP32_1_224_224_3_NHWC",
diff --git a/src/test/prediction_service_test.cpp b/src/test/prediction_service_test.cpp
index 03e41d5e..f18aa989 100644
--- a/src/test/prediction_service_test.cpp
+++ b/src/test/prediction_service_test.cpp
@@ -26,11 +26,17 @@
 #include <openvino/openvino.hpp>
 #include <stdlib.h>
 
+#include "../buffer.hpp"
 #include "../deserialization.hpp"
 #include "../executingstreamidguard.hpp"
+#include "../inferenceparameter.hpp"  // TODO move bytesize util
+#include "../inferencerequest.hpp"
+#include "../inferenceresponse.hpp"
+#include "../inferencetensor.hpp"
 #include "../kfs_frontend/kfs_utils.hpp"
 #include "../modelinstance.hpp"
 #include "../modelinstanceunloadguard.hpp"
+#include "../modelversion.hpp"
 #include "../prediction_service_utils.hpp"
 #include "../sequence_processing_spec.hpp"
 #include "../serialization.hpp"
@@ -38,8 +44,12 @@
 #include "test_utils.hpp"
 
 using testing::Each;
+using testing::ElementsAre;
 using testing::Eq;
 
+using ovms::Buffer;
+using ovms::InferenceResponse;
+using ovms::InferenceTensor;
 using ovms::StatusCode;
 
 // TODO: These tests test both TFS and KFS for prediction,
@@ -51,14 +61,14 @@ void serializeAndCheck(int outputSize, ov::InferRequest& inferRequest, const std
     std::vector<float> output(10);
     tensorflow::serving::PredictResponse response;
     ovms::OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, outputsInfo, &response, ovms::getTensorInfoName);
+    auto status = serializePredictResponse(outputGetter, UNUSED_SERVABLE_NAME, UNUSED_MODEL_VERSION, outputsInfo, &response, ovms::getTensorInfoName);
     ASSERT_EQ(status, ovms::StatusCode::OK) << status.string();
     ASSERT_EQ(response.outputs().count(outputName), 1) << "Did not find:" << outputName;
     std::memcpy(output.data(), (float*)response.outputs().at(outputName).tensor_content().data(), DUMMY_MODEL_OUTPUT_SIZE * sizeof(float));
     EXPECT_THAT(output, Each(Eq(1.)));
 }
 
-ovms::Status getOutput(const KFSResponse& response, const std::string& name, KFSOutputTensorIteratorType& it, size_t& bufferId) {
+static ovms::Status getOutput(const KFSResponse& response, const std::string& name, KFSOutputTensorIteratorType& it, size_t& bufferId) {
     it = response.outputs().begin();
     bufferId = 0;
     while (it != response.outputs().end()) {
@@ -74,7 +84,7 @@ ovms::Status getOutput(const KFSResponse& response, const std::string& name, KFS
     return StatusCode::INVALID_MISSING_INPUT;
 }
 
-ovms::Status getOutput(const TFSResponseType& response, const std::string& name, TFSOutputTensorIteratorType& it, size_t& bufferId) {
+static ovms::Status getOutput(const TFSResponseType& response, const std::string& name, TFSOutputTensorIteratorType& it, size_t& bufferId) {
     it = response.outputs().find(name);
     if (it != response.outputs().end()) {
         return StatusCode::OK;
@@ -82,6 +92,33 @@ ovms::Status getOutput(const TFSResponseType& response, const std::string& name,
     return StatusCode::INVALID_MISSING_INPUT;
 }
 
+using inputs_info_elem_t = std::pair<std::string, std::tuple<ovms::shape_t, ovms::Precision>>;
+size_t calculateByteSize(const inputs_info_elem_t& e) {
+    auto& [inputName, shapeDatatypeTuple] = e;
+    auto& [shape, precision] = shapeDatatypeTuple;
+    size_t shapeProduct = std::accumulate(shape.begin(), shape.end(), 1, std::multiplies<size_t>());
+    return shapeProduct * ovms::DataTypeToByteSize(ovms::getPrecisionAsOVMSDataType(precision));
+}
+template <typename RequestType>
+class Preparer {
+    std::vector<std::unique_ptr<std::vector<float>>> dataKeeper;
+
+public:
+    void preparePredictRequest(RequestType& request, inputs_info_t requestInputs) {
+        ::preparePredictRequest(request, requestInputs);
+    }
+};
+template <>
+void Preparer<ovms::InferenceRequest>::preparePredictRequest(ovms::InferenceRequest& request, inputs_info_t requestInputs) {
+    auto inputWithGreatestRequirements = std::max_element(requestInputs.begin(), requestInputs.end(), [](const inputs_info_elem_t& a, const inputs_info_elem_t& b) {
+        return calculateByteSize(a) < calculateByteSize(b);
+    });
+    size_t byteSizeToPreserve = calculateByteSize(*inputWithGreatestRequirements);
+    auto& currentData = dataKeeper.emplace_back(std::make_unique<std::vector<float>>(byteSizeToPreserve));
+    memset(reinterpret_cast<void*>(const_cast<float*>(currentData->data())), '1', byteSizeToPreserve);
+    ::preparePredictRequest(request, requestInputs, *currentData);
+}
+
 template <typename Pair,
     typename RequestType = typename Pair::first_type,
     typename ResponseType = typename Pair::second_type>
@@ -117,7 +154,8 @@ public:
                 std::thread(
                     [this, initialBatchSize, &releaseWaitBeforeGettingModelInstance, i]() {
                         RequestType request;
-                        preparePredictRequest(request,
+                        Preparer<RequestType> preparer;
+                        preparer.preparePredictRequest(request,
                             {{DUMMY_MODEL_INPUT_NAME,
                                 std::tuple<ovms::shape_t, ovms::Precision>{{(initialBatchSize + (i % 3)), 10}, ovms::Precision::FP32}}});
 
@@ -130,7 +168,8 @@ public:
                 std::thread(
                     [this, initialBatchSize, &releaseWaitBeforePerformInference, i]() {
                         RequestType request;
-                        preparePredictRequest(request,
+                        Preparer<RequestType> preparer;
+                        preparer.preparePredictRequest(request,
                             {{DUMMY_MODEL_INPUT_NAME,
                                 std::tuple<ovms::shape_t, ovms::Precision>{{initialBatchSize, 10}, ovms::Precision::FP32}}});
 
@@ -166,7 +205,8 @@ public:
                 std::thread(
                     [this, initialBatchSize, &releaseWaitBeforeGettingModelInstance, i]() {
                         RequestType request;
-                        preparePredictRequest(request,
+                        Preparer<RequestType> preparer;
+                        preparer.preparePredictRequest(request,
                             {{DUMMY_MODEL_INPUT_NAME,
                                 std::tuple<ovms::shape_t, ovms::Precision>{{(initialBatchSize + i), 10}, ovms::Precision::FP32}}});
                         performPredict(config.getName(), config.getVersion(), request,
@@ -194,6 +234,31 @@ public:
         ASSERT_EQ(0, std::memcmp(actualValues.data(), expectedValues.data(), expectedValues.size() * sizeof(float)))
             << readableError(expectedValues.data(), actualValues.data(), expectedValues.size() * sizeof(float));
     }
+    static void checkOutputValues(const ovms::InferenceResponse& res, const std::vector<float>& expectedValues, const std::string& outputName = INCREMENT_1x3x4x5_MODEL_OUTPUT_NAME) {
+        InferenceResponse& response = const_cast<InferenceResponse&>(res);  // TODO decide if output should be const
+        size_t outputCount = response.getOutputCount();
+        ASSERT_GE(1, outputCount);
+        size_t outputId = 0;
+        while (outputId < outputCount) {
+            const std::string* cppName;
+            InferenceTensor* tensor;
+            auto status = response.getOutput(outputId, &cppName, &tensor);
+            ASSERT_EQ(status, StatusCode::OK) << status.string();
+            ASSERT_NE(nullptr, tensor);
+            ASSERT_NE(nullptr, cppName);
+            if (outputName == *cppName) {
+                const Buffer* buffer = tensor->getBuffer();
+                ASSERT_NE(nullptr, buffer);
+                ASSERT_EQ(expectedValues.size() * sizeof(float), buffer->getByteSize());
+                float* bufferRaw = reinterpret_cast<float*>(const_cast<void*>(buffer->data()));
+                ASSERT_EQ(0, std::memcmp(bufferRaw, expectedValues.data(), expectedValues.size() * sizeof(float)))
+                    << readableError(expectedValues.data(), bufferRaw, expectedValues.size() * sizeof(float));
+                return;
+            }
+            ++outputId;
+        }
+        ASSERT_TRUE(false) << "did not found output with name: " << outputName;
+    }
     static void checkOutputValues(const KFSResponse& response, const std::vector<float>& expectedValues, const std::string& outputName = INCREMENT_1x3x4x5_MODEL_OUTPUT_NAME) {
         KFSOutputTensorIteratorType it;
         size_t bufferId;
@@ -222,14 +287,14 @@ public:
         if (!status.ok()) {
             return status;
         }
-
         response.Clear();
         return model->infer(&request, &response, unload_guard);
     }
 
     ovms::Status performInferenceWithShape(ResponseType& response, const ovms::shape_t& shape = {1, 10}, const ovms::Precision precision = ovms::Precision::FP32) {
         RequestType request;
-        preparePredictRequest(request,
+        Preparer<RequestType> preparer;
+        preparer.preparePredictRequest(request,
             {{DUMMY_MODEL_INPUT_NAME, std::tuple<ovms::shape_t, ovms::Precision>{shape, precision}}});
         return performInferenceWithRequest(request, response);
     }
@@ -238,15 +303,22 @@ public:
         ovms::shape_t shape = {1, 10};
         shape[batchSizePosition] = batchSize;
         RequestType request;
-        preparePredictRequest(request,
+        Preparer<RequestType> preparer;
+        preparer.preparePredictRequest(request,
             {{DUMMY_MODEL_INPUT_NAME, std::tuple<ovms::shape_t, ovms::Precision>{shape, precision}}});
         return performInferenceWithRequest(request, response);
     }
 
     ovms::Status performInferenceWithImageInput(ResponseType& response, const std::vector<size_t>& shape, const std::vector<float>& data = {}, const std::string& servableName = "increment_1x3x4x5", int batchSize = 1, const ovms::Precision precision = ovms::Precision::FP32) {
         RequestType request;
-        preparePredictRequest(request,
-            {{INCREMENT_1x3x4x5_MODEL_INPUT_NAME, std::tuple<ovms::shape_t, ovms::Precision>{shape, precision}}}, data);
+        Preparer<RequestType> preparer;
+        if (data.size()) {
+            preparePredictRequest(request,
+                {{INCREMENT_1x3x4x5_MODEL_INPUT_NAME, std::tuple<ovms::shape_t, ovms::Precision>{shape, precision}}}, data);
+        } else {
+            preparer.preparePredictRequest(request,
+                {{INCREMENT_1x3x4x5_MODEL_INPUT_NAME, std::tuple<ovms::shape_t, ovms::Precision>{shape, precision}}});
+        }
         return performInferenceWithRequest(request, response, servableName);
     }
 
@@ -274,6 +346,31 @@ void TestPredict<TFSInterface>::checkOutputShape(const TFSResponseType& response
     }
 }
 
+template <>
+void TestPredict<CAPIInterface>::checkOutputShape(const ovms::InferenceResponse& cresponse, const ovms::shape_t& shape, const std::string& outputName) {
+    size_t outputCount = cresponse.getOutputCount();
+    EXPECT_GE(1, outputCount);
+    size_t outputId = 0;
+    while (outputId < outputCount) {
+        const std::string* cppName;
+        InferenceTensor* tensor;
+        InferenceResponse& response = const_cast<InferenceResponse&>(cresponse);  // TODO decide if output should be const
+        auto status = response.getOutput(outputId, &cppName, &tensor);
+        EXPECT_EQ(status, StatusCode::OK) << status.string();
+        EXPECT_NE(nullptr, tensor);
+        EXPECT_NE(nullptr, cppName);
+        if (outputName == *cppName) {
+            auto resultShape = tensor->getShape();
+            EXPECT_EQ(shape.size(), resultShape.size());
+            for (size_t i = 0; i < shape.size(); ++i) {
+                EXPECT_EQ(resultShape[i], shape[i]);
+            }
+        }
+        ++outputId;
+    }
+    return;
+}
+
 template <>
 void TestPredict<KFSInterface>::checkOutputShape(const KFSResponse& response, const ovms::shape_t& shape, const std::string& outputName) {
     auto it = response.outputs().begin();
@@ -289,7 +386,7 @@ void TestPredict<KFSInterface>::checkOutputShape(const KFSResponse& response, co
 class MockModelInstance : public ovms::ModelInstance {
 public:
     MockModelInstance(ov::Core& ieCore) :
-        ModelInstance("UNUSED_NAME", 42, ieCore) {}
+        ModelInstance(UNUSED_SERVABLE_NAME, UNUSED_MODEL_VERSION, ieCore) {}
     template <typename RequestType>
     const ovms::Status mockValidate(const RequestType* request) {
         return validate(request);
@@ -361,12 +458,13 @@ void TestPredict<Pair, RequestType, ResponseType>::performPredict(const std::str
         DUMMY_MODEL_OUTPUT_NAME);
 }
 
-using MyTypes = ::testing::Types<TFSInterface, KFSInterface>;
+using MyTypes = ::testing::Types<TFSInterface, KFSInterface, CAPIInterface>;
 TYPED_TEST_SUITE(TestPredict, MyTypes);
 
 TYPED_TEST(TestPredict, SuccesfullOnDummyModel) {
     typename TypeParam::first_type request;
-    preparePredictRequest(request,
+    Preparer<typename TypeParam::first_type> preparer;
+    preparer.preparePredictRequest(request,
         {{DUMMY_MODEL_INPUT_NAME,
             std::tuple<ovms::shape_t, ovms::Precision>{{1, 10}, ovms::Precision::FP32}}});
     ovms::ModelConfig config = DUMMY_MODEL_CONFIG;
@@ -429,8 +527,9 @@ public:
 TYPED_TEST_SUITE(TestPredictWithMapping, MyTypes);
 
 TYPED_TEST(TestPredictWithMapping, SuccesfullOnDummyModelWithMapping) {
+    Preparer<typename TypeParam::first_type> preparer;
     typename TypeParam::first_type request;
-    preparePredictRequest(request,
+    preparer.preparePredictRequest(request,
         {{this->dummyModelInputMapping,
             std::tuple<ovms::shape_t, ovms::Precision>{{1, 10}, ovms::Precision::FP32}}});
     ovms::ModelConfig config = DUMMY_MODEL_CONFIG;
@@ -441,8 +540,9 @@ TYPED_TEST(TestPredictWithMapping, SuccesfullOnDummyModelWithMapping) {
 }
 
 TYPED_TEST(TestPredict, SuccesfullReloadFromAlreadyLoadedWithNewBatchSize) {
+    Preparer<typename TypeParam::first_type> preparer;
     typename TypeParam::first_type request;
-    preparePredictRequest(request,
+    preparer.preparePredictRequest(request,
         {{DUMMY_MODEL_INPUT_NAME,
             std::tuple<ovms::shape_t, ovms::Precision>{{1, 10}, ovms::Precision::FP32}}});
     ovms::ModelConfig config = DUMMY_MODEL_CONFIG;
@@ -454,12 +554,13 @@ TYPED_TEST(TestPredict, SuccesfullReloadFromAlreadyLoadedWithNewBatchSize) {
 
 TYPED_TEST(TestPredict, SuccesfullReloadWhen1InferenceInProgress) {
     //  FIRST LOAD MODEL WITH BS=1
+    Preparer<typename TypeParam::first_type> preparer;
     typename TypeParam::first_type requestBs1;
-    preparePredictRequest(requestBs1,
+    preparer.preparePredictRequest(requestBs1,
         {{DUMMY_MODEL_INPUT_NAME,
             std::tuple<ovms::shape_t, ovms::Precision>{{1, 10}, ovms::Precision::FP32}}});
     typename TypeParam::first_type requestBs2;
-    preparePredictRequest(requestBs2,
+    preparer.preparePredictRequest(requestBs2,
         {{DUMMY_MODEL_INPUT_NAME,
             std::tuple<ovms::shape_t, ovms::Precision>{{2, 10}, ovms::Precision::FP32}}});
 
@@ -488,14 +589,15 @@ TYPED_TEST(TestPredict, SuccesfullReloadWhen1InferenceInProgress) {
 
 TYPED_TEST(TestPredict, SuccesfullReloadWhen1InferenceAboutToStart) {
     //  FIRST LOAD MODEL WITH BS=1
-    typename TypeParam::first_type requestBs1;
-    preparePredictRequest(requestBs1,
-        {{DUMMY_MODEL_INPUT_NAME,
-            std::tuple<ovms::shape_t, ovms::Precision>{{1, 10}, ovms::Precision::FP32}}});
+    Preparer<typename TypeParam::first_type> preparer;
     typename TypeParam::first_type requestBs2;
-    preparePredictRequest(requestBs2,
+    preparer.preparePredictRequest(requestBs2,
         {{DUMMY_MODEL_INPUT_NAME,
             std::tuple<ovms::shape_t, ovms::Precision>{{2, 10}, ovms::Precision::FP32}}});
+    typename TypeParam::first_type requestBs1;
+    preparer.preparePredictRequest(requestBs1,
+        {{DUMMY_MODEL_INPUT_NAME,
+            std::tuple<ovms::shape_t, ovms::Precision>{{1, 10}, ovms::Precision::FP32}}});
 
     this->config.setBatchingParams("auto");
     this->config.setNireq(2);
@@ -561,22 +663,18 @@ TYPED_TEST(TestPredict, SuccesfullReshapeViaRequestOnDummyModel) {
     config.parseShapeParameter("auto");
     ASSERT_EQ(this->manager.reloadModelWithVersions(config), ovms::StatusCode::OK_RELOADED);
 
-    // Get dummy model instance
-    std::shared_ptr<ovms::ModelInstance> model;
-    std::unique_ptr<ovms::ModelInstanceUnloadGuard> unload_guard;
-    auto status = this->manager.getModelInstance("dummy", 0, model, unload_guard);
-
     // Prepare request with 1x5 shape, expect reshape
+    Preparer<typename TypeParam::first_type> preparer;
     typename TypeParam::first_type request;
-    preparePredictRequest(request,
+    preparer.preparePredictRequest(request,
         {{DUMMY_MODEL_INPUT_NAME,
             std::tuple<ovms::shape_t, ovms::Precision>{{1, 5}, ovms::Precision::FP32}}});
 
     typename TypeParam::second_type response;
 
     // Do the inference
-    ASSERT_EQ(model->infer(&request, &response, unload_guard), ovms::StatusCode::OK);
-
+    auto status = this->performInferenceWithRequest(request, response, "dummy");
+    ASSERT_EQ(status, StatusCode::OK) << status.string();
     // Expect reshape to 1x5
     this->checkOutputShape(response, {1, 5}, DUMMY_MODEL_OUTPUT_NAME);
 }
@@ -642,7 +740,6 @@ TYPED_TEST(TestPredict, ReshapeViaRequestAndConfigChange) {
  */
 TYPED_TEST(TestPredict, ChangeBatchSizeViaRequestAndConfigChange) {
     using namespace ovms;
-
     // Prepare model with shape=auto (initially (1,10) shape)
     ModelConfig config = DUMMY_MODEL_CONFIG;
     this->config.setBatchingParams("auto");
@@ -699,9 +796,9 @@ TYPED_TEST(TestPredict, PerformInferenceChangeModelInputLayout) {
     typename TypeParam::second_type response;
 
     // Perform inference with NHWC layout, ensure status OK and correct results
-    ASSERT_EQ(this->performInferenceWithImageInput(response, {1, 4, 5, 3}), ovms::StatusCode::OK);
+    auto status = this->performInferenceWithImageInput(response, {1, 4, 5, 3});
+    ASSERT_EQ(status, ovms::StatusCode::OK) << status.string();
     this->checkOutputShape(response, {1, 3, 4, 5}, INCREMENT_1x3x4x5_MODEL_OUTPUT_NAME);
-
     // Perform inference with NCHW layout, ensure error
     ASSERT_EQ(this->performInferenceWithImageInput(response, {1, 3, 4, 5}), ovms::StatusCode::INVALID_SHAPE);
 
@@ -936,6 +1033,9 @@ TYPED_TEST(TestPredict, NetworkNotLoadedWhenLayoutAndDimsInconsistent) {
  * 6. Do the inference with single binary image tensor - expect status OK and result in NCHW layout
  * */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputChangeModelInputLayout) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
+
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -977,6 +1077,8 @@ TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputChangeModelInputLayout) {
  * 2. Do the inference with single binary image tensor with witdth exceeding shape range - expect status OK and reshaped output tensor
  */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputAndShapeDynamic) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -1001,6 +1103,8 @@ TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputAndShapeDynamic) {
  * 2. Do the inference with batch=5 binary image tensor - expect status OK and result in NCHW layout
  */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputBatchSizeAuto) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -1025,6 +1129,8 @@ TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputBatchSizeAuto) {
  * 2. Do the inference with binary image tensor with no shape set - expect status INVALID_NO_OF_SHAPE_DIMENSIONS
 */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputNoInputShape) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -1147,14 +1253,34 @@ TYPED_TEST(TestPredict, PerformInferenceDummyBatchSizeAny) {
  * 2. Do the inferences with (3, 10) shape, expect correct output shapes and precision
 */
 
-ovms::Precision getPrecisionFromResponse(KFSResponse& response, const std::string& name) {
+static ovms::Precision getPrecisionFromResponse(ovms::InferenceResponse& response, const std::string& name) {
+    size_t outputCount = response.getOutputCount();
+    EXPECT_GE(1, outputCount);
+    size_t outputId = 0;
+    while (outputId < outputCount) {
+        const std::string* cppName;
+        InferenceTensor* tensor;
+        auto status = response.getOutput(outputId, &cppName, &tensor);
+        EXPECT_EQ(status, StatusCode::OK) << status.string();
+        EXPECT_NE(nullptr, tensor);
+        EXPECT_NE(nullptr, cppName);
+        if (name == *cppName) {
+            return ovms::getOVMSDataTypeAsPrecision(tensor->getDataType());
+        }
+        ++outputId;
+    }
+    return ovms::getOVMSDataTypeAsPrecision(OVMS_DATATYPE_UNDEFINED);
+}
+
+static ovms::Precision getPrecisionFromResponse(KFSResponse& response, const std::string& name) {
     KFSOutputTensorIteratorType it;
     size_t bufferId;
     auto status = getOutput(response, name, it, bufferId);
     EXPECT_TRUE(status.ok());
     return ovms::KFSPrecisionToOvmsPrecision(it->datatype());
 }
-ovms::Precision getPrecisionFromResponse(TFSResponseType& response, const std::string& name) {
+
+static ovms::Precision getPrecisionFromResponse(TFSResponseType& response, const std::string& name) {
     TFSOutputTensorIteratorType it;
     size_t bufferId;
     auto status = getOutput(response, name, it, bufferId);
@@ -1173,7 +1299,8 @@ TYPED_TEST(TestPredict, PerformInferenceDummyFp64) {
     typename TypeParam::first_type request;
     typename TypeParam::second_type response;
 
-    preparePredictRequest(request, {{"input:0", std::tuple<ovms::shape_t, ovms::Precision>{{3, 10}, ovms::Precision::FP64}}});
+    Preparer<typename TypeParam::first_type> preparer;
+    preparer.preparePredictRequest(request, {{"input:0", std::tuple<ovms::shape_t, ovms::Precision>{{3, 10}, ovms::Precision::FP64}}});
     ASSERT_EQ(this->performInferenceWithRequest(request, response, "dummy_fp64"), ovms::StatusCode::OK);
     this->checkOutputShape(response, {3, 10}, "output:0");
     ASSERT_EQ(getPrecisionFromResponse(response, "output:0"), ovms::Precision::FP64);
@@ -1221,6 +1348,8 @@ TYPED_TEST(TestPredict, PerformInferenceDummyAllDimensionsHaveRange) {
  * 2. Do the inference with batch=5 binary image tensor 1x1 - expect status INVALID_SHAPE, because if any dimension is dynamic, we perform no resize operation.
  */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputBatchSizeAnyResolutionNotMatching) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -1243,6 +1372,8 @@ TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputBatchSizeAnyResolutionNot
  * 2. Do the inference with batch=5 binary image tensor 1x1 - expect status OK, and correct results.
  */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputBatchSizeAnyResolutionMatching) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -1267,6 +1398,8 @@ TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputBatchSizeAnyResolutionMat
  * 2. Do the inference with resolution 1x1 binary image tensor - expect status OK and result in NCHW layout
  */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputResolutionAny) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
@@ -1291,6 +1424,8 @@ TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputResolutionAny) {
  * 3. Do the inference with resolution 1x1 binary image tensor - expect status OK and result in NCHW layout
  */
 TYPED_TEST(TestPredict, PerformInferenceWithBinaryInputResolutionRange) {
+    if (typeid(typename TypeParam::first_type) == typeid(ovms::InferenceRequest))
+        GTEST_SKIP() << "Binary inputs not implemented for C-API yet";
     using namespace ovms;
 
     // Prepare model with changed layout to nhwc (internal layout=nchw)
diff --git a/src/test/serialization_tests.cpp b/src/test/serialization_tests.cpp
index 8f3ce4d8..f5c339fc 100644
--- a/src/test/serialization_tests.cpp
+++ b/src/test/serialization_tests.cpp
@@ -166,6 +166,11 @@ const std::vector<ovms::Precision> UNSUPPORTED_CAPI_OUTPUT_PRECISIONS{
     // ovms::Precision::UNDEFINED,  // Cannot create ov tensor with such precision
 };
 
+namespace {
+const std::string UNUSED_NAME{"UNUSED_NAME"};
+const model_version_t UNUSED_VERSION{0};
+}  // namespace
+
 class TensorflowGRPCPredict : public ::testing::TestWithParam<ovms::Precision> {
 protected:
     void SetUp() override {
@@ -286,7 +291,7 @@ TEST(SerializeTFGRPCPredictResponse, ShouldSuccessForSupportedPrecision) {
     ov::Tensor tensor(tensorInfo->getOvPrecision(), ov::Shape{1, 10});
     inferRequest.set_tensor(DUMMY_MODEL_OUTPUT_NAME, tensor);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, tenMap, &response, getTensorInfoName);
+    auto status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, tenMap, &response, getTensorInfoName);
     EXPECT_TRUE(status.ok());
 }
 
@@ -510,7 +515,7 @@ TEST(SerializeKFSGRPCPredictResponse, ShouldSuccessForSupportedPrecision) {
     ov::Tensor tensor(tensorInfo->getOvPrecision(), ov::Shape{1, 10});
     inferRequest.set_tensor(DUMMY_MODEL_OUTPUT_NAME, tensor);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, tenMap, &response, getTensorInfoName);
+    auto status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, tenMap, &response, getTensorInfoName);
     ASSERT_TRUE(status.ok());
     EXPECT_EQ(DUMMY_MODEL_OUTPUT_NAME, response.outputs(0).name());
     EXPECT_EQ("FP32", response.outputs(0).datatype());
@@ -535,7 +540,7 @@ TEST(SerializeKFSGRPCPredictResponse, ShouldSuccessForSupportedPrecisionWithuseS
     ov::Tensor tensor(tensorInfo->getOvPrecision(), ov::Shape{1, 10});
     inferRequest.set_tensor(DUMMY_MODEL_OUTPUT_NAME, tensor);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, tenMap, &response, getTensorInfoName, true);
+    auto status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, tenMap, &response, getTensorInfoName, true);
     ASSERT_TRUE(status.ok());
     EXPECT_EQ(DUMMY_MODEL_INPUT_NAME, response.outputs(0).name());
     EXPECT_EQ("FP32", response.outputs(0).datatype());
@@ -561,7 +566,7 @@ TEST(SerializeKFSGRPCPredictResponse, ShouldSuccessForSupportedPrecisionWithshar
     ov::Tensor tensor(tensorInfo->getOvPrecision(), ov::Shape{1, 10});
     inferRequest.set_tensor(DUMMY_MODEL_OUTPUT_NAME, tensor);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, tenMap, &response, getTensorInfoName, false);
+    auto status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, tenMap, &response, getTensorInfoName, false);
     ASSERT_TRUE(status.ok());
     EXPECT_EQ(DUMMY_MODEL_INPUT_NAME, response.outputs(0).name());
     EXPECT_EQ("FP32", response.outputs(0).datatype());
@@ -619,7 +624,7 @@ TEST(SerializeCApiTensorSingle, NegativeMismatchBetweenTensorInfoAndTensorPrecis
     std::memcpy(tensor.data(), data, tensor.get_byte_size());
     inferRequest.set_tensor(DUMMY_MODEL_OUTPUT_NAME, tensor);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, tenMap, &response, getTensorInfoName);
+    auto status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, tenMap, &response, getTensorInfoName);
     EXPECT_EQ(status.getCode(), ovms::StatusCode::INTERNAL_ERROR);
 }
 
@@ -641,7 +646,7 @@ TEST(SerializeCApiTensorSingle, NegativeMismatchBetweenTensorInfoAndTensorShape)
     std::memcpy(tensor.data(), data, tensor.get_byte_size());
     inferRequest.set_tensor(DUMMY_MODEL_OUTPUT_NAME, tensor);
     OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-    auto status = serializePredictResponse(outputGetter, tenMap, &response, getTensorInfoName);
+    auto status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, tenMap, &response, getTensorInfoName);
     EXPECT_EQ(status.getCode(), ovms::StatusCode::INTERNAL_ERROR);
 }
 
@@ -666,6 +671,8 @@ TEST_P(SerializeCApiTensorPositive, SerializeTensorShouldSucceedForPrecision) {
 
     auto inputs = prepareInputs(testedPrecision);
     auto status = serializePredictResponse(outputGetter,
+        UNUSED_NAME,
+        UNUSED_VERSION,
         inputs,
         &response,
         getTensorInfoName);
@@ -693,6 +700,8 @@ TEST_P(SerializeCApiTensorNegative, SerializeTensorShouldFailForPrecision) {
 
     auto inputs = prepareInputs(testedPrecision);
     auto status = serializePredictResponse(outputGetter,
+        UNUSED_NAME,
+        UNUSED_VERSION,
         inputs,
         &response,
         getTensorInfoName);
@@ -721,12 +730,21 @@ TEST_F(CApiSerialization, ValidSerialization) {
 
     auto inputs = prepareInputs(ovms::Precision::FP32, shape);
     auto status = serializePredictResponse(outputGetter,
+        UNUSED_NAME,
+        UNUSED_VERSION,
         inputs,
         &response,
         getTensorInfoName);
     ASSERT_EQ(status.getCode(), ovms::StatusCode::OK);
     InferenceTensor* responseOutput{nullptr};
-    ASSERT_EQ(response.getOutput(DUMMY_MODEL_OUTPUT_NAME, &responseOutput), StatusCode::OK);
+    uint32_t outputCount = response.getOutputCount();
+    ASSERT_EQ(1, outputCount);
+    ASSERT_EQ(status, ovms::StatusCode::OK) << status.string();
+    const std::string* outputName{nullptr};
+    status = response.getOutput(0, &outputName, &responseOutput);
+    ASSERT_EQ(status, ovms::StatusCode::OK) << status.string();
+    ASSERT_NE(outputName, nullptr);
+    ASSERT_EQ(*outputName, DUMMY_MODEL_OUTPUT_NAME);
     ASSERT_NE(responseOutput, nullptr);
     EXPECT_EQ(responseOutput->getDataType(), OVMS_DATATYPE_FP32);
     EXPECT_THAT(responseOutput->getShape(), ElementsAre(1, NUMBER_OF_ELEMENTS, 1, 1));
diff --git a/src/test/stateful_modelinstance_test.cpp b/src/test/stateful_modelinstance_test.cpp
index 229fe3f3..121d56f7 100644
--- a/src/test/stateful_modelinstance_test.cpp
+++ b/src/test/stateful_modelinstance_test.cpp
@@ -30,6 +30,7 @@
 #include "../get_model_metadata_impl.hpp"
 #include "../global_sequences_viewer.hpp"
 #include "../modelinstanceunloadguard.hpp"
+#include "../modelversion.hpp"
 #include "../ov_utils.hpp"
 #include "../sequence_processing_spec.hpp"
 #include "../serialization.hpp"
@@ -41,6 +42,8 @@
 using testing::Return;
 
 namespace {
+const std::string UNUSED_NAME{"UNUSED_NAME"};
+const ovms::model_version_t UNUSED_VERSION{0};
 static bool testWarningPrinted = false;
 
 enum SequenceTimeoutScenarios {
@@ -147,8 +150,12 @@ public:
     MockedValidateStatefulModelInstance(const std::string& name, ovms::model_version_t version, ov::Core& ieCore) :
         StatefulModelInstance(name, version, ieCore, nullptr, nullptr, &sequencesViewer) {}
 
-    const ovms::Status mockValidate(const tensorflow::serving::PredictRequest* request, ovms::SequenceProcessingSpec& processingSpec) {
-        return validate(request, processingSpec);
+    const ovms::Status mockValidate(const tensorflow::serving::PredictRequest* request) {
+        ovms::StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse> sp(*this->getSequenceManager());
+        auto status = sp.extractRequestParameters(request);
+        if (!status.ok())
+            return status;
+        return validate(request);
     }
 };
 
@@ -203,8 +210,11 @@ public:
         };
         ovms::Timer<TIMER_END> timer;
         using std::chrono::microseconds;
-        ovms::SequenceProcessingSpec sequenceProcessingSpec;
-        auto status = validate(requestProto, sequenceProcessingSpec);
+        ovms::StatefulRequestProcessor<tensorflow::serving::PredictRequest, tensorflow::serving::PredictResponse> requestProcessor(*this->getSequenceManager());
+        auto status = requestProcessor.extractRequestParameters(requestProto);
+        if (!status.ok())
+            return status;
+        status = validate(requestProto);
         if (!status.ok())
             return status;
 
@@ -212,30 +222,22 @@ public:
             std::cout << "Waiting before sequenceManagerLock" << std::endl;
             waitBeforeManagerLock->get();
         }
-        std::unique_lock<std::mutex> sequenceManagerLock(sequenceManager->getMutex());
-        status = sequenceManager->processRequestedSpec(sequenceProcessingSpec);
+        status = requestProcessor.prepare();
         if (!status.ok())
             return status;
-        const uint64_t sequenceId = sequenceProcessingSpec.getSequenceId();
-        if (!sequenceManager->sequenceExists(sequenceId))
-            return ovms::StatusCode::INTERNAL_ERROR;
-        ovms::Sequence& sequence = sequenceManager->getSequence(sequenceId);
 
-        if (waitBeforeSequenceLock) {
+        if (waitBeforeSequenceLock) {  // TODO remove since right now it is in singel step
             std::cout << "Waiting before waitBeforeSequenceLock" << std::endl;
             waitBeforeSequenceLock->get();
         }
 
-        std::unique_lock<std::mutex> sequenceLock(sequence.getMutex());
-        sequenceManagerLock.unlock();
-
         timer.start(GET_INFER_REQUEST);
         ovms::ExecutingStreamIdGuard executingStreamIdGuard(getInferRequestsQueue(), this->getMetricReporter());
         ov::InferRequest& inferRequest = executingStreamIdGuard.getInferRequest();
         timer.stop(GET_INFER_REQUEST);
 
         timer.start(PREPROCESS);
-        status = preInferenceProcessing(inferRequest, sequence, sequenceProcessingSpec);
+        status = requestProcessor.preInferenceProcessing(inferRequest);
         if (!status.ok())
             return status;
         timer.stop(PREPROCESS);
@@ -257,13 +259,13 @@ public:
 
         timer.start(SERIALIZE);
         ovms::OutputGetter<ov::InferRequest&> outputGetter(inferRequest);
-        status = serializePredictResponse(outputGetter, getOutputsInfo(), responseProto, ovms::getTensorInfoName);
+        status = serializePredictResponse(outputGetter, UNUSED_NAME, UNUSED_VERSION, getOutputsInfo(), responseProto, ovms::getTensorInfoName);
         timer.stop(SERIALIZE);
         if (!status.ok())
             return status;
 
         timer.start(POSTPROCESS);
-        status = postInferenceProcessing(responseProto, inferRequest, sequence, sequenceProcessingSpec);
+        status = requestProcessor.postInferenceProcessing(responseProto, inferRequest);
         timer.stop(POSTPROCESS);
         if (!status.ok())
             return status;
@@ -272,16 +274,8 @@ public:
             std::cout << "Waiting before waitBeforeSequenceUnlocked" << std::endl;
             waitBeforeSequenceUnlocked->get();
         }
-
-        sequenceLock.unlock();
-        if (sequenceProcessingSpec.getSequenceControlInput() == ovms::SEQUENCE_END) {
-            sequenceManagerLock.lock();
-            status = sequenceManager->removeSequence(sequenceId);
-            if (!status.ok())
-                return status;
-        }
-
-        return ovms::StatusCode::OK;
+        status = requestProcessor.release();
+        return status;
     }
 };
 
@@ -1119,21 +1113,21 @@ TEST_F(StatefulModelInstanceInputValidation, positiveValidate) {
     setRequestSequenceId(&request, seqId);
     setRequestSequenceControl(&request, ovms::SEQUENCE_START);
 
-    auto status = modelInstance->mockValidate(&request, spec);
+    auto status = modelInstance->mockValidate(&request);
     ASSERT_TRUE(status.ok());
 
     preparePredictRequest(request, modelInput);
     setRequestSequenceId(&request, seqId);
     setRequestSequenceControl(&request, ovms::SEQUENCE_END);
 
-    status = modelInstance->mockValidate(&request, spec);
+    status = modelInstance->mockValidate(&request);
     ASSERT_TRUE(status.ok());
 
     preparePredictRequest(request, modelInput);
     setRequestSequenceId(&request, seqId);
     setRequestSequenceControl(&request, ovms::NO_CONTROL_INPUT);
 
-    status = modelInstance->mockValidate(&request, spec);
+    status = modelInstance->mockValidate(&request);
     ASSERT_TRUE(status.ok());
 }
 
@@ -1144,7 +1138,7 @@ TEST_F(StatefulModelInstanceInputValidation, missingSeqId) {
     preparePredictRequest(request, modelInput);
     setRequestSequenceControl(&request, ovms::SEQUENCE_END);
 
-    auto status = modelInstance->mockValidate(&request, spec);
+    auto status = modelInstance->mockValidate(&request);
     ASSERT_EQ(status.getCode(), ovms::StatusCode::SEQUENCE_ID_NOT_PROVIDED);
 }
 
@@ -1157,7 +1151,7 @@ TEST_F(StatefulModelInstanceInputValidation, wrongSeqIdEnd) {
 
     uint64_t seqId = 0;
     setRequestSequenceId(&request, seqId);
-    auto status = modelInstance->mockValidate(&request, spec);
+    auto status = modelInstance->mockValidate(&request);
     ASSERT_EQ(status.getCode(), ovms::StatusCode::SEQUENCE_ID_NOT_PROVIDED);
 }
 
@@ -1170,7 +1164,7 @@ TEST_F(StatefulModelInstanceInputValidation, wrongSeqIdNoControl) {
 
     uint64_t seqId = 0;
     setRequestSequenceId(&request, seqId);
-    auto status = modelInstance->mockValidate(&request, spec);
+    auto status = modelInstance->mockValidate(&request);
     ASSERT_EQ(status.getCode(), ovms::StatusCode::SEQUENCE_ID_NOT_PROVIDED);
 }
 
@@ -1183,7 +1177,7 @@ TEST_F(StatefulModelInstanceInputValidation, wrongProtoKeywords) {
     input.set_dtype(tensorflow::DataType::DT_UINT64);
     input.mutable_tensor_shape()->add_dim()->set_size(1);
     input.add_uint64_val(12);
-    auto status = modelInstance->mockValidate(&request, spec);
+    auto status = modelInstance->mockValidate(&request);
     ASSERT_EQ(status.getCode(), ovms::StatusCode::SEQUENCE_ID_NOT_PROVIDED);
 }
 
@@ -1196,7 +1190,7 @@ TEST_F(StatefulModelInstanceInputValidation, badControlInput) {
     input.set_dtype(tensorflow::DataType::DT_UINT32);
     input.mutable_tensor_shape()->add_dim()->set_size(1);
     input.add_uint32_val(999);
-    auto status = modelInstance->mockValidate(&request, spec);
+    auto status = modelInstance->mockValidate(&request);
     ASSERT_EQ(status.getCode(), ovms::StatusCode::INVALID_SEQUENCE_CONTROL_INPUT);
 }
 
@@ -1210,7 +1204,7 @@ TEST_F(StatefulModelInstanceInputValidation, invalidProtoTypes) {
         input.set_dtype(tensorflow::DataType::DT_UINT32);
         input.mutable_tensor_shape()->add_dim()->set_size(1);
         input.add_uint32_val(12);
-        auto status = modelInstance->mockValidate(&request, spec);
+        auto status = modelInstance->mockValidate(&request);
         ASSERT_EQ(status.getCode(), ovms::StatusCode::SEQUENCE_ID_BAD_TYPE);
     }
     {
@@ -1220,7 +1214,7 @@ TEST_F(StatefulModelInstanceInputValidation, invalidProtoTypes) {
         input.set_dtype(tensorflow::DataType::DT_UINT64);
         input.mutable_tensor_shape()->add_dim()->set_size(1);
         input.add_uint64_val(1);
-        auto status = modelInstance->mockValidate(&request, spec);
+        auto status = modelInstance->mockValidate(&request);
         ASSERT_EQ(status.getCode(), ovms::StatusCode::SEQUENCE_CONTROL_INPUT_BAD_TYPE);
     }
 }
diff --git a/src/test/test_utils.cpp b/src/test/test_utils.cpp
index 6845ff9b..006cfc70 100644
--- a/src/test/test_utils.cpp
+++ b/src/test/test_utils.cpp
@@ -17,7 +17,10 @@
 
 #include <functional>
 
+#include "../capi_frontend/capi_utils.hpp"
+#include "../inferenceparameter.hpp"
 #include "../kfs_frontend/kfs_utils.hpp"
+#include "../pocapiinternal.hpp"
 #include "../prediction_service_utils.hpp"
 #include "../tensorinfo.hpp"
 #include "../tfs_frontend/tfs_utils.hpp"
@@ -27,6 +30,10 @@ using tensorflow::serving::PredictResponse;
 
 using ovms::TensorInfo;
 
+void prepareBinaryPredictRequest(ovms::InferenceRequest& request, const std::string& inputName, const int batchSize) { throw 42; }         // CAPI binary not supported
+void prepareBinaryPredictRequestNoShape(ovms::InferenceRequest& request, const std::string& inputName, const int batchSize) { throw 42; }  // CAPI binary not supported
+void prepareBinary4x4PredictRequest(ovms::InferenceRequest& request, const std::string& inputName, const int batchSize) { throw 42; }      // CAPI binary not supported
+
 void preparePredictRequest(::KFSRequest& request, inputs_info_t requestInputs, const std::vector<float>& data, bool putBufferInInputTensorContent) {
     request.mutable_inputs()->Clear();
     request.mutable_raw_input_contents()->Clear();
@@ -35,6 +42,13 @@ void preparePredictRequest(::KFSRequest& request, inputs_info_t requestInputs, c
     }
 }
 
+void preparePredictRequest(ovms::InferenceRequest& request, inputs_info_t requestInputs, const std::vector<float>& data, uint32_t decrementBufferSize, BufferType bufferType, std::optional<uint32_t> deviceId) {
+    request.removeAllInputs();
+    for (auto const& it : requestInputs) {
+        prepareCAPIInferInputTensor(request, it.first, it.second, data, decrementBufferSize, bufferType, deviceId);
+    }
+}
+
 void preparePredictRequest(tensorflow::serving::PredictRequest& request, inputs_info_t requestInputs, const std::vector<float>& data) {
     request.mutable_inputs()->clear();
     for (auto const& it : requestInputs) {
@@ -374,6 +388,32 @@ void prepareKFSInferInputTensor(::KFSRequest& request, const std::string& name,
         data, putBufferInInputTensorContent);
 }
 
+void prepareCAPIInferInputTensor(ovms::InferenceRequest& request, const std::string& name, const std::tuple<ovms::shape_t, const ovms::Precision>& inputInfo,
+    const std::vector<float>& data, uint32_t decrementBufferSize, BufferType bufferType, std::optional<uint32_t> deviceId) {
+    auto [shape, type] = inputInfo;
+    prepareCAPIInferInputTensor(request, name,
+        {shape, getPrecisionAsOVMSDataType(type)},
+        data, decrementBufferSize, bufferType, deviceId);
+}
+
+void prepareCAPIInferInputTensor(ovms::InferenceRequest& request, const std::string& name, const std::tuple<ovms::shape_t, OVMS_DataType>& inputInfo,
+    const std::vector<float>& data, uint32_t decrementBufferSize, BufferType bufferType, std::optional<uint32_t> deviceId) {
+    auto [shape, datatype] = inputInfo;
+    size_t elementsCount = 1;
+    for (auto const& dim : shape) {
+        ASSERT_GE(dim, 0);
+        elementsCount *= dim;
+    }
+
+    request.addInput(name.c_str(), datatype, shape.data(), shape.size());
+
+    size_t dataSize = elementsCount * ovms::DataTypeToByteSize(datatype);
+    if (decrementBufferSize)
+        dataSize -= decrementBufferSize;
+
+    request.setInputBuffer(name.c_str(), data.data(), dataSize, bufferType, deviceId);
+}
+
 void prepareKFSInferInputTensor(::KFSRequest& request, const std::string& name, const std::tuple<ovms::shape_t, const std::string>& inputInfo,
     const std::vector<float>& data, bool putBufferInInputTensorContent) {
     auto it = request.mutable_inputs()->begin();
diff --git a/src/test/test_utils.hpp b/src/test/test_utils.hpp
index 3aa3c42a..6db6bc32 100644
--- a/src/test/test_utils.hpp
+++ b/src/test/test_utils.hpp
@@ -34,9 +34,13 @@
 #pragma GCC diagnostic ignored "-Wall"
 #include "tensorflow_serving/apis/prediction_service.grpc.pb.h"
 #pragma GCC diagnostic pop
+#include "../config.hpp"
 #include "../execution_context.hpp"
+#include "../inferencerequest.hpp"
+#include "../inferenceresponse.hpp"
 #include "../kfs_frontend/kfs_grpc_inference_service.hpp"
 #include "../metric_registry.hpp"
+#include "../modelinstance.hpp"
 #include "../modelmanager.hpp"
 #include "../node_library.hpp"
 #include "../tensorinfo.hpp"
@@ -128,6 +132,7 @@ constexpr const char* INCREMENT_1x3x4x5_MODEL_INPUT_NAME = "input";
 constexpr const char* INCREMENT_1x3x4x5_MODEL_OUTPUT_NAME = "output";
 constexpr const float INCREMENT_1x3x4x5_ADDITION_VALUE = 1.0;
 
+const std::string UNUSED_SERVABLE_NAME = "UNUSED_SERVABLE_NAME";
 constexpr const ovms::model_version_t UNUSED_MODEL_VERSION = 42;  // Answer to the Ultimate Question of Life
 
 static const ovms::ExecutionContext DEFAULT_TEST_CONTEXT{ovms::ExecutionContext::Interface::GRPC, ovms::ExecutionContext::Method::Predict};
@@ -141,6 +146,7 @@ using TFSInputTensorIteratorType = google::protobuf::Map<std::string, TFSInputTe
 using TFSOutputTensorIteratorType = google::protobuf::Map<std::string, TFSOutputTensorType>::const_iterator;
 using TFSInterface = std::pair<TFSRequestType, TFSResponseType>;
 using KFSInterface = std::pair<KFSRequest, KFSResponse>;
+using CAPIInterface = std::pair<ovms::InferenceRequest, ovms::InferenceResponse>;
 
 #pragma GCC diagnostic push
 #pragma GCC diagnostic ignored "-Wunused-function"
@@ -158,16 +164,26 @@ void prepareKFSInferInputTensor(::KFSRequest& request, const std::string& name,
 void prepareKFSInferInputTensor(::KFSRequest& request, const std::string& name, const std::tuple<ovms::shape_t, const ovms::Precision>& inputInfo,
     const std::vector<float>& data = {}, bool putBufferInInputTensorContent = false);
 
+void prepareCAPIInferInputTensor(ovms::InferenceRequest& request, const std::string& name, const std::tuple<ovms::shape_t, OVMS_DataType>& inputInfo,
+    const std::vector<float>& data, uint32_t decrementBufferSize = 0, BufferType bufferType = BufferType::OVMS_BUFFERTYPE_CPU, std::optional<uint32_t> deviceId = std::nullopt);
+void prepareCAPIInferInputTensor(ovms::InferenceRequest& request, const std::string& name, const std::tuple<ovms::shape_t, const ovms::Precision>& inputInfo,
+    const std::vector<float>& data, uint32_t decrementBufferSize = 0, BufferType bufferType = BufferType::OVMS_BUFFERTYPE_CPU, std::optional<uint32_t> deviceId = std::nullopt);
+
 void preparePredictRequest(::KFSRequest& request, inputs_info_t requestInputs, const std::vector<float>& data = {}, bool putBufferInInputTensorContent = false);
 
+void preparePredictRequest(ovms::InferenceRequest& request, inputs_info_t requestInputs, const std::vector<float>& data,
+    uint32_t decrementBufferSize = 0, BufferType bufferType = BufferType::OVMS_BUFFERTYPE_CPU, std::optional<uint32_t> deviceId = std::nullopt);
+
 void prepareBinaryPredictRequest(tensorflow::serving::PredictRequest& request, const std::string& inputName, const int batchSize);
 void prepareBinaryPredictRequest(::KFSRequest& request, const std::string& inputName, const int batchSize);
+void prepareBinaryPredictRequest(ovms::InferenceRequest& request, const std::string& inputName, const int batchSize);  // CAPI binary not supported
 
 void prepareBinaryPredictRequestNoShape(tensorflow::serving::PredictRequest& request, const std::string& inputName, const int batchSize);
 void prepareBinaryPredictRequestNoShape(::KFSRequest& request, const std::string& inputName, const int batchSize);
-
+void prepareBinaryPredictRequestNoShape(ovms::InferenceRequest& request, const std::string& inputName, const int batchSize);  // CAPI binary not supported
 void prepareBinary4x4PredictRequest(tensorflow::serving::PredictRequest& request, const std::string& inputName, const int batchSize = 1);
 void prepareBinary4x4PredictRequest(::KFSRequest& request, const std::string& inputName, const int batchSize = 1);
+void prepareBinary4x4PredictRequest(ovms::InferenceRequest& request, const std::string& inputName, const int batchSize = 1);  // CAPI binary not supported
 
 template <typename TensorType>
 void prepareInvalidImageBinaryTensor(TensorType& tensor);
@@ -298,6 +314,24 @@ public:
     }
 };
 
+class MockedMetadataModelIns : public ovms::ModelInstance {
+public:
+    MockedMetadataModelIns(ov::Core& ieCore) :
+        ModelInstance("UNUSED_NAME", 42, ieCore) {}
+    MOCK_METHOD(const ovms::tensor_map_t&, getInputsInfo, (), (const, override));
+    MOCK_METHOD(ovms::Dimension, getBatchSize, (), (const, override));
+    MOCK_METHOD(const ovms::ModelConfig&, getModelConfig, (), (const, override));
+    const ovms::Status mockValidate(const tensorflow::serving::PredictRequest* request) {
+        return validate(request);
+    }
+    const ovms::Status mockValidate(const ::KFSRequest* request) {
+        return validate(request);
+    }
+    const ovms::Status mockValidate(const ovms::InferenceRequest* request) {
+        return validate(request);
+    }
+};
+
 class ResourcesAccessModelManager : public ConstructorEnabledModelManager {
 public:
     int getResourcesSize() {
@@ -502,4 +536,49 @@ static const std::vector<ovms::Precision> UNSUPPORTED_KFS_INPUT_PRECISIONS_TENSO
     // ovms::Precision::CUSTOM)
 };
 
+static const std::vector<ovms::Precision> SUPPORTED_CAPI_INPUT_PRECISIONS_TENSORINPUTCONTENT{
+    // ovms::Precision::UNDEFINED,
+    // ovms::Precision::MIXED,
+    ovms::Precision::FP64,
+    ovms::Precision::FP32,
+    // ovms::Precision::FP16,
+    // ovms::Precision::Q78,
+    ovms::Precision::I16,
+    ovms::Precision::U8,
+    ovms::Precision::I8,
+    ovms::Precision::U16,
+    ovms::Precision::I32,
+    ovms::Precision::I64,
+    ovms::Precision::U32,
+    ovms::Precision::U64,
+    // ovms::Precision::BIN,
+    ovms::Precision::BOOL
+    // ovms::Precision::CUSTOM)
+};
+
+static const std::vector<ovms::Precision> UNSUPPORTED_CAPI_INPUT_PRECISIONS_TENSORINPUTCONTENT{
+    ovms::Precision::UNDEFINED,
+    ovms::Precision::MIXED,
+    // ovms::Precision::FP64,
+    // ovms::Precision::FP32,
+    ovms::Precision::FP16,
+    ovms::Precision::Q78,
+    // ovms::Precision::I16,
+    // ovms::Precision::U8,
+    // ovms::Precision::I8,
+    // ovms::Precision::U16,
+    // ovms::Precision::I32,
+    // ovms::Precision::I64,
+    // ovms::Precision::U32,
+    // ovms::Precision::U64,
+    ovms::Precision::BIN,
+    // ovms::Precision::BOOL
+    // ovms::Precision::CUSTOM)
+};
+
 void randomizePort(std::string& port);
+
+class ConstructorEnabledConfig : public ovms::Config {
+public:
+    ConstructorEnabledConfig() {}
+};
diff --git a/src/test/tfs_rest_parser_column_test.cpp b/src/test/tfs_rest_parser_column_test.cpp
index 13667569..2793d6b8 100644
--- a/src/test/tfs_rest_parser_column_test.cpp
+++ b/src/test/tfs_rest_parser_column_test.cpp
@@ -29,7 +29,6 @@ using namespace ovms;
 using namespace testing;
 using ::testing::ElementsAre;
 
-using tensorflow::DataType;
 using tensorflow::DataTypeSize;
 
 const char* predictRequestColumnNamedJson = R"({
@@ -80,12 +79,12 @@ TEST(TFSRestParserColumn, ParseValid2Inputs) {
     ASSERT_EQ(parser.getProto().inputs().count("inputB"), 1);
     const auto& inputA = parser.getProto().inputs().at("inputA");
     const auto& inputB = parser.getProto().inputs().at("inputB");
-    EXPECT_EQ(inputA.dtype(), DataType::DT_FLOAT);
-    EXPECT_EQ(inputB.dtype(), DataType::DT_FLOAT);
+    EXPECT_EQ(inputA.dtype(), tensorflow::DataType::DT_FLOAT);
+    EXPECT_EQ(inputB.dtype(), tensorflow::DataType::DT_FLOAT);
     EXPECT_THAT(asVector(inputA.tensor_shape()), ElementsAre(2, 2, 3, 2));
     EXPECT_THAT(asVector(inputB.tensor_shape()), ElementsAre(2, 2, 3));
-    ASSERT_EQ(inputA.tensor_content().size(), 2 * 2 * 3 * 2 * DataTypeSize(DataType::DT_FLOAT));
-    ASSERT_EQ(inputB.tensor_content().size(), 2 * 2 * 3 * DataTypeSize(DataType::DT_FLOAT));
+    ASSERT_EQ(inputA.tensor_content().size(), 2 * 2 * 3 * 2 * DataTypeSize(tensorflow::DataType::DT_FLOAT));
+    ASSERT_EQ(inputB.tensor_content().size(), 2 * 2 * 3 * DataTypeSize(tensorflow::DataType::DT_FLOAT));
     EXPECT_THAT(asVector<float>(inputA.tensor_content()), ElementsAre(
                                                               1.0, 2.0,
                                                               3.0, 4.0,
diff --git a/src/test/tfs_rest_parser_row_test.cpp b/src/test/tfs_rest_parser_row_test.cpp
index b519b805..56d036f3 100644
--- a/src/test/tfs_rest_parser_row_test.cpp
+++ b/src/test/tfs_rest_parser_row_test.cpp
@@ -30,7 +30,6 @@ using namespace ovms;
 using namespace testing;
 using ::testing::ElementsAre;
 
-using tensorflow::DataType;
 using tensorflow::DataTypeSize;
 
 const char* predictRequestRowNamedJson = R"({
@@ -81,12 +80,12 @@ TEST(TFSRestParserRow, ParseValid2Inputs) {
     ASSERT_EQ(parser.getProto().inputs().count("inputB"), 1);
     const auto& inputA = parser.getProto().inputs().at("inputA");
     const auto& inputB = parser.getProto().inputs().at("inputB");
-    EXPECT_EQ(inputA.dtype(), DataType::DT_FLOAT);
-    EXPECT_EQ(inputB.dtype(), DataType::DT_FLOAT);
+    EXPECT_EQ(inputA.dtype(), tensorflow::DataType::DT_FLOAT);
+    EXPECT_EQ(inputB.dtype(), tensorflow::DataType::DT_FLOAT);
     EXPECT_THAT(asVector(inputA.tensor_shape()), ElementsAre(2, 2, 3, 2));
     EXPECT_THAT(asVector(inputB.tensor_shape()), ElementsAre(2, 2, 3));
-    ASSERT_EQ(inputA.tensor_content().size(), 2 * 2 * 3 * 2 * DataTypeSize(DataType::DT_FLOAT));
-    ASSERT_EQ(inputB.tensor_content().size(), 2 * 2 * 3 * DataTypeSize(DataType::DT_FLOAT));
+    ASSERT_EQ(inputA.tensor_content().size(), 2 * 2 * 3 * 2 * DataTypeSize(tensorflow::DataType::DT_FLOAT));
+    ASSERT_EQ(inputB.tensor_content().size(), 2 * 2 * 3 * DataTypeSize(tensorflow::DataType::DT_FLOAT));
     EXPECT_THAT(asVector<float>(inputA.tensor_content()), ElementsAre(
                                                               1.0, 2.0,
                                                               3.0, 4.0,
